"The paper investigates an interesting problem, which is highly relevant for the ICLR community. The main contribution of the paper is empirical, the variants of the ACB algorithm admit a solid and novel design. All these support ACB as a viable exploration bonus for deep reinforcement learning.","This paper tackles the problem of exploration in Deep RL in settings with a large action space. To this end, the authors introduce an intrinsic reward inspired by the exploration bonus of LinUCB. This novel exploration method called anti-concentrated confidence bounds (ACB) provably approximates the elliptical exploration bonus of LinUCB by using an ensemble of least-squares regressors. This allows ACB to bypass costly covariance matrix inversion, which can be problematic for high-dimensional problems (hence allowing it to be used in large state spaces). Empirical experiments show that ACB enjoys near-optimal performance in linear stochastic bandits. However, experiments on Atari benchmark fail to show",0.8411008715629578
"The authors improve upon existing work to obtain a 2-Wasserstein mixing time bound of O(d0.5/epilson) for LMC, and show this is optimal. The article could be better written. For example, incorporating some of the remarks into the main text may read better. The main strength of the paper is to get sqrt(d) dependence on a dimension with not very restrictive assumptions. In my opinion, the linear growth condition of the third derivative could be discussed in more detail, to improve the presentation of the results.",This paper provides a near-optimal analysis of the unadjusted Langevin Monte Carlo (LMC) algorithm with respect to the W2 distance. The main statement is that the mixing time is  d1/2/eps under standard assumptions. The authors also give a nearly matching lower bound under these assumptions. The reviewers agreed that this is an interesting contribution obtained via non-trivial techniques. The consensus recommendation is to accept the paper.,0.8537102341651917
"This paper proposes a new learning method by revisiting the purpose of loss functions, which is to distinguish the performance of models. Hence, the authors aim to learn surrogate losses by making the surrogate losses have the same discriminability as the evaluation metrics. The proposed method can be widely applied to NLP and CV tasks, and its benefits seem significant according to the experiments. The authors conduct experiments on the synthetic and large-scale benchmark datasets, and gain significant improvements in performance and efficiency compared to existing surrogate loss learning methods.","The paper presents an approach to learn the surrogate loss for complex prediction tasks where the task loss is non-differentiable and non-decomposable. The novelty of the approach is to rely on differentiable sorting, optimizing the spearman correlation between the true loss and the surrogate. This leads to a pipeline that is simpler to integrate to existing works than approaches that try to learn a differentiable approximation to the task loss, and to better experimental results. The paper is well written and the approach clearly presented. The reviewers liked the simplicity of the approach and the promising experimental results on a variety of challenging tasks (human pose estimation and machine reading).",0.8658053874969482
This paper proposes a method to encode morphological information. The proposed approach is straightforward. The experiments show that the proposed approach outperformed competitive baselines such as SMP and AMORPHEUS. The reviewer has some concerns about the novelty of the paper. The paper is well-motivated in that structural and morphological information about the robot could help improve the performance in MTRL and transfer learning. The proposed method can still achieve good performance in MTRL because the training in MTRL allows the policy to overfit to the learnable positional embeddings. But the proposed embedding has limited generalization ability so it is not suitable for tasks like zero-shot policy,"This paper studies the role of positional and relational embedding s for multi-task reinforcement learning with transformer-based policies, The paper is well-motivated, the experiment shows its effectiveness against other competitive methods. In the rebuttal period, the authors solved most of the reviews’ questions such as novelty and ablation studies. There are still some concerns about the generalizability of this approach for other tasks and more experiments are needed.",0.865362823009491
The method has some great results and might really be the key to unlocking the lower precision. The paper is about from-scratch training to improve the performance to improve from-bit training. The authors propose a design methodology of NPU quantization. The authors show the superiority of the proposed metricization scheme. This reviewer is not sure whether the proposed method is good for ResNet-18.,"This paper introduces a method to determine which precision to use for the weights, as well as a quantisation method using hysteresis to improve performance with low-precision weights, including 4-bits. Reviewers tend to agree that the two points presented are useful and can have a large impact on the field. Generally, reviewers pointed out that motivations, notations and experimental studies could be improved. This has been partly addressed by the authors. I recommend to accept this paper for ICLR 2022.",0.8617965579032898
"This paper proposes a new deformed sampling module that explicitly downscales the image into target resolution. The proposed method is end-to-end trainable, and can be plugged into different backbone networks.","Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors' rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.",0.8385668396949768
"The paper provides a new view to parallelize the training task of machine learning models. The paper is based on solid techniques (multigrid reduction in time solver) that have been used in other domains (solving ordinary differential equations). The authors propose a novel parallel-in-time (PinT) training method based on a MGRIT solver for GRU networks with long sequences. The proposed algorithm enables the accelerated parallel training of GRUs on long sequence, which is a unique capability that permits growth in this dimension. The paper is well written and clear, especially the part of methodology. Overall, the paper is well written and clear,","This paper presents a way of using multigrid techniques to parallelize GRU networks across the time dimension. Reviewers are uniformly in favor of accepting the paper. The main strength is that the paper provides a new perspective on dealing with long input sequences by parallelizing RNNs across time. The main weaknesses are around the experiments: only CPU experiments are run, and sequences are not very long (max 128 length). All-in-all, though, it provides an interesting perspective that should be valuable to the community.",0.8608822226524353
"This paper proposes a sparse adversarial attack based on a random target image. The proposed methods are well-motivated and novel. The experiments are convincing and the experiment results show the effectiveness of the proposed attack. The main issue with this paper is the lack of an intuitive explanation as to why this attack is better at finding sparse and effective adversarial examples than previous work. I would have liked to see a more detailed algorithmic comparison with previous work. The proposed methods are well-motivated and novel. The paper is easy to follow for an adequately prepared reader. Prior work is sufficiently discussed. The amount of detail is good, it seems sufficient to reproduce","This paper introduces a technique to generate L0 adversarial examples in a black-box manner. The reviews are largely positive, with the reviewers especially commenting on the paper being well written and clearly explaining the method. The main drawbacks raised by the reviewers is that the method is not clearly compared to some prior work, but in the rebuttal the authors provide many of these numbers. On the whole this is a useful and interesting attack that would be worth accepting.",0.8664774894714355
"The paper makes nontrivial theoretical progress. The authors provided enough explanation about how this progress can be distinguished from other existing works, so the contribution looks to be unique. In Table 1&2, the comparisons of Top-1 prediction for 'Our methods' are missing. I recommend the author to compare with another L0 verification methods like https.openreview. net/pdf?id=HyeaSkrYPH.","Thank you for your submission to ICLR. The reviewers ultimately have mixed opinions on this paper, but reading in a bit more depth I don't feel that the critical comments raised by the sole negative reviewer really raise valid points. Specifically, the fact that this reviewer directly asks e. g. for comparisons to Levine and Feiz 2019, when the paper (before its revisions) contains an entire section devoted to exactly this comparison, strikes me as not sufficient for a thorough review. However, while I'm thus going to recommend the paper for acceptance (it does present a notable, if somewhat minor, advance upon the state of the art in randomized smoothing), I also feel the paper is generally rather",0.8204910755157471
This paper is an extension of previous theoretical results on SLTH from fully connected layers to convolutional layers. The main theorem is well supported by the empirical experiments.,"The paper presents interesting new results for pruning random convolutional networks to approximate a target function. It follows a recent line of work in the topic of pruning by learning. The results are novel, and the techniques interesting. There are some technical issues that are easy to fix within the camera ready timeline (see comments of reviewers below). I would also suggest refining the title of the paper: the lottery ticket hypothesis has an algorithmic component too, which clearly is not covered by existence results.",0.8588002324104309
"In terms of novelty, the sparse regression step is fairly well-known. The authors discuss the topic but it is not really clear. The authors mention this themselves, but any guidance on whether the entire mechanism need be restarted would be interesting.","The paper introduces a pipeline to discover PDEs from scarce and noisy data. Reviewers engaged in a very thoughtful discussion with the authors. I read the extensive rebuttal, and I believe the authors have addressed the major concerns claimed by the reviewers. I ask the authors to make sure to include all the changes and additional experiments in the camera-ready version.",0.8588513135910034
"The proposed method is novel, to the best of my knowledge. While both  -discounting and advantage weighting have been studied in a number of prior works, I am unaware of prior work that applies them to goal-conditioned behavior cloning methods. The paper is well written and the notation very easy to follow. The idea, although somewhat an obvious followup to GCSL, is well explained and analyzed, and experimental results are convincing. Overall I vote to accept this paper, but would appreciate responses from the authors on my questions.","The authors introduce a method that improves goal-conditioned supervised learning (GCSL) by iteratively re-weighting the experience by a variable that correlates with the number of steps till the desired goal. The reviewers mention that the authors focus on an important problem, their method is simple and the empirical results are significant. However, they do point several flaws of the paper, the main ones being questionable theoretical claims and the clarity of the presentation. After an extensive discussion, most reviewers agree that the paper should be accepted but I do encourage the authors to take into account the comments by the reviewers for the final version of the paper and make the theory more clear.",0.8629692196846008
The paper tackles an important and challenging problem faced in dimensionality reduction. The proposed method is simple and justified by existing research. The paper is hampered by a lack of notational clarity in essential sections that makes it difficult to determine the full contributions and impact of the proposed method.,"This paper proposes loss functions to encode topological priors during data embedding, based on persistence diagram constructions from computational topology. The paper initially had some expositional issues and technical questions, but the authors did an exceptional job of addressing them during the rebuttal period----nearly all reviewers raised their scores (or intended to but didn't update the numbers on their original reviews). The AC is willing to overlook some of the remaining questions. For example, concerns that topology isn't well known in the ICLR community (8muq) are partially addressed by the improved exposition (and it's OK to have technically sophisticated papers so long as some reviewers were able to evaluate them). And, future work can",0.845716118812561
"This paper proposes an algorithm that learns expressive graph representations. The proposed model is found to be very effective in graph isomorphism testing and in the detection of graph properties, while it outperforms the baselines on three real-world datasets.","This paper presents a neural version of individual-refinement (IR) architecture for improving the expressiveness of GNN in terms of isomorphism tests. As IR is the dominant approach of practical graph isomorphism tests, adapting IR to GNN is a novel and important idea. As IR suffers from the exponential number of branches, the paper adapts particle filtering algorithms to sample K paths to approximate the full version of the IR algorithm. Simulation and real-world datasets are used to demonstrate the improvement over base GNN. Strengths: + The paper is well written and easy to follow. + The originality of the paper is high since it is both technically rich. Adapting individual-refine",0.8623456954956055
"The authors provide theoretical support for the use of the pre-existing GIN model for ICA. They propose novel assumptions and prove this leads to identifiability. In particular, the main indeterminacy of a nonlinear ICA framework using volume-preserving is the rotation of latent variables. The authors further perform experiments to show that their method outperforms the state-of-the-art nonlinear ICA method iVAE.","This paper proposes an identifiable nonlinear ICA model based on volume-preserving transformations. The overall approach is very similar to the GIN method published @ ICLR 2020. There is a weak consensus among the reviewers that this paper has some merit, although none pushed for acceptance. After reviewing the paper myself, I agree that the contributions here appear to be incremental, but the results do push this growing field of identifiable latent variable models forward.",0.8724923133850098
The paper is well written and overall quite clear. The experiments were still with relatively small teams and with agent behavior largely coming from pre-trained RL policies. In RL the failures are seldom so consistent that unlearning would have such a low spread around the mean. Overall it's still difficult to say if the selected policies for testing are representative enough as say doing human evaluation where the style of play might be completely different.,"The paper presents a method for cooperative ad-hoc collaboration by learning latent representations of the teammates. The method is evaluated in three domains. All the reviewers agree that the method is novel and adds an interesting contribution to the important and difficult problem of the ad-hoc collaboration, making fewer assumptions about the team and the teammates. The next version of the paper should comment: - On the societal impact of the centralized training. - Wang et al, CoRL 2020, https://arxiv. org/abs/2003.06906, which addresses the cooperative tasks in the ad-hoc teams without privileged knowledge and assumptions about the teammates.",0.819940447807312
"The paper focuses on an important problem of activation normalization which to my knowledge has not yet been properly explored in the continual learning setting. The practicality and relevance of the proposed technique are high. The improvements offered by the method are mostly consistent between different tested settings, with few exceptions. The empirical evaluation provided in the paper is extensive and authors consider multiple important perspectives, such as the computational complexity of CN (the running time in Table 3), how it behaves in different CL settings, and how an ""oracle method"" performs (BN*). The paper does not provide the standard deviation of the results for the case of the COCOSeq benchmark.","The paper sheds light on issues with BN in continual learning and proposes a quite simple, which is a strength, solution to fix it. The Authors first draw attention to the fact that using recalculated moments boosts performance and reduces forgetting, which serves as an argument that at least partially BN contributes to catastrophic forgetting in continual learning. Given that BN remains quite important in certain application areas such as vision, it is a strong motivation for the paper. The experiments are thorough and clearly show that CN is a practically relevant alternative to BN in continual learning. One weakness of the paper is that the method is poorly motivated, and relatedly, it has quite limited novelty. CN combines the strengths and weaknesses of BN and",0.845694363117218
"This paper proposes a novel model for object-centric learning, based on an object-centric view and the dynamics of each object are treated unitarily. The theoretical analysis around the newly proposed equivariant layer is nice and useful. The evaluation only looks at a single simulated dataset. While these simulations are nice and insightful, I would have expected some real-world evaluation as well. The authors could have looked at some motion capture data where different parts of the human body could be treated as rigid-body constraints. For instance, the authors could have looked at some motion capture data where different parts of the human body could be treated as rigid","The manuscript develops a new kind of graph neural network (a Graph Mechanics Network; GMN) that is particularly well suited to representing and making predictions about physical mechanics systems (and data with similar structure). It does so by developing a way to build geometric constraints implicitly and naturally into the forward kinematics of the network, while still allowing for effective learning from data. The manuscript proves some essential properties of the new architecture and runs experiments both with simulated particles, hinges, sticks (and their combination), as well as with motion capture data. Reviewers were generally impressed by the writing and clarity of the work, as well as the main results. In addition, in those cases where reviewers thought that the experiments were",0.831382155418396
"This paper proposes a new benchmark and metric to measure the retention of time-invariant knowledge, updated knowledge, and new knowledge. The proposed CKL problem is quite interesting and important. The benchmark is useful and the proposed FUAR metric is technically sound. The experiments are very limited to the LAMA probing which does not necessarily connect to real downstream applications of these LMs. The experimental results will be more convincing if more pre-trained language models (such as GPT) are included. The authors did not show experiments or analysis in such a setting, where the new task has some ""task similarity"" with the learned tasks.","The paper introduces the problem of continual knowledge (language) learning. The authors point out the interesting duality between continual learning and knowledge learning where: in knowledge learning one must avoid forgetting time-invariant knowledge (avoid forgetting in CL), be able to acquire new knowledge (learn new tasks in CL), and replace outdated knowledge (a form of forgetting and re-learning or adaptation). In their paper, the authors develop an initial benchmark for the task along with a set of baselines and provide empirical studies. The initial reviews were quite mixed. The reviewers seem to agree this work studies an interesting and fairly novel direction for continual learning of language. However, the reviewers did not agree on whether this initial stab",0.8498232960700989
"The paper proposes to use GAN to create an inverse function to detect the pattern and severity of disease from real data. The reasoning behind the regularizations is overall reasonable. There could be more explanation, however, on why double decomposition is a good idea for the model. Regarding the experiment with simulated data, it is hard to understand why the number of patterns were chosen to be 3, unlike 2 which was suggested to be optimal from the real data. The paper also compares between other methods, without a reasonable explanation on why they were selected for comparison. Other references on cases where these approaches were used for the same goal would be","The authors present a GAN for learning a continuous representation of disease-related image patterns from regional volume information generated from structural MRI images. The reviewers find the problem relevant and appreciate the proposed solution. They find the paper well-written and find the empirical results on Alzheimer brain MRIs relevant for the neuroscience community. The overall objective function includes several hyper-parameters. As pointed out as the main weak point by multiple reviewers this may hint at overengineering/overfitting to a data set. However, the reviewers also mention that the regularizers are all sufficiently well-motivated in the paper and the author response. Reviewers highlight comparisons on the real data as a strong result demonstrating that Surreal GAN",0.8491945266723633
"The paper proposes a semi-supervised reward learning pipeline to reduce reward engineering efforts, which is an important topic. The paper is well organized and easy to follow. The proposed method is motivated by the success of data augmentation in the supervised learning area. The authors use a preference-based reinforcement learning method, which is based on a surrogate reward model.","The topic of learning reward functions from preferences and how to do this efficiently is of high interest to the ML/RL community. All reviewers appreciate the suggested technical approach and the thorough evaluations that demonstrate clear improvements. While the technical novelty of the paper is not entirely compelling, all reviewers recommend acceptance of the paper.",0.8680389523506165
The paper is well-written and motivates the choice of the linear transition map from theoretical prospective. The paper provides extensive ablation studies on their modelling choices as well as the snippet of the code for the model. The authors may consider to extend this part to probabilistic cases.,"The paper got four accepts (after the reviewers changed their scores), all with high confidences. The theories are complete and the experiments are solid. The AC found no reason to overturn reviewers' recommendations. However, the AC deemed that all the pieces are just routine, thus only recommended poster.",0.8368837237358093
"This paper tackles an important problem and provides a convincing demonstration that current SOTA models are susceptible to reliance on spurious correlation in the training set. I detail key strengths, weaknesses, and additional questions below.","This paper presents a new method to decrease the supervision cost for learning spurious attributes using worst-group loss minimization. Their method uses samples both with and without spurious attribute annotations to train a model to predict the spurious attribute, then use the pseudo-attribute predicted by the trained model as supervision on the spurious attribute to train a new robust model having minimal worst-group loss. The experiments show promising results in this domain for reducing annotation cost. The reviewers vote to accept the paper, and some of them increased their scores during the discussions since the authors have addressed their concerns.",0.8510436415672302
The paper shows improved performance over 2 recent baselines. The proposed approach is generally faster. The paper also includes a visualization of the latent space colored by objective prediction scores to motivate the concatenation of the predicted objective to the embeddings towards clustering.,"This paper presents a conditional variational autoencoder (CVAE) approach to solve an instance of stochastic integer program (SIP) using graph convolutional networks. Experiments show that their method achieves high quality solutions with high performance. It holds merit as an interesting novel application of CVAEs to the ML for combinatorial optimization literature, as well as for the nice empirical results which show a very nice improvement. Two reviewers had a concern that the contribution is a bit narrowly focused toward MILP-focused journal rather than a general-purpose ML conference since the core contribution is the novel application. On the other hand, they believe that combinatorial optimization has received growing interest from the ML community in recent years. All",0.8261153697967529
"This paper provides a simple and intuitive insight to ridge (or ridgeless) regression based multi-class classification. This provides a simple and intuitive insight to this phenomenon, and demonstrates how the leave-one-out error spikes at the point with mild assumptions. The main novelty of the manuscript is the characterization of the double descent computation of ridge (or ridgeless) regression based multi-class classification in terms of its closed form leave-one-out formulations and spectral decomposition of the kernel matrix. This provides a simple and intuitive insight to this phenomenon, and demonstrates how the leave-one-out error spikes at the point with quite",This paper proposes to use LOO to characterize the generalization error of neural networks via the connection between NN and kernel learning. The reviewers find the new results interesting. The meta reviewer agrees and thus recommend acceptance.,0.8332002758979797
"The authors need to clarify the utility of their 'prior knowledge' - how much of the recursive filter results are due to this prior knowledge as opposed to the Bayesian filtering? The paper is clear written. The experiments provide evidence for 1) model working in simple synthetic examples (passing sanity tests) and 2) model performing better than KF and EKF in both synthetic and real dataset. However, a more extensive comparison with other unsupervised methods is recommended. Given at least 9 other unsupervised methods in Table 1, adding at least one more unsupervised baseline to the audio denoting experiment can be helpful. The performance of SIN in figure 5 is a bit surprising","This paper presents a method for inference in state-space models with non-linear dynamics and linear-Gaussian observations. Instead of parameterizing a generative model, the paper proposes to parameterize the conditional distribution of current latent states given previous latent states and observations using locally linear transitions, where the parameters of the linear mappings are given by neural networks. Under fairly standard conditionally-independence assumptions, the paper uses known Bayesian filtering/smoothing tricks to derive a recursive estimation algorithm and a parameter-estimation method based on a simple maximum likelihood objective. Overall, the reviewers found the idea to be novel and interesting and I agree. They also found the relation to the noise2noise objective worth",0.8272153735160828
"The paper provides a possible explanation as to why representations learned over many classes by a lone classifier can serve as a competitive solution to few-shot learning problems, by exploring the idea of neural collapse. While I understand that the paper is not focused on beating the SOTA (Tian et al.), I don't agree with the reasoning provided for it. The theoretical justifications about NC occurring in unseen classes, and therefore translating to better accuracy in few-shot tasks seem like a novel contribution that gives an insight into the reasons a single classifier works well in a few shot problems when compared to algorithms devised specifically for such tasks.","Based on the previously observed neural collapse phenomenon that the features learned by over-parameterized classification networks show an interesting clustering property, this paper provides an explanation for this behavior by studying the transfer learning capability of foundation models for few-shot downstream tasks. Both theoretical and empirical justifications are presented to elaborate that neural collapse generalizes to new samples from the training classes, and to new classes as well. The problem that this paper delves into is important. The paper is well-motivated, and well structured with a good flow. Both theoretical and empirical analyses of the paper are solid. Preliminary ratings are mixed, but during rebuttal, multi-round responses and in-depth discussions were carried out between authors and",0.8478415012359619
"This section is a collection of doubts, suggestions about some more experiments (including other baselines), and how this submission can be improved overall. The paper extends the existing research direction in general learning. The proposed methods seem well-suited to the current problem. The key contribution is proposing continual learning. The paper extends the existing research direction in general learning.","One way of avoiding catastrophic forgetting in continual learning is through keeping a memory buffer for experience replay. This paper addresses the problem of online selection of representative samples for populating such memory buffer for experience replay. The paper proposes novel information-theoretic criteria that selects samples that captures surprise (samples that are most informative) and learnability (to avoid outliers). They utilize a Bayesian formulation to quantify informativeness. They provide two algorithms: a greedy approach, and an approach that takes timing (when to) update memory into account based on reservoir sampling to mitigate possible issues with class imbalance. Pros: The paper is well written and organized. It was easy to follow. The formulation is novel and technically sound",0.8334423899650574
"This paper makes a logical and theoretical connection to the proposed method based on the  -stationarity. The non-stationarity is represented by a distance of joint policy to an equilibrium. In the game-motivated MARL literature, the non-stationarity is represented by a distance of joint policy to an equilibrium. The proposed method cannot be applied to competitive and/or general-sum settings because an agent cannot directly control the learning of other agents in these settings. In particular, limiting the divergence between an agent's consecutive policies in competitive settings would not be desirable because the opponent can learn faster, exploit the agent's","Although the initial scores of the paper were not positive, the authors managed to properly address the questions/concerns of the reviewers and the changes they made to the paper convinced the reviewers' to update their scores. This clearly shows that there were flaws in the original presentation of the paper. So, I would recommend the authors to take the reviewers' comments into account when they prepare the camera-ready version of their work.",0.8198800086975098
"The paper is interesting, well written, and covers a relevant controversy (albeit in a niche). While previous works focused predominantly on deterministic networks with continuous activation functions, this work is among the first in which the information planes (IPs) can be interpreted from an information-theoretic perspective.","Initially, some reviewers have raised several points of criticism regarding certain aspects of the model whose novelty/significance was a bit unclear. After the rebuttal and the discussion phase, however, everyone agreed that most of these concerns could be addressed in a convincing way, and finally all reviewers were in favor of this paper. After carefully going over all the reviews, the rebuttal and the discussions, I fully agree with the reviewers and came to the conclusion that this paper indeed contains some interesting, novel and relevant contributions.",0.8418939113616943
"This paper argues that a subgraph-level positional encoding is enough for subgraph representation learning. It also introduces three self-supervised learning task to guide GNN learning. The main concern is: This paper adds the self-supervised learning into the framework, and compare with subGNN without such SSL learning. The motivation and insights of the SSL learning task is less than the subgraph encoding. I think the authors could consider either remove this part, or at least compare with other SSL learning methods, such as graph contrastive learning or generative learning.","This paper proposes a labeling trick for subgraph representation learning with GNNs. The proposed method, GLASS, improves on subgraph-level tasks. The topic of subgraph representation learning is relatively new, and this paper makes progress in that community which would be appreciated by other researchers interested in the same problem. The paper in the original submission state raised some concerns from the reviewers about unclear writing of the motivation and potential applications, technical novelty, and comparisons with existing approaches (even one that are not specifically designed for subgraph representation learning). It is good that the authors conducted additional experiments to show the effect of SSL (that the approach makes improvements without SSL). This and other clarifications from the",0.8607742786407471
"The proposed model allows to integrate of both structured and unstructured heterogeneous views which often happens in the context of omics data. In order to align nodes/features in every pair of views, the Gromov-Wasserstein (FGW) has been used. The experimental results show a significant improvement of the proposed model to the baselines in several downstream tasks. The authors demonstrate the ability of MoReL to learn from unpaired data and in a setting with missing data. It all comes at a cost of 4.5 fold increase in computational time. The evaluation is focused on the comparison to BayReL a related approach mentioned throughout the","A deep Bayesian generative model is presented for multi-omics integration, using fused Gromov-Wasserstein regularization between latent representations of the data views. The method removes several non-trivial and practically important restrictions from an earlier method BayRel, enabling application in new setups, while still performing well. Reviewers discussed the paper with the authors, resolving misunderstandings of the differences from earlier work (esp. BayReL). The authors reported more extensive experiments in the rebuttal, though not comparisons. The main remaining weakness is that the contributions are in a very narrow field, or at least aplications have only been demonstrated in the narrow field of multi-omics data analysis. And",0.8474506139755249
All experiments are based on synthetic datasets and no real datasets are used in this paper. It is not a good assumption that there is a supervised dataset of sparse recovery parameters of the proposed algorithm. The approach from this paper is promising and mostly novel. Overall I recommend accept.,"Dear Authors, The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted: - This paper does contribute solidly to a timely topic of theoretical understanding of sparisty recovery with deep unroling. - The original version had very limited experiments and only synthetic ones, which raised concerns about whether the setting is motivated and whether the algorithm works on actual real data. The revision fixed that to an extent with some experiments on real data. Yet, there are still some concerns that we suggest to be tackled for the final version: - The capacity analysis is carried out inside a strongly convex regime while the algorithm is advocated for nonconvex sparsity",0.8524540662765503
"This paper presents a new approach for adversarial training of image-to-image translation networks. The proposed approach makes a novel use of image-to-image translations network to generate corrupted samples for the classifier. The proposed approach seems to be significantly more computationally demanding than its counterparts such as AugMix and DeepAugment. The main results suggest that none of the single Ada methods can consistently outperform DeepAugment or AugMix, in any of the corruptions, clean accuracy, or adversarial attacks. In fact, none of the 4 proposed variants (EDSR,CAE,UNET, VQVAE) can by their own outperform","The paper proposes an adversarial data augmentation technique searching for adversarial weight perturbations of a corruption network (e. g. a pretrained image-to-image model). The goal is to achieve common corruption robustness as well as a non-trivial level of adversarial robustness. The authors claim state-of-the-art-performance on CIFAR10-C. Most reviewers had initial concerns which the authors could clarify in most cases. Finally, all reviewers argue for acceptance. Strengths: - no pre-defined corruption model necessary - extensive experiments on CIFAR10 and ImageNet with SOTA results - all reviewers agree that this paper would be valuable as a future reference Weaknesses: - the theoretical",0.8338308334350586
"The proposed approach is simple, effective, and query-efficient. The paper is well written and easy to follow. It would be interesting to see if the proposed approach achieves similar performance on other datasets e. g.","The paper shows that the transfer attack is query efficient and the success rate can be kept high with the zeroth-order score-based attack as a backup. Experiments show state-of-the-art results. Pros: - Simple method based on a simple idea. - State of the art performance. Cons: - Proposal is a straightforward combination of two methods, and therefore technical contribution is marginal. - The threat model is easy (surrogate can be trained on the same datasets and use the same loss function) and questionable. Most of the experimental evidence shows that the research for this threat model is almost saturated (and the problem seems almost solved). This paper got a borderline score with reviewer",0.8561687469482422
"This paper proposes a method for upscaling generative models. The method is based on ARDM, which generalizes a wide class of models. The experimental results are very promising in two ways: performance measure, and efficiency on the number of steps. The authors explicitly provide the limitation of the their work. The implementation is not provided (even though the authors provide algorithms and pseudo-codes, and promised to release the code). The paper is well written, easy to read and well-structured. This is very clear how the method relates to D3PM. The experiments are well designed and interpreted, this gives a good understanding of","This paper introduces Autoregressive Diffusion Models (ARDMs), which generalises order-agnostic autoregressive models and absorbing discrete diffusion. All reviewers appreciated the paper with a few also finding it very dense. The experimental section is a bit lacking in detail. This has to some degree been answered in the discussion and should also be included in the final version of the paper. Acceptance is recommended.",0.8476473689079285
"This paper proposes a training-free architecture search approach for vision transformers (ViTs). The paper also proposes elastic-tokens: progressive re-tokenization for fast ViT training. While the design search is efficient: requires only 7+5 GPU hours for seed+scaling stages. Compared with existing NAS based approaches, the proposed method requires less time to search for the optimal architecture. However, the final searched architecture by the proposed method has more FLOPs than existing methods e. g., ViT-ResNAS-t with 1.8B FLOPs compared with 8.9B of the proposed architecture, with slightly better","The paper introduces As-ViT, an interesting framework for searching and scaling ViTs without training. Overall, the paper received positive reviews. On the other hand, R1 rated the paper as marginally below the threshold, raising concerns about search on small datasets and issues regarding the comparison in terms of FLOPS/accuracy with other methods. The authors adequately addressed these concerns in the rebuttal, and helped clarify other questions by R2 and R3. R1 did not participate in the discussion after the author response nor updated his/her review. The AC agrees with R2 and R3 that the paper passes the acceptance bar of ICLR, as the unified approach for efficient search/scaling/training",0.8388970494270325
"This paper provides a novel view of the intersection of differentiable simulation and cloth simulation by thinking at the yarn level. The experiments are too coarse to show these advantages, which makes me question why the yarn-level simulator is used. The range of simulated experiments is not very wide, so I can't judge the expressivity / gamut of the model for solving secondary problems.","This paper introduces a differentiable yarn-level model of fabrics. The model is more detailed and physically realistic than proposed in earlier work, which may allow for applications to manufacturing guidance and textile design. The paper is generally well-written and contains detailed problem formulation and derivations. Experiments show it is possible to successfully learn a control policy and material parameters using the differentiable model.",0.864611804485321
The proposed approach is simple and does not require training additional components. The results are comparable to the results obtained to the reference approaches. The paper is easy to follow and the idea of the paper is clear. The proposed approach is easy to applied so the results are easy to reproduce. The novelty is a bit limited due to the fact that the authors combine two known approaches - KS statistical test and random projections. It seems that this framework is general and can be even applied to the bottleneck models like VAE or even for any low dimensional features extracted from the data. The question is can we apply this framework to any low dimensional representation of,"The paper investigates the use of flow models for out-of-distribution detection. The paper proposes to use a combination of random projections in the latent space of flow models and one-sample / two-sample statistical tests for detecting OOD inputs. The authors present results on image benchmarks as well as non-image benchmarks. The reviewers found the approach well-motivated and appreciated the ablations. The authors did a good job of addressing reviewer concerns during the rebuttal. During the discussion phase, the consensus decision leaned towards acceptance. I recommend accept and encourage the reviewers to address any remaining concerns in the final version. It might be worth discussing this paper in the related work: Density of States Estimation for Out",0.8448308110237122
"This paper proposes a new method to address the problem of missingness in convolutional neural networks (CNNs) and Vision Transformer (VIT) architecture. The proposed method uses a VIT architecture to simply drop target regions instead of blocking them out. The reviewers found the paper to be well written and the authors provided clear explanations at each stage. While overall I found the paper to provide meaningful insight and analysis, the paper could be improved through further analysis of adversarial robustness of CNNs vs VITs. The contribution is based on the empirical validation across a board of tasks and across datasets, which at the moment is not the case.","This work identifies an interesting bias that can occur when applying occlusion based interpretability methods to debug image classifiers. For context, the motivation behind many of these methods is that by occluding various parts of the image, one can ask counterfactuals such as ""what would the model have predicted if this object were not present in the image""? However, the authors note that when occluding pixels, classifiers are still functions of the occlusions themselves, so this process may introduce a bias as a result. This is most clearly demonstrated in Figure 2 where a convolutional architecture classifies various occluded images as ""jigsaw"" or ""crossword puzzle"", arguably due to the",0.8311847448348999
This submission is well written and easy to follow. It would be great if the authors could expand the discussion of bi-Lipschitz regularization. The paper is well written and easy to follow. It would be great if the authors could expand the discussion of bi-Lipschitz regularization. The paper is well written and easy to follow. It would be great if the authors could expand the discussion of bi-Lipschitz regularization. The paper is well written and easy to follow. It would be great if the authors could expand the discussion of bi-Lipschitz regularization. The paper is well written and,"Reviewers all found the work well-motivated in addressing uncertainty, a topic that has not seen much focus in meta-learning and few-shot learning. They describe the challenges well: small sample sizes and OOD shift. They then propose a solution they find works well empirically to overcome these challenges based on a set encoder and an energy function respectively. The proposal is largely one of engineering components that have been found to work well in the literature. I'm sympathetic to this style of research (particularly in today's neural network research), although the reviewers raise a primary concern about whether the choices leading to the proposal are justified. In particular, two Reviewers argue that there are no clear ablations compared",0.7935193777084351
"This paper proposes a method to produce coherent object-centric representations in video instead of static images. The main idea of learning object representations and physical dynamics from videos is interesting. I found anecdotal evidence for segmenting and tracking corresponding parts of objects very interesting, opening the door for more hierarchical concepts of objects using self/semi-supervised approaches. The proposed approach is heavily based on the Slot Attention [2] model. I could not find an apparent and exciting technical novelty that could be interesting for other domains or different tasks.","This work proposes a new framework that can learn the object-centric representation for video. The authors did a good job during rebuttal and turned one slightly negative reviewer into positive ones. The final scores are 6,6,8,8. AC agrees that this work is very interesting and deserves to be published on ICLR. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are also encouraged to make other necessary changes.",0.8504406809806824
"This paper is the first one to discover the scaling insights on both pretraining and fine-tuning Transformers. The authors perform comprehensive experiments and provide insights from different perspectives. The experiments results may be effected by the randomness and I don't find the error bar in the paper, which is the most concern of mine.","The results reported in this paper and the model checkpoints released are of interest and broad utility to the community in the opinion of the NLP. While one reviewer was somewhat negative, most reviewers were in favor of acceptance of this paper, which expands the results from [1] to downstream tasks. The AC therefore recommends acceptance.",0.8453395366668701
"The paper proposes a method to infer CAD sketches from hand-drawn images. The authors do not compare their results to any of these methods, making it hard to judge the relative quality of the generated CAD sketches.","The paper describes an approach for automatically generating CAD sketches, including both the primitives that describe the drawing, as well as the constraints that describe relationships between the primitives that need to be maintained even if the primitives are changed. This is an important problem that is starting to receive a lot of attention from the literature. Overall, the paper is very well executed and the results are quite compelling. There were some concerns about the relationship with the work by Willis et al. and other papers that were published around the time when this paper was submitted. There is still some novelty in this paper relative to those works as argued in appendix H, but it would have been really good to have a more quantitative",0.8634301424026489
The paper proposes a new architecture based on their new operators and implementing the finite impulse response (FIR) filter. The authors also extend their stability analysis in the main paper to not only consider single-layer features but also extend their analysis to multiple features. The experiments in this paper are done on synthetically generated datasets for two applications of flocking and unlabelled motion planning. I would have liked to see experiments done on more real-world mobility datasets to better understand the applicability of the proposed method.,"This paper proposes a new time-varying convolutional architecture (ST-GNN) for dynamic graphs. The reviewers were positive about the presentation and detailed theory, especially on the stability analysis. The shared criticism was on experimental validation synthetic datasets that the reviewers did not find appealing. The AC believes that while the lacking validation concerns are legit, there is a lack of sophisticated dynamic graph benchmarks in the community yet, so the authors did their best effort to test their method. We thus recommend to accept the paper.",0.8470010757446289
"The paper proposes an approach to scale rotationally transform CNNs on spherical domains to work with signals with arbitrary resolutions. This is accomplished by leveraging the properties of the scattering transform, by converting the input signal to the corresponding wavelet-based representation, before feeding it to some arbitrary spherical representation. The method is validated experimentally by testing the properties of equivariants to resolution on synthetic datasets. The paper is well written and enough references and context are provided for non-experts. However, the experimental section is very weak. The authors cite many other spherical CNN papers, but only compare to a single instance of one other paper. This is","The submission develops a rotationally equivariant scattering transform on the sphere. Many developments in deep learning make use of spherical representations, and the development of a rotationally equivariant scattering transform is an important if not unexpected development. The reviews are split with half of the reviewers believing it to be slightly above the threshold for acceptance, and half believe it to be slightly below the threshold for acceptance. In the papers favor, it solves an important case of the scattering transform framework, which has been demonstrated to be important in diverse machine learning applications such as learning with small data sets, differentially private learning, and network initialization. As such, continued fundamental development in this area is valuable, especially in the context of",0.8377938270568848
"The authors generalize their results on one FCNN to all FCNNs and on one CNN to all CNNs, on one task. The consistency across models and datasets should have been checked. The experimental report is interesting for researchers who want to have a better intuition of the influence of the prior.","This paper provides some empirical investigation of the choice of the prior distribution for the weights in Bayesian neural networks. It shows empirically that, when trained via SGD, weights in feedforward neural networks exhibit heavy-tails, while weights in convolutional neural networks are spatially correlated. From this observation they show that the use of such priors leads to some improved performances compared to the iid Gaussian prior in some experimental settings. Reviewers have conflicting views on this paper, that have not been reconcilied after the author's response and the discussion. On the plus side, the paper is very well written, the experimental part is carefully conducted, and provides some insights on the choice of the prior in Bayesian neural",0.838090717792511
The authors have done well to address and rectify the potential issues around the on-policy feature of AlphaZero with HER. The results provided are clear and show that this approach can confer significant benefit over AlphaZero in a reward sparse setting.,"The main detractor of this paper feels that the paper makes a relatively small technical and empirical contribution given existing results on HER (Andrychowicz et al., NeurIPS 2017). However, several other reviewers, who had more engagement in the discussion, were strong supporters. Having looked at the paper myself I thought the selection of experimental problems undermined the results. Experiments are most compelling when many unaffiliated groups compete on the same benchmarks. But the basic idea of integrating HER with AlphaZero, and a reasonable attempt at this, seems to be interesting enough to warrant a poster.",0.8434635400772095
"The main idea of the paper is to get the ""best of both worlds"" when different types of random features are useful in different regimes. The experiments in section 4.1 on pointwise kernel estimation also nicely support this narrative. The other experiments suggest at least some benefits, with perhaps the strongest results being for the speech model example in section 4.4. However, there are a few serious issues that need to be addressed. The paper presents HRFs in great generality, but very little of that generality is actually used in a consequential way. The main contribution of trading off whether the trig or positive random features are ""active""","Kernel methods are among the most flexible and powerful approaches of our times. Random features (RF) provide a recent mechanism to also make them scalable due to the associated finite (and often small)-dimensional approximate feature map (in the paper referred to as linearization). The focus of the submission is the linearization of the softmax kernel (defined in (1)) while making sure that the obtained RF approximation is accurate simultaneously for the small and the large kernel values. The authors present a hybrid random feature (HRF, defined in (8)) construction parameterized by base estimators and weights, and show that specific choice of these parameters is capable of implementing the goal. Some of the HRF estimators",0.8355160355567932
"The paper presents a novel view on the problem, and draws a novel conclusion out of the experiments. Although the paper could be a good survey for readers who want to learn about continual learning, I think its viewpoint is not new and its conclusion is not surprising. The authors didn’t give a comprehensive explanation on how they read such information or a concrete quantitative comparison to support this claim. The main reason for this is that the ""veins of CL methods"" has a different number over the course of the paper, which makes it hard to identify when a given list of N things is a list of the ""veins of CL","This paper presents a comparison and analysis of continual learning methods for pretrained language models. The authors categorise continual learning methods into three categories, those that use cross task regularisation, those that employ some form of experience replay of previous training examples, and those that dynamically alter the network architecture for each task. Evaluation results from representative examples of these three paradigms are then presented and analysed. In general methods that incorporate experience reply appear to perform the best, while analysis of the predictive power of individual layers of the pretrained models suggests that some network layers are more robust to catastrophic forgetting than others, and that this also varies across architectures (BERT, ALBERT, etc.). In general the reviewers agree that this",0.826855480670929
"This paper tackles an important problem and provides a convincing demonstration that current SOTA models are susceptible to reliance on spurious correlation in the training set. The focus on the ImageNet dataset is impressive and crucial to demonstrating that the reliance on spurious correlation issue is not isolated to toy datasets, but prevalent for one of the currently most important datasets in machine learning. The use of crowd source annotations is also interesting and interesting. While the pieces of the entire scheme presented in this work are not new, the combination is interesting and extends recent work that is focused on detecting whether a model relies on spurious correlation.","The paper tackles the important problem of spurious feature detection in deep neural networks. Specifically, it proposes a framework to identify core and spurious features by investigating the activation maps with human supervision. Then, it produces an annotated version of the ImageNet dataset with core and spurious features, called Salient ImageNet, which is then used to empirically assess the robustness of the method against spurious training signals in comparison with current SOTA models. As pointed out by the reviewers, this work is not about causality and the definitions of causal and spurious features were originally vague and inaccurate. During the revision and discussion, the authors changed the terms ""causal"" features/accuracy to ""core"" features/accuracy. They",0.8583943247795105
"This paper presents a differentiable DAG sampling which I think is novel in this field. It is easy to implement the algorithm in a few lines of code. The experimental evaluation is thorough. The results are convincing, with the proposed method achieving the best performance or comparable for structure learning.","The authors proposed an algorithm for sampling DAGs that is suited for continuous optimization. The sampling algorithm has two main steps: In the first step, a causal order over the variables is selected. In the second step, edges are sampled based on the selected order. Moreover, based on this algorithm, they proposed a method in order to learn the causal structure from the observational data. The causal structure learning algorithm is guaranteed to output a DAG at any time and it is not required any pre- or post-processing unlike previous work. There were concerns by two reviewers on the slight lack of novelty (""the proposed method of this paper is only a combination of well-developed techniques"") but I believe",0.8586424589157104
This paper investigates a topic that is interesting and timely. The results illustrate settings in which the quality of the policy proposals are very important to the final performance. The proposed hybrid method is straightforward.,The paper examines the advantage of using models in RL. The authors' rebuttals convinced us of the value of the paper.,0.8787065744400024
"This paper proposes a method for training a robot to learn a new task. The proposed method is intuitive and shows potential for a versatile agent which can quickly adapt to a new task. All tasks start from Microwave. In some of the tasks, the baseline approaches (goal-BC, FIST-no-pretrain, SPiRL (H-steps), SPiRL (H-steps)) fail to accomplish the Microwave sub-task for all random seeds while in some other cases, they succeed. In 1, a skill z is sampled every environment step. Is this better than executing H actions from the sampled skill z","The reviewers agree that addressing long-horizon tasks with off-line learning and fine tuning afterwards from demonstrations is an interesting and relevant topic. The technical ideas about learning a relevance metric to select relevant off-line data, and to learn an inverse skill dynamics models. The experimental results are convincing, even if success rates are sometimes lower than expected. All reviewers recommend acceptance of the paper.",0.8353386521339417
"The idea of hierarchical probabilistic clustering inference from the trees seems novel and interesting. The experimental section is very unconvincing, however. The comparisons are performed only wrt. primitive graph clustering methods. The metric space clustering baselines are given very suboptimal features for the clustering. The original DW paper uses d=128 for all graph sizes, and neural embedding-based models are known to perform poorly in low dimensional regime. While the paper claims scalability, it is only being evaluated on one large-scale dataset, which is not supporting the claim. The paper states that the baselines were trained 5 times and results were reported for the best run,",The authors introduce a novel probabilistic hierarchical clustering method for graphs. In particular they design an end-to-end gradient-based learning to optimize the Dasgupta cost and Tree Sampling Divergence cost at the same time. Overall the paper presents solid results both from a theoretical and experimental perspective so I think it is a good fit for the conference and I suggest accepting it.,0.84869784116745
"The paper focuses on an important aspect of experimental design for drug discovery, which has not previously been studied. The authors study only two candidate algorithms so far. The problem formulation should be clarified and is difficult to understand. The paper is straightforward, does not have any particular shortcomings, and is potentially useful to the community. However, I feel it lacks the kind of machine learning novelty and significance that would be expected from an ICLR publication.","This paper introduces a benchmark for experimental design algorithms for an important cellular biological question, causal discovery of effective genetic knock-out interventions. It uses existing datasets. The paper was discussed by the reviewers after the authors correctly pointed out that methodological machine learning novelty is not a necessary condition for accepting papers. Two reviewers increased their scores and all are slightly positive. The benchmark was seen as valuable, and one reviewer even commented they might use it in their own research. However, the paper is still on the borderline as this benchmark is only a first step. It has not been shown yet that machine learning insights can be produced with it, as the authors have not actually used it for benchmarking yet. In other words",0.8617637157440186
"This work empirically indicates that the learned node representations will be influenced by the neighbor label distribution. In particular, those nodes with minor class can have the neighbor memorization problem, that is, the target node's embedding might mainly represent the neighbor nodes but not itself. Experiments with class-imbalance setting are conducted to demonstrate the superiority of the proposed augmentation method.","Although reviews were initially a little polarized, they trend toward accepting the paper after rebuttal and discussion. The most negative review raised issues of datasets, baselines, and experiments, and various details that they find confusing. These concerns were not shared by the other reviewers for the most part. Following a detailed rebuttal the most negative reviewer ended up siding with the more positive reviewers.",0.8295519948005676
The problem is well-formulated and experiments are validating the claims authors made in the paper. The overall idea of the proposed method makes sense to me. The paper is well structured and describes concisely and clearly the work done. The experimental evaluation is appropriate and demonstrates the benefits of the proposed method in a large variety of domains.,"In this paper, a new method is proposed to discover diverse policies solving a given task. The key ideas are to (1) learn one policy at a time, with each new policy trying to be different enough from the previous ones, and (2) switch between two rewards on a per-trajectory basis: the ""normal"" reward on trajectories that are unlikely enough under previoiusly discovered policies, and a ""diversity-inducing"" reward on trajectories that are too likely (so as to push the policy being learned away from the previous ones). The main benefit of this switching mechanism is to ensure that the new policy will be optimal, because the reward signal isn't ""diluted"" by the",0.8271526098251343
"This paper proposes a new way of looking at a very well-studied problem in traffic management. The new approach is novel, technically sound, and well motivated. As a first step, there is much opportunity for further research into more sophisticated models for intelligent pattern extraction, filtering, and inference.",The paper presents a neural architecture based on neural memory modules to model the spatiotemporal traffic data. The reviewers think this is an important application of deep learning and thus fits the topic of ICLR. The writing and the novelty of the proposed method need improvement.,0.8688130378723145
The paper provides a relative new framework for GNNs where the label trick is replaced by a deterministic alternative which focuses on exclusion of self propagation. The proposed use cases based on the theoretical are interesting and promising according to the empirical evaluations.,"The paper provides the theoretical justification for the ""label trick"" (using labels in graph-based semisupervised learning tasks). The authors performed a thorough evaluation of their analysis, which constitutes an experimental contribution. The authors provided a rebuttal that the AC finds to have reasonably addressed the reviewers' concerns. We recommend acceptance.",0.8538365960121155
The paper proposes a new approach to computation-based optimization for recurrent architectures. The paper has empirical evidence that the proposed method works across a range of gradient estimators.,"This paper adapts a method called ""real-time recurrent learning"" for training recurrent neural networks. The idea is to project the true gradient onto a subspace of desired dimensionality along a candidate direction. There are a variety of possible candidates: random directions, backpropagation through time, meta-learning approaches, etc. The main strength of the paper is that it is a very simple idea that seems to have practical utility. While often presented in different contexts, it should be clearly noted by the authors that the general idea of using low dimensional directional derivatives for computational efficiency is fairly common in optimization. Reviewers mention sketch and project methods. This has also been looked, for example, in the context",0.8587512969970703
"This paper investigates whether maximum entropy reinforcement learning can be thought as a robust problem. The authors correctly describe the related works, which shows the relationship between MaxEnt RL and some kind of robustness w. r. t the reward.","The reviewers thought this paper tackles an interesting question around whether MaxEnt RL already provides an important form of robustness. Such work helps us better understand the intersection between generalization, regularization and robustness. The reviewers had a number of comments, questions and clarifications and were generally satisfied with the detailed responses provided by the authors. There was some concern over the strength of the experiments and the authors also ran additional experiments. These addressed one reviewer’s concerns, though the other still thought the existing experiments were a bit too simple.",0.8487750887870789
The paper is the first to propose sampling from an unsupervised adversarially trained model. The proposed Contrastive Energy-based Models (CEM) gives a probabilistic interpretation for adversarial training. The authors show results on a relatively small-scale and low-resolution dataset. The results presented are not ideal to explain the effectiveness of the proposed framework.,"This paper presents a probabilistic framework that explains why models trained adversarially are robust generators. It received fairly high initial scores. The reviewers thought the work was novel and interesting. They liked that the analysis provided a way to derive a novel training method and sampling algorithms. Reviewers confirmed their support of acceptance and I think this paper is clearly above the bar. Respectfully, I’d prefer that the authors don’t ask the reviewers to “raise your score”. It is up to the reviewers to make that decision.",0.8572010397911072
"All reviewers agreed that the paper is well written and the theoretical part is very natural and nicely presented. The idea of stability will likely be picked up in future works and the authors show how to construct PE-stable layers which can be useful. In particular, the discussion with Laplacian eigenvectors based PEs that could have ambiguities due to the sign and multiplicity is reviewed clearly and the work attempts to address the issues. The focus in this work is only on link prediction tasks. Although I believe the work can be applied to general graph representation learning for other tasks as well. The experimental results have demonstrated the superiority of the","This work studies the question of increasing the expressive power of GNNs by adding positional encodings while preserving equivariance and stability to graph perturbations. Reviewers were generally positive about this work, highlighting its judicious problem setup, identifying the right notion of stability and how it should drive the design of positional encodings. Despite some concerns about the discrepancy between the theoretical results and the empirical evaluation, the consensus was ultimately that this work is an interesting contribution, and therefore the AC recommends acceptance.",0.8460928797721863
"The paper addresses a relevant topic and proposes a novel method that has not been covered in the literature. The proposed method is simple yet effective. It would be interesting to provide some insights into whether the entire pre-training dataset needs to be poisoned for the attack to be effective. Specifically, in section 4.1 ""Pretraining the foundation model"", the paper does not provide any details of how the pretraining task is modified for the poisoned data. However, I am not fully convinced about the empirical evaluation and why the proposed approach should work in practice. The paper uses a vague novelty to say that ""training procedure mainly follows the training process","The paper presents a backdoor attack approach against pre-trained models that may affect different downstream languages tasks with the same trigger. The paper shows that the downstream models can inherit security holes from upstream pre-trained models. The paper is on the borderline and disagreement remains after discussion and author responses. In general, the new setting introduced in the paper is interesting and well-motivated. However, the options split in how realistic the setting is (e. g., use of uncommon trigger), the evaluation of stealthiness, and the novelty of the idea. After checking the paper, I believe the ideas and insights are justifiable for an ICLR paper and they differ significantly enough from the prior work. I do",0.8498114943504333
"The paper is well written and the theoretical results are well explained. I believe the results are correct and sound, albeit I did not check every detail in the proof. In particular, what do we intuitively gain in terms of expression power when extending the space of function from bandlimited functions to Korobov space.","The authors theoretically analyze the approximation of Korobov functions by neural networks, obtaining upper and nearly matching lower bounds, for shallow and deep networks, with different activation functions. The bounds are stronger than what can be proved for the more commonly studied Sobolev functions. But Korobov functions are a natural and fairly wide class of functions. This work makes a substantial step forward in clarifying what kind of functions are especially amenable to representation by neural networks. The reviewers also appreciated the clarity of the writing.",0.8540388345718384
"This paper proposes a method to learn an augmentation policy using adversarial objective, enforce the augmented samples to be similar to the original samples, and use continuous relaxation to train the augmentation policy. Overall, the paper is well-written and easy to understand. The proposed method is intuitive, simple to apply and effective over many baseline augmentation methods on various datasets, especially on low-resource, class-imbalanced regimes. The proposed algorithm in this paper is a combination of known techniques, i. e. e. all the data augmentation techniques are well-established, and the relaxation method used to train the policy is also not new. The main","We appreciate the authors for addressing the comments raised by the reviewers during the discussion period, which includes providing more experimental results to address the concerns. We believe the publication of this paper can contribute to the important topic of data augmentation. The authors are highly recommended to consider all the comments and suggestions made by the reviewers when further revising their paper for publication.",0.8396033644676208
The paper is well-written and adequately clear (but can be improved). The experiment is performed thoroughly. It shows clear advantages to the previous works. The explanation can be improved for the readers who are not familiar with the previous works of KMs.,"The authors present a new memory-augmented neural network that is related to the Kanerva machine of Wu et. al. The reviewers considered the ideas in the paper novel and interesting, but were concerned about presentation issues and literature review. The authors have improved both... however- authors: please even under limited space constraints, make more room for related work! Clarifying your contribution in the context of the literature is critical for reader understanding, and neglecting this almost had your paper rejected out of hand. I am voting to accept",0.8424858450889587
"This manuscript proposes an interesting latent mixture survival model that makes a useful contribution to the field. The experiments are well designed with good baselines and evaluation methodology. The results seem to be presented fairly, and the authors discuss both the limitations of their approach. The proposed method is compared against a large collection of previous methods on both synthetic and real data sets. Overall, experiments with simulated and real data demonstrate that the proposed method can provide more accurate/meaningful cluster assignments than previous methods, while at the same time providing survival predictions that are on par with the existing methods.","Four knowledgeable referees recommend Accept. I also think the paper provides a unique contribution to the field of deep survival models and I, therefore, recommend Accept",0.8472694754600525
"This paper focuses on improving the dialogue policy with the responses by utilizing a pre-trained language model and offline RL. The proposed method is reasonable and moderately novel. The experimental results are promising for both settings. However, there are unclear parts to be addressed or clarified. The paper can be better if adding the real-user interactions, because the performance may be different between the simulation environment and the real-user interactions reported by prior results (DSTC in ConvLab). The paper can be better if adding the real-user interactions, because the performance may be different between the simulation environment and the real-user interactions reported by prior results","The reviewers are all weakly positive. The author response clarified important aspects of the paper. The new human evaluation was critical. However, the human evaluation result presentation is flawed: presenting Likert scores as means does not reflect them well. The authors should use something similar to a Gantt chart to fully reflect the distribution across Likert categories. Another detail in the human evaluation that are troubling: it does not reflect interaction with the system, but judgements through observation. Therefore, the human evaluation does not reflect the ability of the learned dialogue system to interact with users. Overall, the paper makes a nice, original contribution, but despite author improvement there are evaluation flaws (even if they are common in papers using these",0.8333498239517212
"This paper presents a new method for the classification of subword-levels in natural language models. The proposed method learns subword tokenization using a weighted average of subword blocks. The main concern is that the proposed method does not seem to be satisfying the criteria for a proper tokenization method. The proposed method does not seem to be satisfying the criteria for a proper tokenization method. In all cases, the proposed method is significantly worse than subword models. In all cases, it is not clear if the proposed GBST module learns meaningful subword tokenization. There is no proper evaluation of the learned segmentations other than","This paper introduces a soft gradient-based subword tokenization module (GBST) that learns latent subword representations from characters. GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. The resulting model was tested on GLUE, and several cross-lingual tasks. The performance is competitive with ByteT5 and often similar to subword models while being more efficient in FLOPs and RAM. Reviewers are mixed on this. The negative reviewer points to how this not being a real tokenizer and does not produce a tokenization, that experiments that use the base model do not address bigger scales, and that there is a",0.8579943776130676
The idea of studying geoemetry-preserving regularization while learning representations is a challenging and an important topic with a large potential impact. The proposed regularization method is convincing and well justified. The idea of coordinate invariant functional is novel (for the best of my knowledge) and theoretically sound. The state of the art is complete. The experiments on the most simpler scenarios like MNIST and CMU motion capture are useful to understand the behavior of the proposed approach. The paper is clear and well-motivated. The problem of introducing geometry properties to the latent space is challenging and not much explored. The paper is clear and well-motivated,"This paper proposes an extension to learning a representation: it motivates, proposes and evaluates a new regularizer term that promotes smoothness via enforcing the representation to be geometry-preserving (isometry, conformal mapping of degree k). Comparisons with a standard VAE and FMVAE (Chen et al. 2020) are shown and experiments are provided on CelebA with several different attributes as target classification tasks. The paper has received extensive reviews and the authors have successfully answered most of the concerns raised, mostly regarding comparisons to other techniques that try to introduce a regularization based on the properties of the Jacobian of the decoder network. The appendix has been extended as a result of the rebuttal and the paper could be",0.8436012268066406
This work addresses a relevant problem that has not been addressed before in the literature. The “the right to be forgotten” is part of the legislation of many countries. AI-Industry needs tools to implement this right. The proposed method is sound and provides theoretical guarantees about the capacity of the presented approach to implement this task. This is something that is really needed in this context. A well-performed experimental evaluation shows the proposed method. The paper should be well structured and well written.,"The article proposes an approach to alter the approximate posterior distribution in order to remove some of the information (unlearning). The approach is applicable when the approximate posterior is obtained via (stochastic gradient) MCMC methods. Unlearning is done by shifting the approximate true posterior by some value delta, which is found via optimisation with an influence function. The approach is novel, tackles an important problem and is mathematically sound. Reviewers have highlighted some of its limitations. In particular, the modified posterior is obtained by translating the original posterior, without changing its shape; many aspects of the data may therefore not be forgotten. The authors have partially addressed this concern in their response and provided additional experiments. Although there is",0.8402735590934753
"The paper is well written, fairly simple to follow, and provides good justifications and reminders about the overall story to the reader when going into mathematical details. Overall, I think the motivation is strong and a cleaner analysis is definitely much appreciated. The main paper and the few pages of proof that I checked seemed solid but I must admit that this paper is behind my reach. That said, I find interesting for the community to get a few theoretical results on actor-critic, and I do not expect such results to be easy to handle. This is the major concern for my score 5; I'd like to raise my score if","The paper makes a significant contribution in the rather sparse and challenging field of convergence analyses of actor-critic style algorithms, under the linear MDP structural assumption, showing that there is a natural bias towards being high-entropy. As one of the reviewers points out, although it is unlikely that the strategy actually proposed is amenable to implementation, the paper nevertheless provides a clean and novel analysis of convergence of learning by eschewing the usual mixing time type assumptions often found in the theoretically-oriented RL literature. Based on this strength of the paper, I am glad to recommend its acceptance.",0.8394955992698669
The paper proposes a method for detecting anomaly/ out-of-distribution (OOD) detection. The novelty meets the bar for a publication without rising to the level of being significant. The writing can be improved in some places (e. g. paragraph 2 in the Introduction).,"This paper introduces a novel approach for out of distribution detection that generates scores from a trained DNN model by using the Fisher-Rao distance between the feature distributions of a given input sample at the logit layer and the lower layers of the model and the corresponding mean feature distributions over the training data. The use of Fisher-Rao distance is novel in the context of OOD, and the empirical evaluations are extensive. The main concerns of the reviewers were the limitations of the Gaussianity assumption used in computing the Fisher-Rao distance and the use of the sum of the Fisher-Rao distances to the class-conditional distributions of the target classes rather than the minimum distance. These concerns were addressed",0.828184187412262
"This paper presents a method for unsupervised pretraining on small network. The method seems to be simple conceptually, yet yields good results on unsupervised pretraining on small network. The explanations are very clear and the paper is well written. Some related references could be added (see bellow), but overall it is complete. The experimental results seem thus less complete for a number of reasons, although the results look to be to extent promising.","Four experts reviewed the paper. All but Reviewer HSTU recommended acceptance. The authors clearly did a great job with the rebuttal, which convinced two reviewers to raise their scores above the acceptance threshold. Notably, the reviewers found the newly added experiments impressively strong. The rebuttal also addressed some clarification questions. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance. As mentioned by the reviewers, some experiments and discussions in the rebuttal should be included in the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!",0.8360996246337891
"This work introduces regularization for training neural network with discrete variables. The proposed method promotes the output of continuous functions of Gaussian random variables to discrete. The authors have conducted various experiments on different benchmarks to showcase its superiority over existing methods. The authors present two versions of the proposed method: one with mean centering and the other without centering. The authors also discuss how the proposed regularization can be combined with the Gumbel-Softmax method, where the stability is computed with the vanilla softmax without adding the Gumbel noise. The authors claim that these either do not impact the performance much or are easier to tune without any quantitative results.","The paper introduces a method to train neural networks based on so-called stability regularisation. The method encourages the outputs of functions of Gaussian random variables to be close to discrete and does not require temperature annealing like the Gumbel Softmax. All reviewers agreed that the proposed method was novel and of interest. The authors conducted extensive experiments. They also adequately addressed the concerns raised by the reviewers (e. g., theoretical foundation and computational cost).",0.8729056715965271
"This paper proposes an extension of unrolled alternating minimization that preserves the structure of the variable A on synthetic and real multispectral imaging data. It is interesting and practical. There is also significant time savings using the unrolled approach compared to the standard AltMin of PALM. The approach presented in the paper is interesting, but several important details are missing or insufficiently explained. As a result, it is hard to judge the impact issues with existing algorithms are never well explained. The authors also make statements like ""LPALM-LLT"", which would be similar to the ISTA-LLT while still updating A through alternating minimization","The paper develops an unrolled version of the PALM algorithm for sparse blind (or semi-blind) source separation. The unrolled version includes a soft-thresholding update, in which the thresholding parameter and one of the weight matrices is learned from data, with a least squares dictionary update, in which the step size is learned from data. The paper provides experimental results showing that this LPALM algorithm is less sensitive to the choice of hyper parameters (since step sizes, etc. are learned from data), and to the choice of the initial dictionary (perhaps since the W matrices are learned from similar examples). It also improves over PALM on experimental data from an astronomy problem. Reviewers expressed appreciation for the",0.8400165438652039
"The authors have done a good amount of technical work to evaluate their approach in different contexts. The authors estimate the Hessian using the method of finite differences (equation 1), which requires four model evaluations to get one entry in the Hessian matrix. The number of pulls needed would be 1000, which is smaller than the 1500 pulls needed for finding only the top 20 interactions. This difference would be even more dramatic for experiments with even more features. The authors mention experiments with more features in supplement F - however, it looks like the authors only discuss the AUCs and don't mention the number of pulls needed for these larger","This paper tackles the problem of feature interactions identification in black-box models, which is an important problem towards achieving explainable AI/ML. The authors formulate the problem under the multi-armed bandit setting and propose a solution based on the UCB algorithm. This simplification of the problem leads to a computationally feasible solution, for which the authors provide several theoretical analyses. The importance of the learned interactions is showcased in a new deep learning model leveraging these interactions, leading to a reduction in model size (thereby competing against pruning methods) as well as an improvement in accuracy (thereby competing against generalization methods). Although the proposed approach essentially builds on the specific UCB algorithm, it could likely be extended/",0.8382890224456787
"The proposed approach is technically sound, easy to implement, and empirically validated on various datasets. The experimental evaluation is thorough and allow one's to evaluate the importance of the type of normalization and the effect de-normalization module.","This paper introduces the ""reversible instance normalization"" (RevIN), a method for addressing temporal distribution shift in time-series forecasting. RevIN consists in normalizing (subtracting the mean and dividing by the standard deviation) each layer of of deep neural network in a given temporal window for a given instance, and de-normalizing by introducing learnable shifting and scaling parameters. The paper initially received one weak accept and two weak reject recommendations. The main limitations pointed out by reviewers relate to the limited novelty of the approach, the positioning with window normalization methods and hybrid methods in times series, and clarifications on experiments. The authors' rebuttal did a good job in answering the main concerns:",0.8427863717079163
"The paper proposes an alternative mechanism (PROBELESS) which behaves different from existing methods Models studied are cross-lingual, unlike most probing work which has focused on English Ranking and intervention experiments are generally convincing and well-executed weaknesses: For example, the conflation between probe quality and ranking quality was not clear to me in the abstract/introduction, and perhaps an illustrative example or figure would help clarify this point.","This paper analyses interpretation methods that use probes to evaluate the information in individual neurons of a deep network and shows that it confounds probe quality and ranking quality, and encoded information and used information. The paper proposes a new method which does not suffer from the same drawbacks. The reviewers were positive about this paper, and the discussion between the reviewers and authors resulted in the authors adding multiple clarifications. I ask the authors to try to optimize the paper for clarity further. I recommend acceptance.",0.8428069949150085
"This paper proposes StarQE for question answering over hyper-relational knowledge graphs. The proposed model is simple but performs well on certain types of queries. It can deal with the question-answering task with qualifier pairs (Yet, some contextual information is not allowed). A new dataset is constructed in order to verify the model efficacy. The experiments could be more convincing by using more datasets. It is not sure that if this method can also be used on other datasets or domains. The novelty is limited. The model is just a combination of two existing approaches. Overall, the motivation is strong, and the method is technically sound.","This paper presents a query embedding approach for answering multi-hop queries over a hyper-relational knowledge graph (KG). The main contributions are a new dataset (WD50K-QE) for this task and a simple but sensible extension to an existing model for query embeddings to also handle relation qualifiers. Reviewers wJVm and Bute note that the reification and StarQE models perform similarly. While this is not a negative result, as the authors note, it does raise the question of the relative pros and cons of the two methods. I hope the authors can add a discussion of when one might prefer StarQE over the conceptually simpler reification method in the final version.",0.854397714138031
"This paper proposes to apply the idea of hypersolvers to numerical optimal control. The model is able to approximate the actual dynamics with a combination of low-order ODE solver with learned truncation residual dynamics. In experiments, the method is shown to perform on par with the higher-order ODE solvers.","The authors propose a novel hypersolver framework for solving numerical optimal control problems, learning a low order ODE and a neural network based residual dynamics. They compare their framework with traditional optimal control solvers on a number of control tasks and demonstrate superior performance. The reviewers are in consensus that the paper makes significant contributions that are validated by the experimental results. The only concern was that the experiments are largely on low dimensional systems, but the reviewers agreed that the results are still worthy of acceptance.",0.8802418112754822
"This paper introduces a novel approach for synthesizing time series data. The proposed approach is novel for generating time series data. The paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of the proposed framework. It shows that DeepAR achieves much better performance. Based on this experiment, wouldn't it be better to use DeepAR for imputing/predicting the time series in the far-forecasting experiment? The authors should compare to this baseline or clarify why this is not possible. The paper is well written and easy to follow.","This paper adapts the idea of progressive growing of GANs to time series synthesis. The reviewers thought that the idea was well motivated. DRP7 initially expressed concern w. r. t. novelty. They were also concerned with the lack of certain baselines. The authors responded, highlighting its contributions w. r. t. Evaluation (Context-FID score) and extensiveness of the evaluation. The authors also added missing references but pushed back on the additional baselines. DRP7 raised their score. Reviewer pbaT was also positive about the work though had some questions and suggestions for improving clarity. They had initially given a low score for “correctness” but raised",0.8298256397247314
"The authors present their motivations well. The paper was clearly written and easy to understand. In addition, it would be interesting to show the performance change of the entire ToM net according to the number of agents. Finally, the ToM model is limited in its novelty, but its use to influence communication is novel. The authors evaluate the approach on two synthetic tasks: cooperative navigation and multi-sensor target coverage.","The current paper presents a new method for communication and cooperation in multi-agent settings. Specifically, the authors propose to model other agents' intentions and internal states using ToM nets and using these predictions to then decide how to communicate/coordinate. The authors present experiments in two multi-agent cooperation tasks (multi-sensor multi-target coverage and cooperative navigation), compare against 4 previous methods (TarMAC, I2C, MAPPO and HiT-MAC) and perform the necessary ablations studies and find that their method achieve better rewards in both environments. All reviewers have found the present study to be novel with convincing experimental findings. Reviewers have raised some concerns however a great deal of those have been",0.8670235276222229
"All reviewers agree that the paper is very novel and useful in their opinion. I think this paper is good. In terms of novelty, clarity, and significance, I think this paper is good. The question of why distillation improves performance is an important one. To my knowledge, the visualization of learning paths is novel, as is the Filter-KD method. I enjoyed reading the paper, and I think the ideas described have the potential to inspire interesting future work. The improvements from KD on these datasets are small to begin with, so they may not be particularly good benchmarks for this work.","The authors made substantial improvements to the originally submitted manuscript; however, reviewers initially remained reluctant to support the paper for acceptance based on the degree to which they were confident in the underlying arguments / position taken by the authors and the evidence provided to support their position and arguments. There are also concerns about the significance of the gains in performance afforded by the proposed approach. During the author response period two reviewers became satisfied with the additions and modifications leading to an increase in the final score. It will be critical for the authors to try to add ImageNet results if possible in addition to other promises made to reviewers. The AC recommends accepting this paper.",0.8467621207237244
"The paper is well-organized and easy to follow. The proposed approach is simple and efficient. The main strength of the paper is the novelty of applying calibration weights directly to the spatial weights in the temporal dimension. This approach appears to have benefits in terms of computation. The paper was easy to read and understand, and the experimental results are extensive and show good performance. There are no comparisons with any of the dynamic modules. The idea of content-adaptive weights (or modules) has been extensively explored in the area of dynamic networks. However, this paper lacks experiments that compare the performance of TAda2D and previous approaches. In","The authors study the problem of video classification and propose a new module which promises to increase accuracy while keeping the computational overhead low. The main idea is not to share the spatial convolution weights over different time steps, but allow some modulation based on pooled local and global frame descriptors. The resulting module can be used as a drop-in replacement for spatial convolutions in existing models and yields competitive performance on multiple video action recognition and localisation benchmarks. The reviewers appreciated this challenging setting and the simplicity of the main idea. They found that the paper was clearly written, well organised, and easy to follow. The reviewers raised some issues in connections with related work and the empirical evaluation which were successfully resolved during the discussion",0.8618628978729248
"The proposed method is very simple, and seems to be relatively simple to optimize hyper parameters. The empirical results seem to convincingly indicate that this approach outperforms prior work. The paper would be stronger if it analyzed more deeply why this approach does not do well with more anchors.","The paper tackles the problem of generalizing to a new environment by learning a small set up anchor policies (even just 2 for the final approach) which span a sub-space that can be searched efficiently in a new environment. The discussion and additional experiments managed to convince most reviewers that the method indeed works as the authors had hypothesized (especially regarding functional diversity). At the moment the analysis is mainly based on empirical observations, it would be good to also have a thorough theoretical analysis of the method.",0.8644707202911377
The authors introduce a new problem of adding sparse local training to federated learning. The problem appears well motivated (sparse training can lead to significant power savings at the edge) and challenging (directly adding sparse training to conventional aggregation methods like FedAvg leads to significant accuracy drops). The proposed solutions are good initial approaches backed by the empirical study and do appear to lead to some improvement in accuracy over the naive approach. The experimental results are also a bit limited. The main weakness of this paper is the experiments section. The results are presented only on CIFAR-10 dataset and do not consider many other datasets from Federated learning benchmarks (e. g,"This paper considers the problem of on-device training for federated learning. This is an important problem since, in real-world settings, the clients have limited compute and memory, and local training needs to be efficient. The paper shows that the standard sparsity based speed-up techniques that consider top-K weights/activations during forward and/or backward pass do not work well in the federated setting and proposes several solutions to mitigate this issue. The proposed solutions are demonstrated to work well on several datasets. In their initial assessment, given that this is largely an empirical insights driven paper, the reviewers mainly expressed concerns about the experimental evaluation (e. g., only one dataset CIFAR10",0.862592339515686
This paper provides a new solution to handle the high-dimensional problems. It is recommended to validate GMCN on more complex datasets. The evaluation is quite insufficient as this paper claims that it is a general purpose deep learning. The results are not very convincing on complex tasks. The paper proposes approaches for GM fitting so that the output of the activation function (ReLu) becomes a GM. The resulting proposed approach shows good performance for classification. The cross product (convolution) of two GMMs has a closed form solution. The paper proposes approaches for GM fitting so that the output of the activation function (ReLu) becomes a GM,"This paper presents a deep learning method that aims to address the curse-of-dimensionality problem of conventional convolutional neural networks (CNNs) by representing data and kernels with unconstrained ‘mixtures’ of Gaussians and exploiting the analytical form of the convolution of multidimensional Gaussian mixtures. Since the number of mixture components rapidly increases from layer to layer (after convolution) and common activation functions such as ReLU do not preserve the Gaussian Mixtures (GM), the paper proposes a fitting stage that fits a GM to the output of the transfer function and uses a heuristic to reduce the number of mixture components. Experiments are presented on MNIST (2d) and ModelNet10 (3D), which show",0.8413286209106445
This is a great step toward better understanding of existing methods. The experimental designs are quite solid and insightful. I like the style of providing conjectures and show results that empirically verifying them.,"This paper provide an explanation why contrastive learning methods like SimSiam avoid collapse without negative samples. As the authors claimed, this is indeed a timely work for understanding the recent success in self-supervised learning (SSL). The key idea in this submission is to decomposes the gradient into a center vector and residual vector which respectively correspond to de-centering and de-correlation. Such an explanation is interesting and novel. The empirical results are solid and convincing. During the rebuttal stage, the concerns from the reviewers are well resolved, and the writing of the new version is significantly better than the original one.",0.8510459065437317
"The paper proposes an interesting model that performs explainable reasoning on the way to justify their predictions. The writing clarity of the paper is good, the idea and content are easy to follow, and there are good visualizations that help understanding. The proposed methodology is well-described over the paper and introduced experimental settings are properly designed with three public datasets. The written results seem to be reasonable. For most presented, the authors' architecture is relatively competitive but does not achieve state of the art results, as I would expect from a model that baked in explainability. The experimental results are relatively weak when comparing the main model with its baselines in","This paper proposes a generalization of the standard Transformer attention mechanism in which keys and queries represent abstract concepts (which must be specified a priori). This in turn yields ""concept embeddings"" (and logits) as intermediate network outputs, providing a sort of interpretability. Reviewers agreed that this is a simple (in a good way) and interesting approach, and may lead to follow-up work that builds on this architecture. Some concerns regarding the relation of this method to prior work—in particular the ""Concept Bottleneck"" model—were raised and addressed in discussion; the authors might incorporate additional discussion in future drafts of the work.",0.8332620859146118
"The main weakness of the method, I think, is its poor robustness, as it requires each triple to have supporting patterns. The proposed method would cost more time than others, as it has an additional process of subgraph retrieval. In my view, the method could cost more time than other baselines, such as those purely embedding-based methods for link prediction. In my view, the method could cost more time than other baselines, such as those purely embedding-based methods for link prediction.","The paper presents an approach to predict relations between node pairs in heterogeneous graphs, with application to recommendation and knowledge-base completion. The author's approach is to compute similarities between subgraphs that are neighborhoods of nodes where the relation holds or not to score a relation. The authors use graph neural networks to scores these subgraphs. The type of subgraphs that are considered are pairs of nodes, 3- and 4- cyles to make inference and training tractable. The paper lies in the stream of work that combines logical reasoning and neural network, even though in that particular instance it mostly combines graph mining techniques and neural networks. The reviewers unanimously liked the presentation of the paper and the high empirical performance. The",0.8273281455039978
"This paper proposes to leverage the RL or POMDPs framework, which is an important capability for continual/lifelong settings. The method is based on an elegant approach with Dirichlet Solves a previously untackled but realistic problem. The experiments show that the model is flexible enough to accommodate new contexts, which is an important capability for continual/lifelong settings. The paper is well-written and mostly well-organized, and presents adequate literature. All reviewers agree that the paper is well-written.",The paper proposes a Bayesian approach to learning in contextual MDPs where the contexts can dynamically vary during the episode. The authors did well in their rebuttal and alleviated most of the reviewers' concerns. During the discussion there was an agreement that the paper should be accepted. Please take all reviewer comments into account when preparing the final version.,0.8556836247444153
"This paper proposes a novel method to perform post-hoc correction from the perspective of optimal transport. The proposed method can be applied to different baselines to boost their performance. There are some concerns that the proposed method may not be feasible in real-world applications. The authors provide a convergence analysis for the proposed OTLM, making the paper easier to be reimplemented and reproduced. The authors assume that the ""Table distribution on the test set"" is known, followed by an explicit constraint YT1N=c for the optimization. In another word, the paper assumes the test set to have a balanced label distribution and optimize the predictions","All reviewers agreed that the idea proposed by the paper is interesting and is well-motivated for handling long-tailed recognition problems. As suggested by the reviewers, it seems important that the limitations the paper be addressed in the final version of the paper.",0.8502687215805054
"The paper proposes a novel idea to change the initial sampling distribution to improve inference speed and training convergence. All reviewers agree that this is a novel idea and the paper is well-written. However, there are several issues that need to be addressed in the rebuttal and discussion.","This paper suggests using a conditional prior in conditional diffusion-based generative models. Typically, only the score function estimator is provided with the conditioning signal, and the prior is an unconditional standard Gaussian distribution. It is shown that making the prior conditional improves results on speech generation tasks. Several reviewers initially recommended rejection, but after extensive discussion and interaction with the authors, all reviewers have given this work a ""borderline accept"" rating. Criticisms included that the idea is too simple or obvious to warrant an ICLR paper. I am inclined to disagree: simple ideas that work are often the ones that persist and see rapid adoption (dropout regularisation is my favourite example). Like the authors, I believe",0.8575230836868286
"This paper presents an approach to tackle an important problem in the field of natural language generation. The proposed augmentation technique is straightforward and well illustrated. The experimental results demonstrate certain improvements in these tasks. The main concern with this paper is that since the augmented target-side input is produced by the decoder conditioning on the ground truth, this method seems helpful but limited. Thus it still cannot narrow the gap of decoding between training time and inference time. The modest improvements in the NMT task and abstract summarization task also indicate the existence of this problem, and cast doubt on its effectiveness.","This paper presents a method for target side data augmentation for sequence to sequence models. The authors of the paper use a relatively straightforward method to generate pseudo tokens that are used for enhanced training. The authors present results on dialog generation, MT and summarization where automatic metrics show improvements. For really robust results, I would have preferred to see more human evaluations since BLEU and ROUGE are metrics that the NLP community is moving away from. Overall, the majority of the reviewers are happy with the paper and there is significant back and forth between the reviewers and authors that have improved the paper; I think the authors went to significant lengths to allay all concerns from the reviewers and the paper should be",0.8561553359031677
"The paper makes an insightful connection between X3d's conv block and transformer block, and build a unified computational block. Compared to current methods focus on adding good properties of conv blocks to new transformer blocks (e. g., swin-transformer). The proposed method achieves SOTA result, which is impressive. The paper provides comprehensive ablation studies on the design choices. The bold highlights are inconsistent. Table 3 highlights the best accuracy results among all methods, while Table 2 highlights results are not so. In addition, highlights in Table 2 may make readers ignore the fact that MoViNet-A6 results","The paper presents an approach for spatio-temporal representation learning using Transformers. It introduces a particular architecture design, which shows an impressive computational efficiency. The reviewers agree that the experimental results are strong, and unanimously recommend the paper for acceptance. The reviewers find their concerns regarding the details of the approach/setting address after the authors' response. We recommend accepting the paper.",0.8437042236328125
"This is an interesting paper that presents a new and interesting problem. I think the use of generative modeling and inference to gain insight into decision making of an online learning agent is quite innovative. The authors seem to be the first to identify this problem as of interest (see discussion about novelty in weaknesses). The proposed algorithm outperforms existing imitation learning methods in terms of power. However, what would also have been interesting is to see success/failure of existing predictive imitation learning methods for gaining insight into decision making process of organ donation acceptance decision. Is that possible? If no, why not and if yes, what is the motivation behind the","All three reviewers suggest acceptance of the paper. The authors study an interesting problem (understanding non-stationary and reactionary policies) and propose a solution to the problem which compares favorably to baselines in experiments. However, some of the reviewers also criticize unclarities in the presentation of the paper and the made assumptions. The authors clarified those points quite well in their rebuttal. Further concerns regarded design decisions and the comparison to failure cases of baselines. The authors addressed those in their rebuttal and promised to include corresponding material in their updated paper. Hence I am suggesting acceptance of the paper. Nevertheless, I would like to urge the authors to carefuly revise their problem presentation in the paper in order to improve clarity",0.8393775224685669
"The paper analyzed the proposed non-linear multi-mode tensor factorization algorithms, and the theoretical results help to understand the gap between the linear and non-linear factorization gap.","The paper considers matrix and tensor factorization, and provides a bound on the excess risk which is an improved bound over the bounds for ordinary matrix factorization. The authors also show how to solve the model with standard gradient-based optimization algorithms, and present results showing good accuracy. The method can be a bit slow but this depends a bit on the number of iterations, and in general it achieves better accuracy in a similar amount of time to other baseline algorithms. The reviewers raised a few points, such as jdoi noting the tensor experiments were for small tensors and should include the method Costco as well; other reviewers mentioned more methods as well. The authors seemed to address most of these concerns in the",0.8610854148864746
"The manuscript is well presented and clear, and the details on experiments are thorough in the main text and would be easy to reproduce, the appendix is also clear and to the point. The problem of reducing spatial overhead in signature methods is an important one and is in need of more study, and I believe the manuscript is a fine addition the the literature.","This paper proposes a novel method for training neural rough differential equations, a recent model for processing very long time-series data. The method involves a lower-dimensional embedding of the log-signature, which is obtained via pretrained autoencoder to reduce overhead. The results show significant and consistent improvements over previous methods on long time-series data. Overall, the reviewers and I all agree that this paper offers a novel and impactful contribution leading to significant improvements over previous state-of-the-art methods for training neural rough differential equations. I recommend acceptance.",0.8486630320549011
"The proposed NHG method is a preconditioning gradient update that F G are updated following standard gradient ascent update. This would require the preconditioning matrix (e. g., pseudo inverse Fisher) be able to recover from the gradient  the information in the gradient.","The paper studies the convergence of a generalized gradient descent ascent (GDA) flow in certain classes of nonconvex-nonconcave min-max setups. While the nonconvex-nonconcave setups are computationally intractable and GDA is known to converge even in some convex-concave setups, the paper argues that (generalized) GDA can converge on what is dubbed ""Hidden Convex-Concave Games"" in the paper and argued that it contains GANs as a special case. The reviewers all found the paper interesting and a worthy contribution to the literature on nonconvex-nonconcave zero-sum games. Main concerns expressed by the reviewers were w. r. t",0.8085635900497437
"This paper proposes a new neural network for contextual learning, called NeuraLCB, that works in an online fashion that trained on one data point at each iteration. The algorithm essentially works in an online fashion that trained on one data point at each iteration. In experiments the authors mentioned a B-mode variant of NeuraLCB, e. g. The main theorem and several lemmas; they look sound and I only found a few mistakes (see below). I would suggest the authors to report the performance of S-mode and B-mode separately. In experiment on the real-world datasets, the authors observed that","This paper studies off-policy learning of contextual bandits with neural network generalization. The proposed algorithm NeuraLCB acts based on pessimistic estimates of the rewards obtained through lower confidence bounds. NeuraLCB is both analyzed and empirically evaluated. This paper received four borderline reviews, which improved during the rebuttal phase. The main strengths of this paper are that it is well executed and that the result is timely, considering the recent advances in pessimism for offline RL. The weakness is that the result is not very technically novel, essentially a direct combination of pessimism with neural networks. This paper was discussed and all reviewers agreed that the strengths of this paper outweigh its weaknesses. I agree and recommended this paper to be accepted",0.8378291726112366
"The overall idea of iterative hierarchical heatmap refinement is interesting and appears to be novel. The experiments demonstrate the ideas validity, although there is definitely room for further strengthening. There are some key insights missing from the experimental section, which will be expanded on below.","This paper proposes a joint multi-agent trajectory prediction framework for multiple agents using a ""heatmap"" estimation approach employing a hierarchical strategy and sparse image generation for for efficient inference. The method takes a set of predicted trajectories for each agent produces reorderings. The work yields a top result on a competitive leaderboard. While multiple reviewers were initially concerned about the paper not making a single major contribution, the author response discussion helped to clear up the degree of novelty. Further experiments provided during the review also led to multiple reviewers increasing their score. In the end, all reviewers recommend acceptance of this paper. As such the AC recommends accepting this paper.",0.8447957038879395
"The paper correctly identifies current challenges of defining CL research evaluations. The paper proposes a classification strategy. This type of review works help other sciences, e. g., and usually have dedicated publishing tracks in journals. Perhaps it is time for such tracks in machine learning main conferences. The lack of such a system is an important source of confusion in CL research. This should be made clear upfront, with a discussion of all the assumptions such learning settings make. The authors do a great job presenting the different aspects of the evaluation and explaining why they are meaningful. The problem of relevant evaluation of the continual learning approaches is a major one, and","This review paper presents a way of comparative assessment of continual learning. Reviewers all agreed that this work is interesting, unique with comprehensive coverage of the CL space. The proposed categorization, CLEVA-Compass, and its GUI have great potential to facilitate future CL work.",0.8482367992401123
"The paper proposes a new learning-based framework for optimization problems. The proposed neural variant of SDDP makes the paper easier to follow for different communities. The motivation of the structure of value functions is well explained. The paper provides a thorough empirical investigation of their algorithm, which shows the advantage of  -SDDP. The experimental setting is very synthetic and as for portfolio optimization, I doubt there are real-world instances of portfolio optimization that do not optimize a quadratic objective that includes a risk term. It would be better if the paper could compare MSSO with RL methods with known  -DP performance from the data. The","This paper applies deep learning to a problem from OR, namely multistage stochastic optimization (MSSO). The main contribution is a method for learning a neural mapping from MSSO problem instances to value functions, which can be used to warm-start the SDDP solver, a state-of-the-art method for solving MSSO. The method is tested on two typical OR problems, inventory control and portfolio management. The reviewers think that the idea is interesting, the empirical results are impressive, and the paper is well-written. However, there are reservations on its relevance to the ICLR community.",0.8555014133453369
"The proposed problem is a natural and practical extension of the offline IL problem. This is a nice step towards practical applications of IL. The paper is overall well written and well organized. However, there are unclear statements that need further clarification.","The paper presents a method for learning sequential decision making policies from a mix of demonstrations of varying quality. The reviewers agree, and I concur, that the method is relevant to the ICLR community. It is non-trivial, the empirical evaluations and theoretical analysis are rigorous, resulting in a novel method that produces near optimal policies from more readily available demonstrations. The authors revised the manuscript to reflect the reviewers' comments.",0.8584793210029602
"The authors do a great job at presenting the general challenge they target, their method, and the design choices. The paper is throughout very well-written and clear. The authors explicitly acknowledge this on multiple occasions and point out why the differences matter. The performance of their presented method increases with the size of the vocabulary. The authors also state that ""Due to the high cost of training JT-VAE and HierVAE, we did not tune their hyper parameters and instead used the default values."" This also seems to handicap these methods. As a minor point, it would be nice if the authors could clarify whether the numbers shown in","Most reviewers were positive about the paper, seeing that the proposed method is practical and has convincing experimental performances. One reviewer was a bit negative and raised questions about clarity. After the authors responded, the negative reviewer didn't respond further. After reviewing all the comments, the AC feels that there is enough support from reviewers to accept this paper.",0.8465515971183777
"This paper proposes a single-shot active learning method that chooses the data to be labeled in the target domain in such a way that the expected risk of the target domain is reduced. The proposed algorithm for active learning can be constructed in the way of hypothesis set-free or loss function-free. The proposed method, Accelerated K-medoids, is not only a method for selecting K inputs to be labeled in the target domain, but also an algorithm for clustering. In the appendix, the authors compare the empirical computational time of the Greedy K-medoids and the Accelerated one, and show the effectiveness of the latter. The","This paper deals with the important topic of active transfer learning. All reviewers agree that while the paper presents some shortcomings, it is considered to be a worth contribution.",0.8427370190620422
"This paper proposes to maximize the similarity of gradients of the domain computation loss for different domains to learn invariant features. In particular, IRM defines its objective by measuring optimality, which is highly related to optimization. One may be able to explain the success of IDGM from the perspective of optimization, and further relates it to IRM. In particular, IRM defines its objective by measuring optimality, which is highly related to optimization. The measure of the paper is it lacks of rigorous analysis about why and how IDGM works. For instance, IRM performs a careful study to analyze the proposed objective from a causal perspective.","This paper proposes a new method for domain generalization. The main idea is to encourage higher inner-product between gradients from different domains. Instead of adding an explicit regularizer to encourage this, authors propose an optimization algorithm called Fish which implicitly encourages higher inner-product between gradients of different domains. Authors further show their proposed method is competitive on challenging benchmarks such as WILDS and DomainBed. Reviewers all found the proposed algorithm novel and expressed that the contributions of the paper in terms of improving domain generalization is significant. A major issue that came up during the discussion period was that we realized that the presented results on WILDS benchmark are misleading. In particular, the following statements in the manuscript are false because on """,0.8450855016708374
"The paper is well written and the ideas are presented with clarity. Experiments are thorough, including ablations that show the effectiveness of different model components. It is interesting to see that using an additional classification loss on a separate stream of clustered RoI object features leads to significant improvement in detection performance.","This paper presents work on open-world object detection. The main idea is to use fixed per-category semantic anchors. These can be incrementally added to when new data appear. The reviewers engaged in significant discussion around the paper with many iterations of improvements to the paper. Initial concerns regarding zero-shot learning were addressed, as were remarks on presentation and claims. In the end the reviewers were split on this paper. I recommend to accept the paper on the basis of the semantic topology ideas and the thorough experimental results. The remaining concern centered around the evaluation protocol used in the paper, which follows that in the literature (e. g. Joseph et al. CVPR 21). While this is not a",0.8510999083518982
The paper proposes a method to model a latent variable that encodes the time-varying aspect of the dynamics of a robot. The proposed work provides a wrapper around the Kalman filter. The key idea of the proposed work is to provide a wrapper around the Kalman filter. The paper misses out on recent works by Xie et al. (2020) that build upon the work by Nagabandi et al. (2018a;b) and Finn et al. (2020). They look at a very similar HiP-MDP setup (and also goes beyond just modeling the time-varying dynamics). The model assumes that,"The paper addresses a few very important points on sequential latent-variable models, and introduce a different view on meta-RL. Even though the methods that this paper poses are incremental, it is such a hot-debated topic that I would prefer to see this published now.",0.8365846276283264
"The paper presents an interesting approach to solving TSP that can outperform some other existing approaches. The proposed method is novel, might be significant, and the quality of this paper seems to be on-par with other papers applying ML techniques to solve TSP published at top-tier conferences (which are also cited in this paper). All the percentages of optimally solved problems are relatively low, so the plots for some algorithms are not clearly visible (however, it is clear that the introduced algorithm outperforms other approaches). I recommend acceptance of this paper.","In this paper, a novel machine learning-based method for solving TSP is presented; this method uses guided local search in conjunction with a graph neural network, which is trained to predict regret. Reviewers disagree rather sharply on the merits of the paper. Three reviewers think that the paper is novel, interesting, and has good empirical results. Two reviewers think that the fact the results are not competitive with the best non-learning-based (""classic"") solvers mean that the paper should be rejected. This area chair believes that research is fundamentally not about beating benchmarks, but about new, interesting, and sound ideas. The conceptual novelty of this method, together with the good results compared with other learning-based methods",0.8592230677604675
"The discussion of the optimization issue caused by the NLL is very clear with nice synthetic examples. The paper is well written and mostly easy to follow. It seems like there’s significant overlap between the performance of the different methods. The notation in Equation (9) is confusing. There is a long line of work about ""critically"" initializing neural networks so that their curvature grows exponentially with depth. The authors might consider looking at Poole et al., ""Exponential expressivity in deep neural networks through transient chaos"" and Raghu et al., ""On the expressive power of deep neural networks""","The paper examines the approach of modeling aleatoric uncertainty by fitting a neural network, that estimates mean and variances of a heteorscedasitic Gaussian distribution, based on log likelihood maximization. The authors identify the problem that gradient based training on the netgative log likelihood (NLL) may result in suboptimal solutions where a high predicted variance compensates for the predicted mean being far from the true mean. To solve this problem, the authors suggest to adjust the log likelihood objective by weighting the log likelihood of each single data point by the corresponding beta-exponentiated variance estimate. This adjusted objective is referred to as beta-NLL. All reviewers agreed that the identified problem and",0.8238109350204468
"This paper shows that DDPMs are good representation learners. The paper is well written. The experiments and baselines are well-motivated. Most importantly, this can lead to further research on DDPMs as feature extractions. Most importantly, this can lead to further research on DDPMs as feature extractions.","The paper proposes using the intermediate representation learned in a denoising diffusion model for the label-efficient semantic segmentation task. The reviewers are generally positive with the submission. They like the simplicity of the proposed algorithm. They also like the effort of the paper in verifying the intermediate representation learned by a diffusion model is semantically meaningful and can be used for segmentation. Initially, there was some concern about the size of the validation set, which is addressed by the rebuttal. Consolidating the reviews and rebuttals, the meta-reviewer agrees with the assessment of the reviewers and would like to recommend acceptance of the paper.",0.8416284918785095
The approach does not require re-training (doing pre-training again) experimental results on several tasks show only a small drop in model quality. The reduction in the number of parameters is modest compared to methods that re-train the network in a compact representation.,"The paper studies the problem of task-specific model compression obtained from fine-tuning large pre-trained language models. The work follows the line of research in which model size is reduced by decomposing the matrices in the model into smaller factors. Two-step approaches apply SVD and then fine-tuned the model on task specific data. The present work makes the observation that after the first step (the SVD compression) the model can dramatically lose its performance, due to the mismatched optimization objectives between the low-rank approximation and the target task. The work provides evidence backing this claim. The paper proposes to address this problem by weighting the importance of parameters for the factorization according to the Fisher information.",0.8501104712486267
"The proposed approach is technically sound and exhibits very competitive performance. The empirical evaluation is thorough as it considers multiple test problems and a broad range of benchmark methods. In particular, I am having a hard time understanding how the decoder is defined when the problem instance is not a graph. The use of the term ""preference"" is confusing. The paper is well-written and presents an interesting approach to solving MOCO problems. The paper is well-written and presents an interesting approach to solving MOCO problems. The proposed method is novel, might be significant, and the quality of this paper seems to be on-par with other papers",This paper develops a preference-conditioned” approach to approximate the Pareto frontier for Multi-Objective Combinatorial Optimization (MOCO) problems with a single model (thus dealing with the thorny problem that there can be exponentially-many Pareto-optimal solutions). It appears to provide flexibility for users to obtain various preferred tradeoffs between the objectives without extra search. The basic idea is to use end-to-end RL to train the single model for all different preferences simultaneously. The technical soundness and practical performance are strong. This work's approximation guarantee depends on the ability to approximately solve several (weighted) single-objective problems. This may be challenging due to the NP-hardness of,0.8340883851051331
"This paper proposes a new method for contrastive learning, called Proto CPC. The proposed method is well described and seems to work as shown by sufficient experiments in various distillation settings. The main concern is the novelty of this method. It combines the method and contrastive learning while these two techniques are well researched in the literature. Also, compared with CRD+KD the improvement is marginal in the knowledge distillation task. After rebuttal: I keep my original rate. I think this paper is well presented with decent results. I think this paper is well presented with decent results. I think this paper is well presented with decent results. I think this","This paper proposes a prototypical contrastive predictive coding by combining the prototypical method and contrastive learning, and presents its efficient implementation for three distillation tasks: supervised model compression, self-supervised model compression, and self-supervised learning via self-distillation. The paper is well-written, and the effectiveness of the proposed method is validated through extensive experiments. Reviewers generally agree the paper has clear merits despite some weaknesses for improvement. Overall, I would like to recommend it for acceptance and encourage authors to incorporate all the review comments and suggestions in the final version.",0.8572850823402405
"The paper provides a plausible causal representation of the adversarial attack process and derives a useful explanation for vulnerability. The robustness approach derived from their explanation is quite reasonable and is novel to my knowledge. I am not sure about all the assumptions used here (e. g., independent gaussians), but at least on the benchmarks reported, the method performs well. I am not sure about all the assumptions used here (e. g., independent gaussians), but at least on the benchmarks reported, the method performs well. The authors provide an interpretation of past methods within their framework, which I thought was a nice insight","The paper shows a causal perspective to the adversarial robustness problem. Based on a causal graph of the adversarial data creation process, which describes the perceived data as a function of content and style variables, the authors identified that the spurious correlation between style and label is the main reason for adversarial examples. Based on this observation, they propose a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. Experiments on MNIST, CIFAR-10, and CIFAR-100 datasets show that the proposed method is better than two baselines, Madry and TRADES. Overall, the paper contains interesting ideas and tackles an important problem. Due to some concerns regarding the clarity",0.8381575345993042
This paper proposes a single optimization problem for linear transformation of fairness. The robustness criterion can be formulated as a single optimization problem as long as the ambiguity set is defined using a single normal distribution. A smooth reformulation is presented which can be solved using standard riemannian gradient descent algorithm. The theoretical results are relatively easy to derive based on existing results on distributionally robust optimization and riemannian optimization.,"This paper considers the problem of distributionally robust fair PCA for binary sensitive variables. The main modeling contribution of the paper is the consideration of fairness and robustness of the PCA simultaneously, and the main technical contribution of the paper is the provision of a Riemannian subgradient descent algorithm for this problem and proof that it reaches local optima of this non-convex optimization problem. The results will be of interest to those working at the intersection of fair and robust learning.",0.870957612991333
The proposed attack is straightforward and empirically effective against different defense strategies. The performance proposed method is also impressive under the combination of different defense methods. The main contribution of this paper is from the empirical side. The problem addressed is very interesting in terms of pointing out a major deficiency in neural graph embedding models against GIA attacks. I believe the contribution of this work is significant in that further work can be built on their results to address vulnerability in deep graph models.,"The reviewers agree that this paper studies an important problem, provides theoretically analysis to understand graph injection attack. The authors propose a new regularizer to improve the attack success. Extensive experimental results also show the effectiveness of the proposed method.",0.8691888451576233
"This paper presents an approach for learning the architect of a new communication channel. The goal of the paper is relevant for the conference, as a communication channel is directly learned and the framework somewhat works towards the bigger goal of AGI. However, it would be useful to add more real world applications of the approach. The empirical evaluation is informative, although the random building agent does not give too much additional insight. It would be good to show relevant ablations in the main part of the paper. There are differences between the frameworks and [1] might work towards HRI (as also mentioned in the paper), but the direction seems quite related","This paper proposes a cognitive science-inspired interaction setting between two agents, an ""architect"" and ""builder"", in which the architect must produce messages to guide the builder to achieve a task. Unlike other related settings (such as typical approaches in MARL, HRL, or HRI), the builder does not have access to the architect's reward function, and must learn to interpret the architect's messages by assuming the architect is telling it to do something sensible. At the same time, the architect determines what is ""sensible"" by building a model of the builder's behavior and planning over it. This setting is common particularly in human-agent interactions, where humans may not",0.8266703486442566
"The main contribution is that they propose that classification performance mostly results from iterated phase collapses. The theoretical analysis is nice, and not to hard to follow, once one commits. In particular, I find the results in Table 2 very convincing. The theoretical analysis is nice, and not to hard to follow, once one commits. It would be interesting to make more explicit the analysis for different types of nonlinearities.","This paper proposes that the superior performance of modern convolutional networks is partly due to a phase collapse mechanism that eliminates spatial variability while ensuring linear class separation. To support their hypothesis, authors introduce a complex-valued convolutional network (called Learned Scattering network) which includes a phase collapse on the output of its wavelet filters and show that such network has comparable performance to ResNets but its performance degrades if the phase collapse is replaced by a threshold operator. Reviewers are all in agreement about the novelty and significance of the work. They also find the empirical results compelling. The main weakness of this work which was highlighted by all reviewers is clarity. The paper can be significantly improved in terms of the",0.8463980555534363
"The goal of augmentation invariance is well motivated in speech recognition, and ML/ representation learning. The paper is well written with the following strengths. The proposed method is self-supervised learning with the teacher-student framework that is an extension to Mean Teacher (MT) and Boost Your Own Latent (BYOL). The proposed method is a novel one that aims to learn the representation denoising of perturbation data with the teacher-student framework. In addition, the proposed method can also be combined with multi-condition training to improve the noise-robustness.","This paper proposed a self-supervised speech pre-training approach, by the name of SPIRAL, to learning perturbation-invariant representations in a teacher-student setting. The authors introduced a variety of techniques to improve the performance and stabilize the training. Compared to the popular unsupervised learning model wav2vec 2.0, better WERs were reported using SPIRAL with a reduced training cost. All reviewers considered the work solid with sufficient novelty but also raised concerns regarding the generalization under unseen real-world noisy conditions and missing decoding details. The authors responded with new Chime-3 results and updated LM decoding results. The new results show that, after a bug fix, SPIRAL can outperform wav2",0.8472395539283752
"The proposed method provides some useful results, but there are some questions needed to be answered. This is not comprehensive to show the advantages of the proposed method in the imbalanced classification. The problem is very important. The solution is novel from theoretical perspective, and it is evident in the experiments as well. The competing methods does not only contain previous published work but also include its variants to improve the performance, which make the proposed method further attractive.","To solve imbalance classification problem, this paper proposes a method to learn example weights together with the parameters of a neural network. The authors proposed a novel mechanism of learning with a constraint, which allows accurate training of the weights and model at the same time. Then they combined this new learning mechanism and the method by Hu et al. (2019), and demonstrated its usefulness in extensive experiments. I would like to thank the authors for their detailed feedback to the initial reviews, which clarified most of the unclear points in the manuscript. Overall the paper is well written and the effectiveness was demonstrated in experiments. Since the contribution is valuable to ICLR2022, I suggest its acceptance.",0.8509044051170349
"The paper provides a comparison to recent methods on IS, FVD, and KVD evaluation metrics. This helps to understand better the performance of the introduced model on video generation.","This work tackles video generation using implicit representations, and demonstrates that using these representations enables improvements to long-term coherence of the generated videos. Reviewers praised the writing, the thorough experimental evaluation, and the strong quantitative results. Some concerns were raised about a lack of discussion of relevant related work, novelty/significance, model architecture, and a lack of qualitative examples, many of which the authors have tried to address during the discussion phase. Several reviewers raised their ratings as a result. Personally I certainly believe that exploring implicit representations for video is important, and I know of no published prior work in this direction, which amplifies the potential significance of this work. Even if results are qualitatively worse than previous work",0.8522640466690063
"The method seems relatively well motivated, and the experiments seem well executed. The paper could use more editing. There are various typos (some below) and many phrasings that could be improved. The lack of human-in-the-loop baselines seems potentially problematic. The same could be said about the environment – given how promising this method seems to be in that context. However, there is one missing experimental comparison that I would like to see before I can recommend an accept.","The paper presents a new algorithm for augmenting RL training with human examples, and this is applied to learning safe driving policies. This algorithm is properly tested and compared to other relevant algorithms with favorable results. Reviewers agree that this is good work and that it should be published. Reviewers had multiple questions, which were in my opinion answered satisfactorily by the authors. Notably, the authors ran additional tests against other human-in-the-loop RL algorithms with good results. In sum, this seems to be a solid paper, worth accepting.",0.856212854385376
"This paper proposes a technique called cross-lingual manifold mixup or X-mixup, which has been inspired by mixup (Zhang et al.,Footnote and Filter (Fang et al., 2020)). The general idea is to combine the hidden representations corresponding to the source-language input with the hidden representations corresponding to the target-language input (in a smarter way than concatenating them). The authors combine this with a specific mixup ratio based on translation quality and 2) scheduled sampling. They perform multiple experiments using two pretrained models (mBERT and XLM-R) and show that their proposed approach","This paper proposes X-Mixup, a model that considers the source languages and target languages together for cross-lingual transfer. The designed model takes a pair of sentences (or the translated sentences) in a source language and a target language as the input and computes the cross-attention between them. The empirical results are convincing. Reviewers think this paper is well-written and the idea is interesting.",0.8589601516723633
"This paper investigates the applicability to clustering methods for selection of policies in evolutionary strategies. The authors present a convincing method that performs well on several toy problems. This is an interesting area of study and the authors present a convincing method that performs well on several toy problems. I would be interested to see how well the clustering algorithm performs at identifying behaviours – for example, a demonstration that policies sampled from the clusters do indeed lead to diverse behaviours when examining those policies that were ranked highly but distinct. The paper is well-written with a clear goal developing a method that optimises for diversity in addition to fitness. The main contributions include sampling policies and","This paper introduces a novel quality-diversity algorithm, ""Evolutionary Diversity Optimization with Clustering-based Selection (EDO-CS)"", and applies it to reinforcement learning. A bandit approach (UCB) is used to select which cluster to sample parents from. The QD algorithm can be evaluated on its own, outside of the RL context, and if so it should be compared to the several approaches to niching and other standard diversity preservation approaches in evolutionary computation that rely on clustering. (And the authors should make an effort to connect to the niching literature in particular.) However, the use of the algorithm for RL makes it possible to use behavioral features as the space in which to",0.8365107774734497
"This paper proposes a dynamic scale convolution for learning-based multi-view stereo. The proposed method uses curvature information to select the patch scale. Although the method does not use deep learning, it limits the technical contribution of this paper. In the related work section, this paper claims that the proposed method should have an advantage of performance compared with Xu et al. (2020) because they do not rely on the handcrafting features. However, the claim was not confirmed through the experiments. The authors added a comparison of the proposed method with Xu et al. (2020). Overall, the paper is relatively well structured, each section is of",All reviewers recommended accept after discussion. I am happy to accept this paper.,0.8319023847579956
"This paper proposes a variance aware approach for offline RL, which leads to the pessimistic value estimation. The paper exploits the reweighted Bellman update, which leads to the improved bounds.","In this paper, the authors motivate the paper well by the gap between the upper bound of the popular offline RL algorithm and the lower bound of the offline RL. By exploiting the special linear structure, the authors designed a variance-aware pessimistic value iteration, in which the variance estimation is used for reweighting the Bellman loss. Finally, the upper bound of the proposed algorithm in terms of the algorithm quantity is proposed, which is more refined to reflexing the problem-dependency. These results are interesting to the offline RL theoretical community. As the reviewers suggested, several improvements can be made to further refinement, e. g., - The intuition about the self-normalization in the algorithm exploited",0.8786583542823792
"This paper introduces several decomposition methods and makes a comprehensive comparison among them from the perspective of compressing Transformer layers. The motivation is clear and the methods are technically sound. The experimental results demonstrate the effectiveness of the method. The authors do not include embedding layer and prediction layer size in experiments, while only report the Transformer encoder size.",This paper reviews a number of parameter decomposition methods for BERT style contextual embedding models. The authors argue for the application of Tucker decomposition to the attention and feedforward layers of such models. Evaluation is performed for a range of models on the GLUE benchmark. Further ablation studies indicate that the distillation procedure employed is crucial for obtaining competitive results and the raw decomposition approaches are ineffective at directly approximating the original pre-trained model. Strengths: The reviewers generally agree that the methods explored and results presented in this paper are interesting and could be of use to those deploying large embedding models. The authors review a range of possible decomposition methods and use this to motivate their approach. The resulting levels compression are high,0.8609800338745117
"The presentation of the method and the intuition behind it is clear and easy to follow. The main strength of the regularization method introduced in this work is in its simplicity. The authors mentioned that ""Compared to these methods, the CLOP layer offers a direct and easy way to augment the RL agent’s ability to generalize to unseen environments. The combination of the CLOP layer and the previous approach could potentially be quite promising. There is no experiment that demonstrates whether CLOP can actually bring benefits to the previous approaches.","This paper presents a novel regularization technique for CNNs based on swapping feature vectors in the final layer. It is demonstrated that this simple technique helps with generalization in supervised learning and RL with image inputs. Following the author rebuttal, all reviewers agreed that the simplicity of this method and the nice empirical performance it obtains is important to report to the community. In this respect, I agree with the reviewers, and recommend acceptance. One important issue that came up during the discussion is how much this work is related to RL, and the authors SL experiments helped to put the contribution in a broader context. Indeed, one way to see the results of this work is that if such performance improvement is obtained in the",0.8563151955604553
"The paper presents strong results for some cases like rare classes in LVIS dataset. The authors have provided clear motivations and intuitions behind most of their design decisions. The diagrams and figures also assist in understanding the paper better. However, there are still several problems with the paper which need to resolved before it can be considered ready for acceptance. The performance on non-rare classes and the complete test sets is significantly lower than prior works even though the performance on rare classes is slightly better in many cases. This raises questions about the utility of the proposed approach. The authors should clarify that the CLIP model in ViLD-image and the","the aim of this work is to produce an open-vocabulary detector. The approach is via knowledge distillation from existing large-scale V+L models, and the evaluation is based on novel classes with LVIS. The reviewers were generally happy with the work (approach and results), but there were substantial points of clarification during discussion that need to be properly integrated into the final manuscript.",0.85005784034729
"This paper is motivated by an inherent trade-off between exploiting the data in the offline dataset, and exploring out-of-distribution states (by means of a model). The main part of the paper revolves around how to meta-learn a policy that can act as a regularizer in a policy-improvement step. The authors propose to regularize policy-improvement updates both towards the meta-learned policy and towards the behaviour policy implicitly defined in the offline dataset. Overall, while I think the presentation of the method can be made much simpler and clearer, I believe this paper presents interesting findings for offline RL and has a strong proposal for","**Summary** This paper proposes a novel offline model-based meta-RL approach called MerPO. MerPO combines conservative value iteration with proximal RL policy iteration. The proposed method is novel despite having some similarities to approaches like COMBO. The paper compares against it in the experiments. The paper provides both empirical and theoretical justification for the proposed approach. **Final Thoughts** Overall, I think the authors did a pretty good job at addressing the reviewers' concerns. Overall, I think this is an interesting contribution to the ICLR community. The reviewers were all positive about this paper. For the camera-ready version of the paper, I would recommend the authors to go over the reviewers' concerns again and",0.8511020541191101
"The approach is interesting, somewhat novel, and easily applicable to a broad range of deep nets. The provided theorems are sufficient for supporting the method and correct. The derivations pertaining to posterior robustness are correct and result in efficient algorithms. The experiments are broad and varied, covering a wide selection of possible practical challenges.","This paper presents a new formulation for the infinitely wide limiting case of deep networks as Gaussian processes, i. e. NNGPs. The authors extend the existing case to incorporate a scale term at the penultimate layer of the network, which results in a scale mixture of NNGPs or a Student-t process in a specific case. This formulation allows for a more heavy tailed output distribution which e. g. can be more robust to outliers. The four reviews averaged just above borderline, with a 5, 8, 6, 6. The reviewers found the approach to be sensible, technically correct and timely given the recent literature. They found the experiments to be compelling for the most part,",0.8387442827224731
"The paper is well-written and enjoyable to read. There is a good mix of theory and intuition. The mathematics is written clearly and precisely. The basic observation around angle contraction in random convolution layers, while possibly implicit in some previous works, does not seem to have been presented systematically as the authors do so here. The way the authors study this issue, using tight lower and upper bounds for the new map with the ReLU activation, is sufficiently systematic to give a good entry point to understand various interesting aspects of this difference. I anticipate this will be useful for follow-up work. The issue is placed well in context via discussion",This paper focuses on understanding how the angle between two inputs change as they are propagated in a randomly-initialized convolutional neural network layers. They demonstrate very different behavior in different settings and provide rigorous measure concentration results. The reviewers thought the paper is well written and easy to read with nice theoretical results. They did raise a variety of technical concerns that were mostly addressed by the authors rebuttal. My own reading of the paper is that this is a nice contribution. I therefore agree with the reviewers and recommend acceptance.,0.8547251224517822
"The proposed method is novel, and shows very strong empirical results. The submission is quite clear; methods (and the model architecture) are proposed clearly and concisely. The results are clearly displayed in tables in figures and are clearly described in the text. The authors failed to consider the model on an important subset of tasks that, in my opinion, would benefit most from this procedure. The proposed models are straightforward and not overly complex, with a proper diagram easily describing the training procedure, thus making the methods reproducible. The findings are relevant and important particularly for those interested in what the authors describe as ""one-to-many"" generation tasks","The authors study the problem of open-ended knowledge-grounded natural language generation, in the context of free-form QA or knowledge-grounded dialogue, focusing on improving the retrieval component of the retrieval-augmented system. By retrieving more relevant passages, the generations are more grounded in retrieved passages. Pros: + The paper is clearly written and motivated. + Presents a straightforward approach that shows improvement over a strong baseline. + A strong paper focuses on a rather under-explored problem of knowledge-grounded open-ended generation, proposing novel objective, significant empirical improvements on two datasets in multiple metrics. + The authors included human evolution results to support their findings. + The authors did",0.8442468643188477
The paper proposes two graph structures to model natural geometry of EEG sensors and dynamic brain connectivity. It compares the two results and shows that correlation based structure (dynamic brain connectivity) performed better than the one based on distance.,"This work tackles an important clinical application. It is experimentally solid and investigates novel deep learning methodologies in a convincing way. For these reasons, this work is endorsed for publication at ICLR 2022.",0.843757152557373
"This paper proposes an interesting approach to address key issues surrounding memory efficiency and weight update sematics in existing pipeline parallelization approach. The overall appears narrow and incremental in the sense that WPipe is fixing a memory efficiency bug of PipeDream-2BW relative to PipeDream-flush. In other words, WPipe is trying to deliver the best-of-both-worlds for these two existing approaches. As a result, it is not obvious how the results applicable beyond the PipeDream* line of pipeline parallelism. The presentation and clarity in the paper could be improved - the authors should re-use terminology used earlier in the pipeline","The paper proposes a new pipeline-parallel training method called WPipe. WPipe works (on a very high level) by replacing the two-buffer structure of PipeDream-2BW with a two-partition-group structure, allowing resources to be shared in a similar way to PipeDream-2BW but with less memory use and less delays in weight update propagation across stages. The 1.4x speedup it achieves over PipeDream-2BW is impressive. In discussion, the reviewers agreed that the problem WPipe tries to tackle is important and that the approach is novel and interesting. But there was significant disagreement among the reviewers as to score. A reviewer expressed concern about the work being incremental and difficult to follow",0.8552582263946533
"The paper is well organized. Although there are many technical details, which are relatively hard to follow every bit, it is due to the rigor and complexity of the theory. There is a gap between the experiments and theory. The authors train 4 different discriminators, which may not cover all of the possible ones. It would be better if the authors can conduct an example where the generator can cheat all of the discriminators. The results are novel, interesting, and non-trivial.","The provides a complexity theoretic look at GANs. The exposition is multi-disciplinary, and in my personal opinion, it is an interesting look at the GANs in the context of random number generators.",0.8531649708747864
This paper proposes to use expectile Bellman operator to interpolate between expected Bellman operator and Bellman optimality operator. The authors has conducted experiments in more than 20 environments. The proposed method's performance either surpasses SOTA methods or on-par with them. The clarity of the manuscript could be significantly improved as some of the discussions/explanations are either not precise or consistent with each other.,Most of the reviewers think this paper is clearly a valuable addition to ICLR based on the convincing theoretical analysis and extensive experimental results. Please refer to reviewers's review for more detailed discussions of the pros and cons of the paper.,0.8392317295074463
"This paper proposes an end-to-end monocular 3D detection without intermediate depth estimation, which integrates depth cues relevant for 3D detection. The main claim of the paper is that the depth-cues can help in 3D object detection from monocular images. However, the results are only shown for only one teacher model. This puts the generality of depth-cue based distillation in question. The motivation for the scene level distillation is not clear. The paper shows that the accuracy improvement compared to the baseline model mainly comes through improvement in depth and dimension prediction of the objects. This has been shown via a valid cross-","This paper received 5 quality reviews, with 3 of them rated 8, 1 rated 6, and 1 rated 5. In general, while there are minor concerns, the reviewers acknowledge the contribution of applying Knowledge distillation to the problem of monocular 3D object detection, and appreciate the SOTA performance on the KITTI validation and test sets. The AC concurs with these important contributions and recommends acceptance.",0.8421164155006409
"This paper provides empirical evaluation and theoretical analysis of the trade-off among memory consumption, accuracy loss, and time overhead. The authors did a good job choosing datasets. The choice of models is good. I appreciate that the authors included GCNII to evaluate how their method scales with depth. The paper is well written. However, I still have several questions and concerns as following: Overall Memory Savings - It's a bit misleading to report memory savings just for activations. The bottomline number should be overall memory savings including other objects such as input data. The authors should separately report memory savings of EXACT from those of AMP.","There are numerous known methods for memory reduction used in CNNs. This paper takes two such---quantization (Q) and random projection (RP)---and applies them to GNNs. This is a novelty, but I agree with the reviewers: on its own this novelty would not be ""surprising"" enough to report at ICLR. The paper further goes to show empirically that these methods, when applied to a reasonable set of datasets, do indeed produce their predicted memory reductions (unsurprising) with a small ( 0.5% ) drop in accuracy (surprising, in the sense of not being something one could predict without doing the experiment). All of the above is in",0.8357878923416138
"The paper proposes on analyzing a special setting of finding a mixed Nash equilibrium, by first observing the convergent behavior of the limiting dynamics, and then design a new algorithm via a reasonable discretization. However, the presentation and organization of this paper largely hinders the understanding of the primary contributions.","The authors first consider a mean field two player zero sum game and consider quasistatic Wasserstein gradient flow dynamics for solving the problem. The dynamics is proved to be convergent under some assumptions. Finally, the authors provide a discretization of the gradient flow and using this proposes an algorithm for solving min-max optimization problems. They use this algorithm for GAN's as the main example. Experimental results claim that the algorithm outperforms langevin gradient descent especially in high dimensionas. This paper sits right at the border. But subsequent to the author response, one of the reviewers has updated the score and seems more positive about the paper. In view of this, I am leaning towards an accept",0.8552301526069641
"The paper is well written, easy to understand with clear definitions and examples. The equations look ok and the paper explains well the complexity and how to improve the loss in the presence of partial transcripts. The experiments on three tasks look sound and good, the method is well detailed. The proposed method could be used to simply do a forced alignment of the training set with a pretrained model to ""clean"" the dataset. One would expect to see i the paper the advantages of the proposed integrated training method vs this simple approach. The authors addressed the doubts and questions raised in the review.","This paper proposes an extension of CTC by considering the wild-card to adjust the label missing issues during training. The authors propose to minimize the loss over all possible sub-segments of the input to automatically align the one that matches the available transcript. It is empirically proved to significantly improve performance over CTC even if up to 40-70% label sequence is missed (overall performance similar to the complete label case) across different tasks. As agreed by the reviewers, the paper is well presented and the problem is interesting to a broad community. Dynamic time warping with unconstrained endpoints itself is not a new idea and a classical topic for speech recognition (e. g. word spotting). The contribution of the paper is",0.8466505408287048
"In particular, I was confused by the claim that Joulani et al. (2013) proves a regret bound of O(KTlogT+KE[] where  is the delay. The paper considers only one of the main aspects: delayed rewards, and I think that this paper considers only one of the main aspects: delayed rewards. The paper is well-written with a meaningful experimental section where they empirically compare the incentive costs with regret. Thus, I suspect that fixing this bug will change the performance of the algorithm. The algorithm is realistic in terms of practical implementation. The main weakness of this paper is that the authors","This paper tackles a bandit problem that incorporates three challenges motivated by common issues encountered in online recommender systems: delayed reward, incentivized exploration, and self-reinforcing user preference. The authors propose an approach called UCB-Filtering-with-Delayed-Feedback (UCB-FDF) for this problem and provide a theoretical analysis showing that UCB-FDF achieves the optimal regret bounds. Their analysis also implies that logarithmic regret and incentive cost growth rates are achievable under this setting. These theoretical results are supported by empirical experiments, e. g. using Amazon review data. The main concern with this paper is that the considered challenges have all been tackled already in different bandit settings, so",0.8365634679794312
"This paper proposes an adaptive augmentation by learning a class-dependent and potentially instance-dependent augmentation policy for each data instance. The motivation of this work is clear and the idea of searching data-dependent augmentation policies seems reasonable. The paper is well-written and easy to follow. There are some related works this paper omitted. For example, [1] has also considered individual sample variations in augmentation policy searching and it proposed a meta-learning approach to learn the augmentation policies. This paper can compare with this in terms of the idea and performance, so that we will know which approach is better under which scenarios. The paper is well-written","Reviewers agreed that this work is well-motivated and presents a novel approach for data augmentation around the adaptive augmentation policies. There were some concerns around the lack of ablation studies and unclear performance improvements, which were addressed well by the authors’ responses. Thus, I recommend an acceptance.",0.8585033416748047
"This paper proposes a new method to extract features from images. The method builds on top of DINO (Caron et al., 2021). The features extracted by DINO are further refined with a small segmetation head that aims to boost performace by balancing Knn, self-correlation and random image correlation loss. The segmentation head is a simple feed forward network - by avioding the backbone retraining (and implicitly knn recomputation), the method is very efficient to train. The authors address this issue by five-cropping the dataset (corners+center crop), resulting in five times more images to find kn","The paper received two accept and two marginally accept recommendations. All reviewers find value in the proposed supervised semantic segmentation methodology (making self-supervised representation learning towards dense prediction tasks like segmentation or clustering without explicit manual supervision) and appreciate the experimental gains, but had (mostly practical) criticism that was reasonably well addressed in the rebuttal.",0.8228638768196106
"This paper presents a fast-kernel algorithm for the unification of existing algorithms using the Shapley-Taylor indices. The empirical contribution is their proposed fast-kernel algorithm, which has been validated through quantitative experiments. The paper is also pretty well written and easy to understand, and addresses a problem with wide applications and relatively little work. The experiments themselves do suggest their approach has merit, although lacking in several aspects in this version. The authors never showed their explanation method was human-interpretable or could be used in any downstream methods that leverage explanations. This is a critical flaw in their paper that is enough to recommend rejection on its own.","The initial reviews for this paper were 6,6,6, the authors have provided a rebuttal and after the rebuttal the recommendation stayed the same. The reviewers have reached the consensus that the paper is borderline but they have all recommended keeping it above the acceptance threshold. Following the recommendation of the reviewers, the meta reviewer recommends acceptance.",0.8362331986427307
"The paper presents interesting ideas on how to utilize the original topological structures among different domains to benefit the domain adaptation results. The idea of using a discriminator to reconstruct rather than classify is novel. Experiments are well thought out and highlight the parameter study which makes the results of the work reproducible. The authors don’t show the strict analysis to this basic assumption, that is, the optimal discriminator will output the conditional expectation of A_ij over all possible combinations of domain pairs. The authors don’t analyze why the change of the task of discriminator from classification to generation would help the model’s performance in domain","This paper proposes to leverage topological structure between domains, expressed as a graph, towards solving the domain adaptation problem. Reviewer n4Lk thought the ideas were interesting, appreciated the theoretical analysis and indicated that the experiments were “well thought out”. The reviewer asked for more detail on Lemma 4.1 and suggested that a proof be provided for Proposition 4.1. They asked for more justification on why the change of task for the discriminator from classification to generation would improve performance. The authors responded to these comments, clarifying the proof of Lemma 4.1 in the appendix. They clarified that proposition 4.1 can be derived from Corollary 4.3 or Corollary 4.4. On the point of classical vs. enhanced discri",0.8605870008468628
"The proposed approach is technically sound in general. Each part of the methodology (i. e., clustering, finding LTH, and pruning) is well motivated. The idea to leverage group convolution for kernel pruning is practical. The paper is well written and easy to follow. The empirical evaluations are not self-contained, somewhat weak, and incomplete. The performance gaintuning on CIFAR-10 can be minor compared with existing baselines, e. g., ResNet-32 has 92.82% acc, which is even worse than other baselines. The accuracies of models before pruning on TinyImageNet are apparently lower than","This paper studies structured pruning methods, called kernel-pruning in the paper which is also known as channel pruning for convolutional kernels. A simple method is proposed that primarily consists of three stages: (i) clusters the filters in a convolution layer into predefined number of groups, (ii) prune the unimportant kernels from each group, and (iii) permute remaining kernels to form a grouped convolution operation and then fine-tune the network. Although the novelty of the method is not high, it is simple and effective in experiments after the supplementary sota results in the long rebuttal. Majority of reviewers increase their ratings after the rebuttal (though one reviewer promised this but forgot to act), while some",0.8284387588500977
"This paper proposes a simple method for the decomposition of the f(s,a)*phi(s,g) into a global component that depends on goal and state and local that looks at state and action. The main experiments clearly demonstrate the usefulness of the method for the purpose of data-efficiency and transfer to unseen goals. The analysis of the method is useful, but does not provide sufficient light into what kind of decompositions are learnt. The proposed idea is simple and elegant. The benefits of the method in terms of data efficiency and transfer to new goals are clear. The paper proposes a modification to UVFAs that","This paper proposes a new bilinear decomposition for universal value functions. The bilinear network has one component dependent on state and goal and another component that depends on state and action. The experiments with the DDPG algorithm in robot simulations show that the proposed architecture improves performance data efficiency and task transfer over several baseline algorithms, including improvements on earlier bilinear decompositions. The reviews noted several aspects of the paper could be improved, and the author response addressed several of these concerns. Multiple reviewers appreciated the insights from the experiment added in section 4.5 on a simple grid environment, which enabled a direct interpretation of the vector fields used in the method. Several aspects of the presentation were clarified based on the reviewers comments. Additional",0.8610488176345825
"The paper is well-written, and has clearly very thorough experiments. The authors include 82 different models (across their 5 categories of training diversity), which include recent high-performing works. They raise a very interesting question for theory as well as empirical studies - why doesn't SGD learn the more diverse representations that are shown to perform better? The authors seem to define ""low accuracy"" models as those below the 74% accuracy (on ImageNet) threshold they use in their main analysis. However, the claim ""low accuracy models can benefit high-accuracy ensembles"" is misleading. In the current form, the claim ""low accuracy models",This well written and well motivated paper has been independently reviewed by four expert reviewers. They all voted for the acceptance with three straight accepts and one marginal. The feedback provided to authors was constructive and the authors responded comprehensively. I recommend acceptance of this work for ICLR.,0.8273398876190186
"In terms of writing, the paper is quite technical and lacks intuition. I don't think putting so many results in the main body is necessary. The presentation of the paper is non-conventional. For example, Theorem 1 is followed by many remarks. The content of these remarks may fit in one or two paragraphs and the presentation would be more elegant. The paper was written in a clear manner, with a very convincing motivation of the problem. The contributions were well outlined in the context of existing works. The significance of the theoretical results were clear, and as far as I could see, the results were correct.","The focus of the paper is kernel thinning, i. e. the extraction of a core set from a sample with good integration properties meant in MMD (maximum mean discrepancy, hence worst case) sense. Particularly, the authors propose generalizations of the kernel thinning method (Dwivedi and Mackey, 2021) which relax the assumptions imposed on the kernel (k) and the target distribution (P), and possess tighter performance guarantees. Designing compressed representation of samples for integration is a fundamental problem in machine learning and statistics with a large number of successful applications. As assessed by the reviewers, the authors deliver important new theoretical insights in the area which can be also of clear practical interest. They also pointed",0.824536919593811
"The paper revisits a broad range of vision and language tasks (VQA, VLN, SNLI-VE, Image Captioning) and evaluates the would-be performance for previous state-of-the-art methods by replacing their visual backbones with CLIP-ResNet/ViT. The proposed method is able to establish new state-of-the-art performance in multiple vision-and-language tasks, which can benefit future research in the related fields.","Reviewers are in agreement that this work is a useful, clear, documentary piece of work that shows the utility of CLIP on a number of popular V+L tasks. There is a somewhat persistent concern that simply demonstrating that a stronger visual encoder leads to improvements downstream is not an insightful result on which the community can build.",0.8507151007652283
"The paper is well-written and very clear. Intrepid is given so as to help the reader dissect the results. In particular, a number of the findings & discussions in Lewkowycz, et al. (2020) have some overlap with the results proven here but are not cited. The presentation of the paper could be improved. The authors could provide much more experimental support. To me it seems that the only experiments which document that larger step size leads to more balancing are provided in Figure 4. However, I feel that much more extensive experiments are needed. (A larger choice of different step sizes and also a","The paper studies gradient descent for matrix factorization with a learning rate that is large relative to the a certain notion of the scale of the problem. In particular, they show that the use of large learning rates leads to balancing between the two factors in the factorization. The discussion between the authors and the reviewers was fruitful in dispelling some of the reviewers' doubts and at the same time improving the paper. The paper seems to make some contribution on a relevant problem for the ICLR community. However, even in the restricted settings they consider, the problem does not appear to be completely solved. That said, I agree with the majority of the reviewers that the step forward seems enough to warrant the acceptance",0.8408809900283813
"This paper studies an important problem and provides valuable insights into the mysterious fact of the very low target transferability between ASR models. The paper is also well-written and nicely structured. The method of how the authors come up with the factors is not explained. The definition of a “factor” is not clear and is not clearly defined in this paper. The paper provides some discussion in each ablation study. However, the current discussion does not go beyond describing the observations of the experimental results. This is not an easy task, where additional small-scale experiments may also be required as the authors generate hypotheses. The paper can benefit from giving","This paper explores why adversarial examples do not transfer well in adversarial examples on automatic speech recognition systems. The authors propose a number of potential causes that are then quickly evaluated in turn. This could be an excellent paper, but in its current form, it is borderline. The main problem with the paper is that it proposes a number of causes for the limited transferability, and then evaluates each of them with one quick experiment and just a paragraph of text. In particular, none of the results actually convince me that the claim is definitely correct, and many of the experimental setups are confusing or would have other explanations other than the one variable that is aiming to be controlled for. That said, even with these weaknesses",0.8525433540344238
"This paper proposes a distributed GCN training on large graphs named PipeGCN. The idea seems simple and straightforward, and experiments show that the algorithm can achieve up to 2.2x speedup. The authors provide the convergence proof for PipeGCN and propose two smoothing methods for faster convergence. The influence of the smoothing technique can be better analyzed. In Table 4, there is not a consistent winner among PipeGCN, PipeGCN-G and PipeGCG-F in model accuracy. The paper provides novel theoretical proof of the convergence of GCN training with stale feature and feature gradients, which is useful for future work. The paper","The paper proposes PipeGCN, a system that uses pipeline parallelism to accelerate distributed training of large-scale graph convolutional neural networks. Like some pipeline-parallel methods (but unlike others), PipeGCN involves asynchrony in the sense that its features and feature-gradients can be stale. The paper provides theoretical guarantees on the convergence of PipeGCN in the presence of this staleness, which is a nice contribution in itself. In discussion, the reviewers found the work to be well-executed and sound. All reviewers recommended acceptance, and I concur with this consensus.",0.8640709519386292
"This paper presents a new neural network algorithm with a linear regret scaling for the neural network function class under the NTK setting. The experiments show some promise in real world experiments especially because of the predictably better running times than NeuralUCB. The main concerns are that the paper fails to discus and compare with the line of work in FALCON. It would be good if the algorithms are just compared over a fixed set of 10, 20, 50 and 100 arms each time. The reference to definition of effective dimension is broken in page 5. It would be great if we can add a full information algorithm (algorithm that sees","This paper studies the design and analysis of contextual bandits algorithms, combining the ideas of neural network models (Zhou et al, 2020 and Zhang et al, 2020) and reward perturbations (Kveton et al, 2019, 2020); this has the computational advantage of avoiding inverting large covariance matrices, as is done in the other neural contextual bandits algorithms. Although the reviewers think that the papers need to do a better job in highlighting differences and extra challenges in the current work compared to prior works, they also acknowledge that this paper is the first that combines the above two ideas. The reviewers also acknowledge that the additional experiments in the rebuttal period help clear the concern the reviewers have about why all regret curves",0.8303241729736328
"This paper presents a new method for detecting hidden trigger backdoor attacks. The proposed algorithm is well-organized and easy to read. Overall, this paper is well-organized and easy to read. The comparison with other baselines also demonstrates superiority, which really convinces me. (2) This research also includes some theoretical analysis, which is actually the missing part in current studies on backdoor attacks. The reviewer is confused with the implementation and the efficiency, since the update of I-BAU in Eqn. (4) requires an inverse of a second-order derivative, which may not be supported directly by PyTorch/Tensorflow.","This paper investigates defense against backdoor attacks for models that have already been trained. It proposes, in particular, a min-max formulation for backdoor defense, in which the inner maximum seeks a powerful trigger that leads to a high loss, while the outer minimum seeks to suppress the ""adversarial loss"", so as to unlearn the injected backdoor behaviors. To solve the minimax, the authors also propose a method, Implicit Backdoor Adversarial Unlearning (I-BAU). In addition, the authors also provide theoretical analysis including the convergence bound and generalization bound. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method. The proposed method is interesting and the implementation is nice. Overall",0.8459571599960327
"This paper proposes an ensemble-based deep RL algorithm that learns similar representations at different time points in the training phase. The results are not clearly separatable. Many lines are hard to see their statistical importance especially only 5 runs were performed. I am not so convinced that the performance of ensemble-based deep RL methods highly (negatively) correlates with the representation similarity. The paper provides many experimental results with different regularizers, baseline algorithms, and the number of ensembles. I highly appreciate that the paper provides many experimental results with different regularizers, baseline algorithms, and the number of ensembles.","This paper concerns ensemble methods in deep reinforcement learning, examining several such methods, and proposes to address an important issue wherein ensemble members converge on a representation of approximately the same function, either by their parameters converging to an identical point or equivalent points that give rise to the same function. The authors propose a set of regularization methods aimed at improving diversity, and benchmark these augmentations on five ensemble methods and a dozen environments. 3 of 4 reviewers generally praised the method's simplicity and generality, and found the experiments convincing. Reviewer a9sA describes it as ""clearly written and easy to follow"", although others found clarity lacking in parts. There was agreement among these 3 reviewers that this",0.8411794304847717
"The proposed structure and way to generate positional embeddings demonstrated good results on several datasets, however, it still lacks a thorough justification of where the improvement comes from and how each component contributes to the final quality. The paper proposes two major components: random walk position embeddings and a standalone feed-forward network to update the embeddings. While I think these two strategies are a good add-on to many basic GNN structures, I do feel there are some points that the first part is of limited novelty, and some points are not properly justified in the arguments and experiments.","This work adds the positional encoding (akin to those in transformers, but adapted) to GNNs. In their reviews, reviewers raised a number of concerns about this work, in particular, lack of novelty, lack of ablations to demonstrate the claims of the paper, lack of comparison to previous work (e. g., position-aware GNNS, Graphormer and GraphiT which would appear very related to this work), lack of motivation (e. g., the introduced positional loss do not actually improve performance), and whether the experimental results were really significant. During the rebuttal, the authors replied to the reviews, to address. the concerns that they could. Of the reviewers",0.8357363343238831
"The proposed algorithm yields high-quality reconstructions. The rationale behind designing such an algorithm is very well-motivated by the authors. The idea of self validation is also interesting since it mimics the presence of a validation set for training. The robustness investigations are appealing and very related to the problem considered. The paper states several times that ZS-SSL-TL significantly reduces convergence time. Yet what is missing is a computational comparison among single-instance reconstruction methods such as traditional sparsity-based, DIP-based, ZS-SSL, ZS-SSL-TL. This is important since one of the critical challenges of single","The paper considers the problem of accelerated magnetic resonance imaging where the goal is to reconstruct an image from undersampled measurements. The paper proposes a zero-shot self-supervised learning approach for accelerated deep learning based magnetic resonance imaging. The approach partitions the measurements from a single scan into two disjoint sets, one set is used for self-supervised learning, and one set is used to perform validation, specifically to select a regularization parameter. The set that is used for self-supervised learning is then again split into two different sets, and a network is trained to predict the frequencies from one set based on the frequencies in the other set. This enables accelerated MRI without any training data. The paper evaluates",0.8268300294876099
"This paper addresses the problem of how to get certified in reinforcement learning (RL) problems. The method that the authors propose brings the gap between randomized smoothing technique and RL. The proposed method is intuitive and the experimental results are convincing. A major competitor of this paper is Zhang et al., NeurIPS 2020, which reduced the significance of contribution. The empirical results seem lacking. If policy smoothing is a robustness, this needs to be clearly stated in both the problem setup and the threat model. The theoretical treatment of the problem is sound, and clearly stated. This does not seem to be the case, as numerous works in the literature","The reviewers appreciated the treatment of the topic of certifiable robustness done in this work and although they had a number of concerns, I feel they were adequately addressed by the authors.",0.8453249931335449
This paper proposes a framework for self-supervised visual representations via meta-learning. The presented meta-learning framework for self-supervised learning improves over the SimCLR in downstream tasks. This shows the potential of using meta-learning for self-supervised visual representation learning. The paper is quite well written and easy to understand and follow. Overall the paper is quite well written and easy to understand and follow.,"This paper was borderline, based on the reviews. The paper points out an interesting connection (somewhat known but not in this specific version) and good experimental results. However, numerous reviewers raised concerns that the paper was lacking a comparison to prior work connecting unsupervised learning and meta-learning, most notably, Hsu et al. (2019). After reading the revised version of the paper, the authors address this issue and also all the other reviewer comments. In relation to prior work they clarify that they focus on the contrastive unsupervised case and also do a good job in answering other reviewer concerns relative to novelty and results. I would also like to point out, as reviewers also did that the previous title was a bit",0.8298763632774353
This paper proposes a method for decomposing the excess risk into the variance of neural networks. The paper proposes that neural networks converge fast when fitting signal but converge relatively slowly when fitting noise. The reviewers have some concerns about the paper. The authors should clarify what the setting is considered in the non-linear decomposition regime. The reviewers have some concerns about the paper. The authors should clarify what the setting is considered in the non-linear decomposition regime. The reviewers have some concerns about the paper. The reviewers have some concerns about the paper. The authors should clarify what the setting is considered in the non-linear decomposition regime. The reviewers have,"The main contribution is a way of analyzing the generalization error of neural nets by breaking it down into bias and variance components, and using separate principles to analyze each of the two components. The submission first proves rigorous generalization bounds for overparameterized linear regression (motivated in a general sense by the NTK); there are settings where this improves upon existing bounds. It extends the case to a matrix recovery model, showing that it's not limited to the linear regime. Finally, experimental results show that the risk decomposition holds empirically for neural nets. The numerical scores would place this paper slightly below the cutoff. The reviewers feel that the paper is well written and have not identified anything that looks like a critical flaw",0.8319749236106873
"All reviewers agree that the paper is well written and the empirical comparisons are extensive. The idea of NWR is seems novel and useful to me. The notation is a bit cumbersome with many subscripts and superscripts that are difficult to keep track of. Some of the choices in matching neighborhoods seem a bit arbitrary and not sufficiently justified (see questions below), although this is somewhat understandable as there are large number of tricks used in this paper. Limited theoretical contribution, with some small inconsistencies.","The paper proposes a novel approach to graph representation learning. In particular, a graph auto-encoder is proposed that aims to better capture the topological structure by utilising a neighbourhood reconstruction and a degree reconstruction objective. An optimal-transport based objective is proposed for the neighbourhood reconstruction that optimises the 2-Wasserstein distance between the decoded distribution and an empirical estimate of the neighbourhood distribution. An extensive experimental analysis is performed, highlighting the benefits of the proposed approach on a range of synthetic datasets to capture structure information. The experimental results also highlight its robustness across 9 different real-world graph datasets (ranging from proximity-oriented to structure-oriented datasets). Strengths: - The problem studied is well motivated and the",0.8231309652328491
"This paper proposes an approach for calibrating model scores without access to sensitive attributes. The authors conduct experiments to demonstrate the effectiveness of FairCal and compare it to previous works. The authors show that FairCal achieves better overall accuracy, false positive rate and fairness calibration compared to other calibration methods, including ones that do have access to sensitive attributes. The authors are suggested to add results related to skin tone and gender bias on IJBC, which contains gender and skin tone labels.","All reviewers agree that the presented approach to fair calibration of face verification models is interesting and needed in the field. The method does not require access to sensitive attributes for calibrating, which makes it sustainable. The reviewers are satisfied with the presented experimental studies in most cases. The rebuttal addressed a large majority of additionally raised questions. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a poster.",0.860066294670105
"The paper provides some nice insights, especially treating the problem of cross-lingual transfer as largely a domain transfer problem. The empirical gains, although not huge, do demonstrate the alignment between the research hypothesis and empirical scores. The main strength of the paper is Section 2 which delves deeper into analysing factors that affect cross-lingual transfer. The ideas of having more invariant representations to improve cross-lingual transfer are not new, and they date back to work on cross-lingual word embeddings (e. g.)","The paper presents a domain adaptation approach based on the importance weighting for unsupervised cross-lingual learning. The paper first analyzes factors that affect cross-lingual transfer and finds that the cross-lingual transfer performance is strongly correlated with feature representation alignments as well as the distributional shift in class priors between the source and the target. Then the paper designs an approach based on the observations. Pros: + The paper is well written and the proposed approach is well motivated. + The analysis about which factors affect cross-lingual transfer is interesting and provides some great insight. Cons: - As the reviewer pointed out, the experiments for verifying the proposed approach are relatively weak. Overall, the paper presents nice insights to",0.8640234470367432
"This paper is well written and well-executed. Experiments are well-designed to illustrate the claimed contributions. Overall, the method is only demonstrated on a simple synthetic dataset with two attributes (mass and charge), while mass only has two values (heavy vs light). All objects in a video in the datasets are of interest; there are no irrelevant objects. The proposed method seems to struggle in the following scenario: If A and B are attracted, and B and C are attracted, A and C should be repelled. The paper doesn't motivate the problem, and I am struggling to see any direct application.","This paper proposes a new dataset called ComPhy to evaluate the ability of models to infer physical properties of objects and to reason about their interactions given these physical properties. The paper also presents an oracle model (named oracle because it requires gold property labels at training time) that is modular and carefully hand designed, but shows considerable improvement over a series of baselines. The reviewers for this submission had several concerns including: (a) [VByS] ""concerns are about the complexity that the proposed method can handle"" (b) [VByS] ""the method is only demonstrated on a simple synthetic dataset"" (c) [8BUA] ""I am struggling to",0.8393656611442566
"The paper proposes an interesting direction of estimating ""instance-based"" noise transition matrix, rather than estimating a class-based transition matrix. The presentation and the writing of the actual method can be improved to make it easier for the reader to understand different steps of the pipeline. The main concern is the assumption of ""good"" network outputs to estimate the PTM. The network is known to overfit the noisy labels, and this assumption may not always work well, especially if the noise ratio is high. The experiments would also benefit from additional baselines.","To tackle the problem of classification under input-dependent noise, the authors proposed the posterior transition matrix (PTM) to achieve statistically consistent classification. Specifically the information fusion approach was developed to fine-tune the noise transition matrix. Experiments demonstrated the effectiveness of the proposed approach. I would like to thank the authors for the detailed feedback to the initial reviews and also further feedback to the reviewers' additional questions. Many concerns were clarified by the feedback, and the additional experiments still demonstrate the effectiveness of the proposed method. The issue of data augmentation still remains, which should be at least experimentally investigated, but the contribution of the current manuscript is still valuable to be presented as ICLR2022.",0.8559133410453796
"The paper is well-written and solid in terms of empirical studies. The authors explicitly summarized some of the most useful design choices of the best models. The paper addresses arguably one of the main challenges in current neural architecture search spaces. While I do not think that it effects the popular argument for the DARTS based search spaces, I do think they are sufficiently reliable. First, the surrogate models has been shown to provide reliable predictions and, second, the paper shows the same results emerge on a subset of architectures trained on the original data. I think the paper has a high chance to impact the future development of new search spaces.","This paper makes the important, albeit somewhat unsurprising, finding, that cell-based NAS search spaces, and in particular the DARTS search space, include some operations that are much better than others. Reducing the search space to these allows even random architectures to yield good performance, similarly to the findings of ""Designing Network Design Spaces"", https://openaccess. thecvf. com/content_CVPR_2020/html/Radosavovic_Designing_Network_Design_Spaces_CVPR_2020_paper. html This paper received mostly positive scores (5,6,6,8). While I agree with the negative reviewer that it would be good to study this on other benchmarks as",0.8313108086585999
This paper makes solid theoretical contributions on analyzing the initial conditioning of feedforward networks without shortcuts. Most of the analysis is derived directly from the concurrent work (DKS),"This paper seeks to find an answer to some quite interesting research question: can deep vanilla networks without skip connections or normalization layers be trained as fast and accurately as ResNets? In this regard, the authors extend Deep Kernel Shaping and show that a vanilla network with leaky RELU-family activations can match the performance of a deep residual network. Four reviewers unanimously suggested acceptance of the paper. There were concerns about the clarity or marginal performance improvement. However, they all including myself agree: achieving the competitive performance with the vanilla deep model itself can be seen as a big contribution and the clarity has been improved to some extent through revision.",0.8413414359092712
The paper provides further insight into possible optima that can be obtained by VAEs during training. The authors do not explore whether the two-stage VAE proposed by Dai & Wipf remedies any of the problems posed in this paper.,"The paper analyzes the behavior of VAEs in modeling data lying on a low dimensional manifold. It formally proves some of the conjectures/informal-statements in an earlier work by Dai and Wipf (2019) in the case of linear VAE and linear manifold, and disproves the same for the nonlinear case. In particular, it proves, by analyzing the objective and its gradient-flow dynamics, that VAE captures the intrinsic dimension of data distribution correctly. For nonlinear cases, the paper shows a counterexample to the conjecture in (Dai & Wipf; 2019) where the support of VAE generators is a superset of that of data distribution. Two of the reviewers had raised following specific",0.8499662280082703
"The paper is well-written and easy to follow. The method section provides helpful intuitions behind the algorithm design. The idea is novel, straightforward, well motivated, and easy to implement for large impact on model training. The experiments are very thorough, demonstrating gains across different settings, and showing that the technique achieves its goal. The paper is extremely clearly and carefully written. The scaling formula seems somewhat ad hoc and hard to characterize. There are some hints that the gains might be partly related to regularization. It would have been good to test the combination with dropout or something similar. In particular, if the sensitivity of some parameter variation","The paper observes that the number of redundant parameters is a function of the training procedure and proposes a training strategy that encourages all parameters in the model to be trained sufficiently and become useful. The method adaptively adjusts the learning rate for each individual parameter according to its sensitivity (a proxy for the parameter's contribution to the model performance). The approach encourages the use of under fitted parameters while preventing overfitting in the well-fitted ones. Experimental results are presented covering a wide range of tasks and in combination with several optimisers, showing improvements in model generalization. The paper is very well written and easy to follow (as mentioned by Reviewers NSqH, 4pzE and sSHP). The",0.8483006954193115
"This paper proposes a novel method for iterative SDE denoising for guided stroke sketch input and perturbed noise output. The paper has compared with a few state-of-the-art baselines and demosntrate its effectiveness in producing more faithful editing results. The proposed method is able to produce a diverse set of predictions given user input by sampling different Gaussian noise in the process. The paper is well written and easy to follow. The main technique in the paper (The SDE process) is previous method (Song et. al. 2020, 2021), the paper only proposed to use the human edited image as of SDE at a","Thank you for your submission to ICLR. This paper presents a technique for image synthesis based on stochastic differential equations and a diffusion model. This looks to be a very nice idea with good results. After discussion, the reviewers converged and all agreed that the paper is ready for publication---the most negative reviewer raised their score after the author rebuttal, from a weak reject to weak accept. The rebuttal clearly and concisely addressed several concerns of the reviewers. I'm happy to recommend accepting the paper.",0.8352465629577637
"This work raises very important and timely concerns with regard to using NNs for critical application domains like health care. The scope of this work is very extensive: 2 synthetic and 1 empirical dataset, 3 different spurious signals, 3 (considering the different variants of feature attribution algorithms 6) different post-hoc methods. The authors did not report the K-SSD, CCM, FAM, and KS-test values in Section 4, which makes some discussions hard to follow. In Table 1,2, FAM values are always much higher than CCM, which means spurious model and normal model generate more similar explanations on spurious inputs than normal inputs. It would be","This paper demonstrates that current post-hoc methods to explain black-box models are not robust to spurious signals based on three metrics especially when the spurious signals are implicit or unknown. Technical novelty is limited because the paper presents primarily empirical results instead of novel machine learning techniques. However, the problem is very important and timely, and significance to the field and potential impact of the presented results to advance the field are high as reviewers emphasized. There are ways to further improve the paper, including the clarity of presentation, although the authors improved in the revised manuscript. Overall, this paper deserves borderline acceptance.",0.8474876284599304
This work proposes a partioning criteria based on gradient-matching score and formulate the splitting as a graph clustering problem. The proposed method also demonstrates consistent performance gains over its counter-part algorithms. There are no ablation studies on the performance changes without considering search cost. It might serve as an important indicator to help understand how much extent the weight-sharing affects the sub-network evaluation accuracy and might also provide some hints for my last question. I am willing to see more theoretical (or at least experimental) justification/analysis on this part. I appreciate the efforts made by this paper on alleviating the weight-sharing problem in one,All reviewers give acceptance scores. One reviewer also commented that they would like to increase their score from 6 to 7 (which isn't possible in the system). I encourage the authors to add the substantial new results generated during the rebuttal into the paper.,0.8309618830680847
"This paper explores and evaluates various random pruning methods with different layer-wise sparsity ratios. The findings of the gradient norm seem consistent and correct, making good connections with previous works. The paper also assesses random pruning from other perspectives for broader evaluation.","### Summary This paper builds on previous work on sparse training that shows the many modern sparse training techniques do no better than a random pruning technique that selects layer wise rations, but otherwise randomly selects which weights within a layer to remove. The key difference in this work is to take these existing results and scale the size of the network to show that as the size of the network increases, the smaller -- as measured in pruning ratio -- a matching subnetwork becomes. ### Discussion #### Strengths Places an emphasis on simple techniques #### Weakness Significant overlap with previous work. Prior already demonstrated the equivalence of random pruning and contemporary pruning at initialization techniques. ### Recommendation I recommend Accept (poster). However,",0.8425572514533997
"This paper presents a novel application of NAT to a multilingual setting, with an interesting new strategy for codeswitched back-translation. There are two new things in this paper: the use of NAT to a multilingual setting, and use of codeswitched backtranslation. It would have been interesting to tease them apart using NAT architectures other than GLAT. The NAT baseline used in the paper is not competitive. This is hard to understand. The authors propose code-switch back-translation to improve model’s performance, but don’t prove why such specific BT is favorable or necessary.","This paper proposes several innovations for machine translation. The reviewers had several questions about the claims that were made and the authors addressed these and also acknowledged that some of their formulations (e. g. 'better') would need to be qualified. Overall, there are several interesting ideas that have been put together in a sensible way, but the story is not super consistent. The detailed exchanges between the reviewers are authors are commendable!",0.8513193726539612
"The idea of the proposed method is interesting, but there are a few concerns in terms of the presentation. The authors need to clarify the contributions of the proposed method, such as the unique properties that previous similar compression methods cannot achieve. Therefore, we need a trained model for applying the proposed method. This means the proposed method requires additional computation. The authors need to make the notations much simpler for better understanding to readers. The paper contains a lot of substance, but it is very dense and hard to follow. This is hard to align the word embedding size that would highly dependent on the sentence length and would significantly differ for various tasks","DictFormer is a method to reduce the redundancy in transformers so they can deployed on edge devices. In the method, a shared dictionary across layers and unshared coefficients are used in place of weight multiplications. The author proposed a l1 relaxation to train the non-differentiable objective to achieve both higher performance and lower parameter counts. All reviewers ended up giving the paper a score of 6 after increasing their scores during discussions. While the results are strong (better performance at much lower parameter counts), the paper is not clearly written. Several reviewers noted that the paper is difficult to understand and has a few unresolved points. For example, the method also ended up performing better than the base transformer model that",0.8486117124557495
This paper proposes a new method to solve the minimax optimization problem. The proposed method converges faster for some convex-concave and non-concave problems. The authors did not provide any analysis for their framework for the convex-concave problem. The authors should make comparisons or discuss the differences between these methods. This is necessary to show the effectiveness of the method for GAN-A applicationse.,"This paper proposes to use Anderson Acceleration on min-max problems, provides some theoretical convergence rates and presents numerical results on toy bilinear problems and GANs. After the discussion, the reviewers agreed that this paper makes a nice contribution to ICLR. Some concerns were originally expressed in terms of incrementality of the theoretical results with respect to previous work (KCYs, gBHU), but the authors have well clarified their contributions in the discussion, and have updated their manuscript accordingly. There were also initial concerns about the related work coverage, but this was also properly addressed in the rebuttal, with additional experimental comparisons as well as extended related work section, as well as an additional convergence result for convex",0.8451904058456421
"This paper studies the optimization of neural network beyond NTK (lazy training) and shows that GD converges to 0 training loss. In particular, the large stepsize in proportion to the width m is also needed to exhibit feature learning as well as network scaling 1/m. It is misleading to say that this theory covers multi-layer neural networks because the trainable layer is limited to the second-to-last layer. Therefore, the model in this paper is essentially two-layer neural networks with random feature inputs. In particular, the large stepsize in proportion to the width m is also needed to exhibit feature learning as well","This paper studies optimization of over-parametrized neural networks in the mean-field scaling. Specifically, when the input dimension in larger than the number of training samples, the paper shows that the training loss converges to 0 at a linear rate under gradient flow. It's possible to extend the result by random feature layers to handle the case when input dimension is low. Empirically the dynamics in this paper seems to achieve better generalization performance than the NTK counterpart, but no theoretical result is known. Overall this is a solid contribution to the hard problem of analyzing the training dynamics of mean-field regime. There was some debate between reviewers on what is the definition of ""feature learning",0.8529444932937622
"The paper analyzes the setting of t|, where the kernel error rate is typically specified by the source and capacity condition. The paper shows that, in the high dimensional setting, Gt,XPmin(t, eigenspace) seems to be the limit of random features and kernel methods. The main technical difference between the paper and [1] seems to be that in [1], Gt,|X| is essentially a matrix. This difference requires certain technical treatment and other contributions.",*Summary:* Study gradient flow dynamics of empirical and population square risk in kernel learning. *Strengths:* - Empirical results studying several cases in MSE curves. - Explaining / solving certain phenomena in DL using kernels. *Weaknesses:* - More motivations would be appreciated. - Technical innovation not so high. *Discussion:* Ud7D found that the main strength of this paper is the take-home message rather than innovations. They concluded 7 might be appropriate for the evaluation. This opinion was seconded by WyHh who considered 7 the most appropriate rating. 5uQz also found that 7 would be the most appropriate rating. qXRH,0.8131988644599915
"This paper provides new insights on what type of equilibria and/or structural assumptions on Markov games may admit sample-efficient learning in the size of the joint action spaces spaces. The result of  -CCE (Theorem 3) is based on previous results. It is not surprising given previous analysis about Nash-V learning (Tian et. al.,) As pointed by the authors, the dependences on S and H are not tight yet.","This paper proposes algorithms for learning (coarse) correlated equilibrium in multi-agent general-sum Markov games, with improved sample complexities that are polynomial in the maximum size of the action sets of different players. This is a very solid work along the line of multi-agent reinforcement learning and there is unanimous support to accept this paper. Thus, I recommend acceptance.",0.846811830997467
"This paper studies the ""silent alignment effect"" on deep neural networks. The effect is analyzed on (deep) linear networks and is a bit surprising that it is well aligned with empirical evaluations. The paper also investigates how varying certain design choices (e. g) can be given on the kernel regression solution of the NTK. The authors identify an interesting silent error in the treatment of deep linear networks. The paper also investigates how varying certain design choices (e. g) can be given on the kernel regression solution of the NTK.","The authors make a case for a phenomenon of deep network training that they call the ""silent alignment effect"": that, while the training error is still large, the NTK associated with the network aligns its eigenvectors with key directions in ""feature space"". They support this with non-rigorous theoretical analysis of linear networks, and extensive experiments with real networks on real data. The consensus view was that this paper provides novel and useful insight into training dynamics, in particular regarding feature learning.",0.856903612613678
"This paper studies the TextGames, a very interesting and exploratory direction, and sets up a new SOTA performance through the model-based RL methods. The paper proposes a reasonable extension based on OO POMDPs for text-based environment. The empirical results give strong evidence that this approach led to reasonable improvement in planning.","This manuscript makes an interesting observation: there is no reason why planning-based methods like MDPs must be limited to physical or grounded environments. One can plan about more abstract textual domains. It adapts the standard methods from planning to such text domains in a fairly straightforward way. The fact that concepts from MDPs map to these problems directly is an asset: ideas could flow between these domains in the long term. While the original submission was lacking clarity and significant technical details, the authors engaged with the reviewers and resolved lingering concerns. Reviewers are unanimous that this a strong contribution.",0.8515170216560364
"This paper tries to explain the success of magnitude based and gradient based methods in compressing neural network models. Although these methods are naive, both of them are found to be robust and are easy to implement.","The paper used the Koopman operator theory to explain and guide the DNN pruning. All the reviewers deemed that such a viewpoint is novel (but at different levels). However, the paper still had some issues, including unclear technical details, vague/overselling statements, being computation and memory expensive, etc. The paper finally got 4 ""marginally above threshold"" (one being of low confidence), making it on the borderline. The AC read through the paper and agreed that the Koopman operator theory brings new perspective to DNN pruning, with potential for other analysis of DNNs. Although the paper is imperfect and not strong, it does not have severe problems either and the issues pointed out by the reviewers could",0.847346305847168
"The authors propose an algorithm to classify the  -manifolds and claim it is efficient. Although the algorithm seems efficient, the authors did not give any analysis about the time complexity. The presentation of the main idea could be better organized (see minor comments below).","The authors' provide a discussion of Cover's Theorem in the setting of equivariance. The reviewers consider the work well explained and interesting, especially after the revisions, and so I will vote to accept.",0.8567436337471008
"The paper claims ""inspired by a Tucker tensor form"", what is the advantage of the Tucker tensor form? And why MHSA can be benefited from the Tucker tensor form? The main questions are: (1) where is Tucker decomposition in this formulation, (2) how does this equation correspond to Fig.4b, and (3) how exactly is this different from a standard MHSA? I would like to see an experiment with the same configuration as MHSA in terms of dimensions of activation. This also seems to be equivalent to a single-head self-attention with R features instead of D, due to the fact that the values C_","This paper presents a tensor diagram view of the multi-headed self-attention (MHSA) mechanism used in Transformer architectures, and by modifying the tensor diagram, introduces a strict generalization of MHSA called the Tucker-head self attention (THSA) mechanism. While there is some concern regarding the incremental nature of the proposition, the identification of where to usefully add the additional parameter that converts from MHSA to THSA was nontrivial, and the experimental results on the performance benefits across multiple tasks is convincing.",0.8398861289024353
"The idea is clear and simple, and the authors give theoretical definitions to support their assumptions and experiments. The authors present experiments to validate it. The authors conduct experiments with different supervision levels and show that CL-InfoNCE can better bridge the gap with the supervised representations by using auxiliary information. The proposed approach can also be applied without auxiliary information with K-means and EM optimization.","The paper proposes a weakly supervised contrastive learning, using auxiliary cluster information, for representation learning. Their method generates similar representations for the intra-cluster samples and dissimilar representations for inter-cluster samples via a clustering InfoNCE objective. Their approach is evaluated thoroughly on three image classification task. The reviewers agree that the paper is well written, presenting interesting theoretical analysis (Reviewer h3zd, a8kw) and solid experimetal results (Reviewer RhYi, 1ziy). The core idea of the paper is relatively simple and well motivated (Reviewer h3zd). While the focus is using the clustering with auxiliary labels, the method can be applied without auxiliary labels with K-",0.8536073565483093
"The authors do a good job at presenting a method and the context in which it is intended to be used. The paper has two sides: a method and the context in which it is intended to be used. The latter (context) is a bit weaker. The authors choose to treat their problem in isolation, which means that the paper sometimes loses sight of why the method was proposed to begin with. The authors clearly explain trade-offs and provide instructions on how to arrive at ideal values. The bit inversion technique, for instance, concisely raised a problem and summarily offered a solution. The prose is generally clean and easy to read.","### Summary The key idea behind this approach is a new technique to map irregular sparsity to a regular, compressed pattern. The results can, in principle, therefore overcome several standard limitations with irregular data storage formats. The results improve over existing (though related) techniques. ### Discussion #### Strenghts - An interesting and timely topic to study - Results show non-compute improvements #### Weakness The primary weakness noted among the reviewers was the lack of study on actual decoding performance. As I note below, this is a serious oversight that given the already existing theoretical work in the area warrants study as the community should begin to turn towards mapping that theory to practice. ### Recommendation I",0.8184762597084045
"The paper provides a principled and interesting approach to an impactful problem. The idea of approximating the optimal policy and then computing information on the sequence of states visited by the optimal policy is both novel and clever. The main idea of the paper is really interesting. Most model based approaches are based on data efficiency, so clearly the idea of applying active learning is interesting. The paper focuses on active learning for policy optimality, instead of the most traditional exploration/exploitation approach. In that setting, there is already a generative model/oracle/simulator that allows arbitrary transitions. Most planning approaches, like the ones presented in the","In this paper, the authors introduce an exploration method for RL according to experimental design perspective via designing an acquisition function, which quantifies how much information a state-action pair would provide about the optimal solution to a Markov decision process, and the state-action that maximizes such acquisition function will be used for sampling for policy update. The empirical evidences show the proposed method is promising. Since most of the reviewers support the paper, I recommend acceptance of this submission. However, besides the questions raised by the reviewers, e. g., computation cost and planning quality from CEM, there is a major issue need to be clarified in the paper: >The algorithm designed for RL with generative model",0.8446862101554871
"The authors propose a new framework, Bayes Augmented with Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget and demonstrate that BAM generalizes many popular Bayesian update rules for non-stationary environments. The paper tackles an important problem of Bayesian online learning in a non-stationary environment. The main contribution is BAM, an algorithm that constructs an informative prior distribution for current observations based on stored previous data. While the method seems technically sound, the method still falls short of the following points: Scalability: the algorithm remembers all the data so far, which requires infinite","The article introduces a Bayesian approach for online learning in non-stationary environments. The approach, which bears similarities with weighted likelihood estimation methods, associate a binary weight to each past observation, indicating if this observation should be including or not to compute the posterior. The weights are estimated via maximum a posteriori. The paper is well written, the approach is novel and its usefulness demonstrated on a number of different experiments. The original submission missed some relevant references that have been added in the revision. The approach has some limitations, highlighted by the reviewers: * it requires to solve a binary optimisation problem whose complexity scales exponentially with the size of the dataset; although the greedy procedure proposed by the authors seems to work fine",0.8604416847229004
This paper proposes to combine a neural network with a physics-based model for full-waveform inversion (FWI). The authors introduce a new large-scale dataset which could be of interest to the community working on data-driven solutions to FWI. The authors compare to other learning-based approaches and perform an ablation study to evaluate their proposed method.,"The paper presents an unsupervised method for learning Full-Waveform Inversion in geophysics, by combining a differentiable physics simulation with a CNN based inversion network. The reviewers agreed that the paper was well written and described an important advance but were concerned about limited novelty and a potential sim2real gap. The authors responded to their critique with significant new experiments and clarified the novelty of their method relative to prior work. Based on the author responses, I recommend acceptance.",0.8694955706596375
"The paper proposes a solution to a very important problem. I will not question neither the motivation nor the importance of the problem. However, I believe some relations to needs be build with kernel literature (I will give examples below). The idea of using the kernel function to address the conditional sampling procedure seems novel to me. Experimental results also verify its effectiveness in weakly supervised contrastive learning and hard-negatives contrastive learning tasks. The motivation of this paper is clear. The paper is well-written and easy to follow. I think it has a higher computational cost compared with baselines due to inversion of gram matrix and its complexity is","The reviewers all acknowledge the importance of the paper as it addressed the challenge of the insufficient data problem in conditional contrastive learning, feeling that the idea was novel, the experiments verified the effectiveness of the model well, and the paper is well written. Reviewers also raised some good questions, such as the computational complexity, comparison with Fair_InfoNCE in the experiments, and kernel ablations. These questions are well addressed in the rebuttal and the revised version. One reviewer raised the issue of similarity to [1]. After taking a close look at this paper and [1], the AC felt that the motivation and focus of this paper are quite different from [1]. The authors should incorporate all the rebuttal info into the final",0.8569676876068115
"This paper proposes a method for cross-domain few-shot learning. The proposed method consists of three stages for training, which seems complicated and a little bit tricky in implementations. The paper is well written and easy to follow. The authors also conduct extensive analysis of the proposed method. The proposed framework has a lot of hyper parameters to tune which is an issue for few-shot learning. In experiments, most results of the proposed method are satisfactory. However, on CropDisease, it is significantly worse than the results of STARTUP. The paper is well written and easy to follow. The authors also conduct extensive analysis of the proposed method","Summary: Paper addresses the cross-domain few-shot learning scenario, where meta-learning data is unavailable, and approaches are evaluated directly on novel settings. Authors propose a 3-step approach: 1) self-supervised pretraining, 2) feature selection, 3) fine-tuning, and demonstrate gains over state-of-art. Pros: - Approach is novel for this setting - Paper is clear and easy to understand - Performance beats several prior methods - Experiments are thorough - Fundamental problem is worthwhile of investigation Cons: - Some concerns among multiple reviewers on how hyperparameters are selected. Authors have provided more information and tables in the paper. - Training process is multi-step and not unified. Authors provided additional",0.848449170589447
"This paper proposes an approach to model the causal gene-region interactions. All reviewers agree that this is an interesting and novel approach to the problem. There are some issues with the model or claims about it that we note. There are several issues with the analysis that we note. However, the following suggestions for improving the data analysis could significantly improve the practical utility of the manuscript. The authors should discuss scalability, esp. on single-cell data applications. There are several issues with the model or claims about it that we note. However, the following suggestions for improving the data analysis could significantly improve the practical utility of the manuscript. The","The AC and reviewers all agree that the paper proposes a very interesting framework to extend Granger Causality to DAG structured dynamical systems with important applications. The submission was the object of extensive discussion, and the AC and reviewers all agree that the author feedback satisfactorily addresses the vast majority of their concerns. We strongly urge the authors to incorporate all the points and revisions mentioned in their feedback. We certainly hope that the author will pursue this line of work and consider scaling their approach to tackle larger applications such as those related to social networks.",0.8457877039909363
"The authors seem to suggest that casting ""the prediction problem into an unrolled optimization process"" is better than previous approaches such as ConfGF. This is not clear to me. The main claim of the work is energy minimization, I don't find any relation between the mathematical formulation and energy minimization.","All reviewers except one agreed that this paper should be accepted because of the strong author response during the rebuttal phase. Specifically the reviewers appreciated the new ablation study showing that improvements are not due to minor architectural changes, the new experiment on the number of time steps required for experiments, the agreement to change language around ""neural energy minimization"", the improvements to the related work, the novelty of the unrolled optimization approach, and the nice experimental results. Given this, I vote to accept. Authors: please carefully revise the manuscript based on the suggestions by the reviewers: they made many careful suggestions to improve the work and stressed that the paper should only be accepted once these changes are implemented. Once these are",0.8450579047203064
"This paper investigates the trainability of the ultra-wide GCNs via the GNTK view and provides a new perspective for the over-smoothing problem. The reviewers agree that DropEdge should be the most relevant baseline. The baseline methods considered in the paper are limited. I agree that DropEdge should be the most relevant baseline, but it would be better to discuss some other methods (e. g., normalization-based approaches). It would be interesting if the new theoretical framework could provide some insights on the other research on understanding/solving the over-smoothing issue.","In this paper, the authors established interesting theoretical results regarding the behavior Graph Neural Tangent Kernel (GNTK). They also provide sufficient evidence (some of which during rebuttal) that their approach is valid. We have had many discussions and I suggest that the authors apply reviewers' comments to the final version of their paper.",0.8584056496620178
"This paper proposes a new neural network model to study the dynamics in C. elegans. The proposed model uses a neural network with interpretable parameters (connectivity weights, neuron time constants, resting membrane potentials) yet are flexible to capture the variability across the population or individual neurons (by incorporating noise in the latent and observed variables). All reviewers agree that the proposed model is interesting, but have some questions about the paper.","The authors build an encoding model of whole-brain brain activity by integrating incomplete functional data with anatomical/connectomics data. This work is significant from a computational neuroscience perspective because it constitutes a proof of concept regarding how whole brain calcium imaging data can be used to constrain the missing parameters of a connectome-constrained, biophysically detailed model of the C. elegans nervous system. There were issues related to clarity in the initial submission which all appeared to have been addressed in the final revision. This paper received 3 accepts (including one marginal accept) and 1 reject. The paper was discussed and the reviewers (including the negative reviewer) were unanimous that the current submission should be accepted.",0.8506050705909729
"The paper proposes a truly innovative technique, which seems to be effective and versatile enough to be applied on several tasks and different biomedical signals (EEG, EMG,...) The technique has been thoroughly evaluated and the results are convincing.","This paper introduces a tree-structured wavelet deep neural network to effectively extract more discriminative and expressive feature representations in time series signals. Based on a frequency spectrum energy analysis, the approach decomposes input signals into multiple subbands and builds a tree structure with data-driven wavelet transforms the bases of which are learned using invertible neural networks. In the end, the scattering subband features are fused using a self-attention-like mechanism. The effectiveness of the proposed approach is verified extensively on a variety of datasets from different domains including follow-up experiments in the rebuttal. Overall, the work is technically novel and provides an interesting way of extracting adaptive finer-grained features to deal with",0.8466251492500305
"The paper proposes for the first time (to the best of my knowledge) to combine bi- and cross-encoders. However, the specific way introduced to merge the encoders is not that special. The main novelty of this paper only comes from the fact that it proposes for the first time (to the best of my knowledge) to combine bi- and cross-encoders. After all, it is still unclear why the iterative and coupled fine-tuning of bi- and cross-encoders results in performance improvement.","For pairs of pieces of text, the central idea of this paper is to combine the approaches of using bi-encoders (where a vector is formed from each text then compared), which are easily trained in an unsupervised manner, with cross-encoders (where the two texts are related at the token level), which are normally trained in a supervised manner. The chief contribution of this paper is to train a combined model (as a ""trans-encoder"") by doing co-training of both model types in an alternating cyclic self-distillation framework. The paper suggests that this allows unsupervised training of a cross-encoder. This claim met some pushback from the reviewers, since the method",0.8493680357933044
"This paper proposes a new deformed sampling module that explicitly downscales the image into target resolution. The proposed method is end-to-end trainable, and can be plugged into different backbone networks.","Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors' rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.",0.8385668396949768
"The paper addresses a limitation of typical morphogenesis approaches, which is that they are usually trained to generate a single image/state. This work is a first step towards learning NCA that generalizes to different target states. The experimental results are interesting despite having some limitations.","Meta Review for Variational Neural Cellular Automata This paper proposes a generative model, a VAE whose decoder is implemented via neural cellular automata (NCA). The authors show that this model performs well for reconstruction, but they also show that the architecture has some robustness properties against damage during generation. Experiments were conducted on 3 datasets: MNIST, Noto Emoji, and CelebA, and while experimental results were great on MNIST, the method was less performant so on the other two datasets, although there is clear evidence that the model can learn to generate meaningful images. For the robustness experiments, the authors show that VNCA is robust to perturbations (occlusions) and show that",0.8446365594863892
"This paper presents a (to my knowledge), novel goal-conditioned RL method. This approach improves appon Hindsight Experience by incoporating demonstrations and efficiently uses them to set goals. The results are also well analyzed and HindRL shows strong performance when compared to HER and other RL + LfD approaches. I think the ablations, especially looking at the number of demonstrations needed for each method, are very detailed and provide good computation about HindRL. The main weakness of this paper is the lack of diversity in the empircal evaluations. It would be good to see results on more complex continuous control tasks in different settings","This paper proposes a method to improve the sample efficiency of the HER algorithm by sampling goals from a distribution that is learned from human demonstrations. Empirical results on a simulated robotic insertion task show that the proposed method enjoys a better sample efficiency compared to HER. The reviewers find the paper well-written overall and the proposed idea reasonable. However, there are concerns regarding the limited novelty of the proposed method, which seems incremental. Also, the empirical evaluation suffers from a lack of diversity. The considered tasks are virtually all equivalent to an insertion task. The paper would benefit from further empirical evaluations that include tasks such as those considered in the original HER paper.",0.8594072461128235
"This paper proposes a novel algorithm for sparse non-linear CCA for N D case. The proposed algorithm is able to pick relevant input features in challenging noisy and high dimensional (ND) cases with few examples. The authors claim that the proposed method is efficient in C.2 (Run Time Analysis) and it showed better performance compared to other methods for predictive performance. The authors did not include the results of the proposed sparse CCA method in the experimental section of the paper. I would be interested to see one biology dataset application for model interpretability on this method. In figure 2, the wide range of coefficients showed around 40","Canonical correlation analysis is a method for studying associations between two sets of variables. However these methods lose their effectiveness when the number of variables is larger than the number of samples. This paper proposes a method, based on stochastic gating, for solving a l0 -CCA problem where the goal is to learn correlated representations based on sparse subsets of variables. Essentially, this paper combines ideas from Yamada et al. and Suo et al. who introduced Gaussian-based relaxations of Bernoulli random variables, and sparse CCA respectively. They also extend their methods to work with nonlinear functions by integrating deep neural networks into the l0 -CCA model. They gave experimental results on various synthetic and real examples",0.8426178693771362
"The idea of model updates in a low-rank subspace is not new. In fact, it has been mentioned in one of the pioneering papers on federated learning: Konecny et al., Federated learning: Strategies for improving communication efficiency, 2016 (Section 2) The core idea of low-rank updates in this paper is the same as the low-rank structured update in the above paper by Konecny et al.","The paper shows that most variance of gradients used in FL and distributed learning in general is in very low rank subspaces, an observation also made in Konecny et al 2016 and some other related works in deep learning, though sometimes for a different purpose. The paper then proposes lightweight updates combining a fresh gradient with old updates. Experiments and a theoretical convergence guarantee complement the results, which are mostly convincing. The experiments compare against ATOMO but strangely not against the more common PowerSGD, which would also work with partial client participation. Overall, reviewers all agreed that the paper is interesting, well-motivated and deserves acceptance. We hope the authors will incorporate the open points as mentioned by the reviewers",0.8497750163078308
"The authors provide some valuable arguments about the performance of GCN on heterophilous graphs and verify the claims with some empirical results. The paper provides a deeper insights into the heterophily problem and help us understand the GCNs’ performance. It also designs a new cross-class neighborhood similarity metric to help explain the performance of GCN on various graphs (although it is not perfect). The paper has several flaws which should be fixed/justified before publication. However, this paper has several flaws which should be fixed/justified before publication. However, this paper has several flaws which should be fixed/justified before publication","Heterophily is known to degrade the performance of graph neural networks. This paper explores whether, for graph convolutional networks (GCNs), this is a general phenomenon, or if there are some circumstances under which a GCN can still perform well in a heterophilous setting. This paper characterizes one such setting under a contextual stochastic block model (CSBM) distribution with two classes (generalized in the appendix to multiple classes). The main takeaway is that there are indeed scenarios where a GCN can be expected to perform well, even under heterophilic neighborhoods. There are limitations, and the reviewers have been fairly thorough in pointing these out: the analysis is specific to GCNs under CS",0.8414523601531982
"This paper proposes a new explainable framework that decomposes the information into target and background portion. The authors claim that the newly proposed method has the advantages of higher fidelity and node-level explainability. The paper focuses on the explanation of GNN performance, which is an interesting and critical problem. The experiments are also comprehensive. The paper proposes a reasonable method for the problem to decomposite the graph. However, there are still some points required to be clarified. The proposed method significantly outperforms other benchmarked methods in at least one of the metrics (some benchmarked methods are only shown for Explanation AUC, not for time efficiency","This paper proposes a decomposition-based explanation method for graph neural networks. The motivation of this paper is that existing works based on approximation and perturbation suffer from various drawbacks. To address the challenges of existing works, the authors directly decompose the influence of node groups in the forward pass. The decomposition rules are designed for GCN and GAT. Further, to efficiently select subgraph groups from all possible combinations, the authors propose a greedy approach to search for maximally influential node sets. Experiments on synthetic and real-world datasets verify the improvements over existing works. During their initial responses, reviewers suggested that the authors experiment with more baselines and also clarify some of the technical details. The authors revised their manuscript",0.8551918268203735
"The paper is well written, the motivation of the authors is clear, and the framework presented is easy to understand. The theory part is written in a clear mathematical manner, with a theorem-proof structure, which I really like. But I have one main point of criticism that seems very important to me, while the entire theory section provides insights and interesting ideas the provided results in sec. 5 experiments does not hold up to this. Certainly, a benchmark comparison on common data sets with other methods is very important, but it is not adequate for the hypotheses presented in the theory part. The connection between the theory and the experiments is unclear","The authors provide a theory for training feed-forward spiking neural networks (SNNs) on input-to-output spike train mappings. They utilise for this heterogenous neurone and skip connections. The resulting method is tested on DVS Gesture, N-Caltech 101 and sequential MNIST. It achieved very good performance. The reviewers agreed that the results are interesting and significant. In the initial reviews, the reviewers pointed out some doubts about the theory and clarity of writing. These doubts and objections were addressed in the revision and the reviewers were quite satisfied with that. In conclusion, the manuscript presents interesting results for SNNs with a solid theory and very good experimental results. All reviewers",0.8388829231262207
"The paper proposes an algorithm for subdmarity-based subset selection. The paper is overall well-written and somehow easy to follow (see minor comments). The authors could emphasize more on the key differences between their analysis and the previous literature. The main questions: - I understand the use of the standard greedy algorithm for the selection of clients in DivFL, however, the role of submodularity doesn’t seem to give an advantage, at least theoretically speaking. In other words, which role does submodularity play in the analysis of the algorithm? Is there any connection between Assumption 1 and submodularity","The paper proposes a novel method for (diverse) client selection at each round of a federated learning procedure with the aim of improving performance in terms of convergence, learning efficiency and fairness. The main idea is to introduce a facility location objective to quantify how representative/informative is the gradient information of a given set of clients is, and then choose a subset that maximizes this objective. Given the monotonicity and submodularity of the proposed facility location objective, the authors have been able to provide theoretical guarantees. Experimental results on two data sets (FEMNIST and CelebA) show the effectiveness to the proposed approach and algorithm. The reviewers had a number of concerns most of which were addressed in",0.8381352424621582
"The proposed method is novel and can inspire future research in recommender systems. The experiments are sufficient to validate the claims. There are some small typos that should be fixed before publication. The motivation is somehow unclear and the presentation of introduction could be further improved. The paper views recommender systems from a new perspective, which is similar but different to existing works in causal collaborative filtering. The experimental studies are conducted in synthetic data, offline data and online data, which can well demonstrate the effectiveness of the proposed method. The main contribution is to address the insufficient overlapping problem that causes the uncertainty of IW and DA-based methods. However,","This paper presented a domain transportation perspective on optimizing recommender systems. The basic motivation is to view recommendation as applying some form of intervention, implying a distributional shift after the recommendation/intervention. Distribution shift brings tremendous difficulty to traditional causal inference or missing data theory perspective of recommender systems as it violates the distributional overlapping assumption: in simple terms, if the model recommends radically different set of items, there isn't much you can say about its generalization ability; on the other hand, if the model only recommends items that it already observed during training (no distribution shift at all), it would inherent all the biases which already exist in the data. To that end, this paper proposed a domain transportation",0.8352763652801514
"The proposed method of learning hierarchical temporal structures by measuring prior and posterior belief states is new in the area of unsupervised representation learning for video data. The model is compared with VTA only on the Moving Ball dataset, which in my view, is not enough to validate the superiority of the proposed model. The authors may consider to compare with VTA on a more challenging dataset such as Bouncing Balls. I believe this dataset can better support the claim in this paper, because it has more obvious event changes represented by the interactions of multiple balls and is also used in the work of VTA. The model is not compared with any existing approaches","Thanks for your submission to ICLR. This paper considers a variational inference hierarchical model called Variational Predictive Routing. Prior to discussion, several reviewers were on the fence about the paper, most notably having concerns about some of the experimental results as well as various clarity issues throughout the paper. However, the authors did a really nice job addressing many of these concerns. Ultimately, several of the reviewers updated their scores, leading to a clear consensus view that this paper is ready for publication. We really appreciate your effort in providing additional details and results. Please do keep in mind the concerns of the reviewers when preparing a final version of the manuscript.",0.8347018361091614
"This paper proposes a novel method to improve the detection performance on small faces. The proposed methods are effective for detection of objects with wide range of scales. The result on the wider-face dataset is very good, outperforming most of the state-of-the-art approaches. The experiments mainly focus on comparison with some general detectors. There is almost no comparison experiment with some well-known network search methods. The paper is basically a kind of Neural Architecture Search, and would like to see a comparison with at least evolutionary method. The choice of TinaFace as base network is straightforward. However, is it possible that after NAS, a","This paper received 4 quality reviews, with the final rating of 8 by 2 reviewers, and 6 by the other 2 reviewers. All reviews recognize the contributions of this work, especially its superior performance. The AC concurs with these contributions and recommends acceptance.",0.8234718441963196
This work explores more complex environments considering potential attacks from non-target sounds in audio-visual navigation. A joint training paradigm for the agent and the attacker is proposed. Experimental results on Replica and Matterport3D can validate the superiority of the proposed method over recent approaches.,"This paper addresses audio-visual navigation tasks where a reinforcement learning agent perceives visual RGB and binaural audio inputs, rendered in a first-person perspective 3D environment, and is tasked to navigate to the audio source. The authors propose to make the RL navigation policy robust, by training the agent with additional adversarial audio perturbations. These perturbations consist of an adversarial ""ghost"" agent (attacker) that emits noise perturbations volume, position and category determined by policies that are trained to maximise the negative rewards for the navigation agent in zero-sum game. The agent is then evaluated on the simulated Replica and MatterPort3D environments and compared to a few baselines. The authors conduct a large number",0.8613799214363098
"This paper studies a problem of spurious association in fitting a predictive function. The intuition of NuRD is clear. The paper studies a variety of representative data sets. The experimental results show that despite highly different training and test distributions, NuRD is able to greatly reduce the influence of nuisance variables. The proposed method allows to remove many of the strong assumptions that limit the application of competing ones, for example it can be used in high dimensional image classification tasks. The experimental results show that despite highly different training and test distributions, NuRD is able to greatly reduce the influence of nuisance variables. The authors may need to check/explain why Nu","The paper studies how to build predictive models that are robust to nuisance-induced spurious correlations present in the data. It introduces nuisance-randomized distillation (NuRD), constructed by reweighting the observed data, to break the nuisance-label dependence and find the most informative representation to predict the label. Experiments on several datasets show that by using a classifier learned on this representation, NuRD is able to improve the classification performance by limiting the impact of nuisance variables. The main concerns were about the presentation and organization of the paper, which was heavily focused on the theoretical justifications but fell short in explaining the intuitions and implementation details. The revision and rebuttal have addressed some of these concerns and improved the",0.858442485332489
"This paper proposes a new algorithm to detect backdoors in neural networks. The proposed setting and algorithm are novel. In particular, the subtle connection between backdoored networks and sparse adversarial example generation can inspire further research in this direction. The experimental results are comprehensive. Although the experiments do not contain the most recent backdoor detection methods as baselines, this reviewer believes that the current comparison baselines are enough to provide the bigger picture. This is since almost all existing methods are typically designed for the white-box scenario, and as such, they have a huge advantage compared to the current method. The paper is generally well-written, although some","This work proposed to detect backdoor in a black-box manner, where only the model output is accessible. Most reviewers think it is a valuable task, and this work provides a novel perspective of using adversarial perturbation to diagnosis the backdoor. Some theoretical analysis for linear models and kernel models are provided. There is still huge gap to analyze the DNN model. But on the other side, it provides some insight to understand the proposed method and could inspire further studies. Besides, since there have been many advanced backdoor attack methods, and many more are coming out, I am not sure that the proposed detection criteria is well generalizable, considering only some typical attack methods are tested. However, I think the studied problem",0.8633466362953186
"This paper studies the convergence of optimization methods non-iid samples, i. e., divergence of SGD with momentum due to certain resonant gradients in co-variate shift. The reviewers found the paper to be quite well written and exemplary in its scientific format. While the divergence phenomenon is interesting, these two facts make me think that the phenomenon might only occur in very limited settings. While the paper tackles an important question, I think the paper's technical contribution looks somewhat weak to me.","This paper studies online learning using SGD with momentum for nonstationary data. For the specific setting of linear regression with Gaussian noise and oscillatory covariate shift, a linear oscillator ODE is derived that describes the dynamics of the learned parameters. This then allows analysis of convergence/divergence of learning for different settings of the learning rate and momentum. The theoretical results are validated empirically, and are shown to generalize to other settings such as those with other optimizers (Adam) or other models (neural nets). The reviewers praise the clear writing and the rigorous and systematic analysis. 3 out of 4 reviewers recommend accepting the paper. The negative reviewer does not find the main contribution interesting and significant enough for acceptance",0.8495243787765503
"This paper is very well written, tackles an interesting area, proposes a novel calibration, and demonstrates the utility in empirical experiments. The proposed calibration results in a more intuitive interpretation than previous approaches. The experiments demonstrate clearly the value of matching the algrithm and metric. Histogram binning, within the appropriate algorithm, is shown to be a competitive post-hoc calibration technique. One of the three empirical observations on Page 9 states that the new TL-HB is the best performing method for 1-vs-rest ECE. In fact, it is the un-normalized variant of 1-vs-rest calibration that is shown","The paper studies the problem of multi-class calibration, proposing new notion of ""top-label calibration"", and presenting and comparing new algorithms for multi-class calibration. Reviewers generally found the paper to be well-written, and tackling a foundational problem. There were some questions regarding the experiments: (1) _Lack of explanation of why unnormalised beats normalised. _ One of the paper's main empirical findings is that using an unnormalised predictor with histogram binning (CW-HB) can significantly outperform a normalised one (N-HB). There is however limited discussion prior to this of why such behaviour is expected. (2) _Lack of comparison to isotonic regression",0.8512211441993713
"In general, the paper is written clearly with enough technical details to understand the arguments. The motivation to study data with anisotropic covariances is good. The findings on the error curve structures and the correspondence between phase transition and induced kernel matrix spectrum are interesting.","This paper extends recent and very active literature on analyzing learning algorithms in the simplified setting of Gaussian data and model weights, with the main generalization being to allow for non-isotropic covariance matrices. The main technical results seem to be correct and slightly novel, though reviewers feel they are not innovative or unexpected enough to stand on their own. However, the main contributions of the paper are then to interpret these results to give phenomenological results (regarding double descent, etc.), and reviewers were unanimously happy with these. In the end, all reviewers were positive about the paper. The largest reviewer criticisms of the paper were technical issues (ot31) and lack of context of recent literature (3RfG).",0.8493239879608154
"The paper proposes an approach to overcoming the so-called backfill problem when time-series data are revised to correct previous recorded values. The authors propose a recurrent neural network to carry out the task. The paper is well written and the problem is well presented, notation is defined carefully, and the model is described with enough detail. The authors carry out a comparative evaluation against several CDC-approved forecasting models showing convincing better results.","This paper introduces Back2Future, a deep learning approach for refining predictions when backfill dynamics are present. All reviewers agree on that the authors successfully motivate their work and introduce a topic of great interest, i. e. that of dealing with the effect of revising previously recorded data and its effect timeseries predictions. The reviewers also underline the strong and thorough experimental section. Among the reviews is also underlined the potential impact of the work for the research domain. Many thanks to the authors for replying to the minor concerns raised. I concur with the reviews and find this submission very interesting, convincing and thus recommend for accept. Thank you for submitting the paper to ICLR.",0.8567561507225037
"The paper makes a valuable contribution towards understanding the ingredients necessary for good kernel performance for image classification. The approximation results are somewhat difficult to understand, but a lot of discussion is provided to parse them. In general, what approach could help improve the computation of these hierarchical convolutional kernels. In the examples provided in this work, no architecture achieves better performance than the Myrtle kernel. The paper is very well written, with detailed and interesting discussions, which makes it a very pleasant read. I really appreciated that 1) the kernel models studied in this paper are motivated by simulation, showing that they match state-of-the-art performance for kernel","The paper addresses hierarchical kernels and provides an analysis of their RKHS along with generalization bounds and cases where improved generalization can be obtained. The reviewers appreciated the analysis and its implications. There were multiple concerns regarding presentation clarity, which the authors should address in the camera ready version.",0.8532826900482178
"This paper proposes a simple and elegant state representation method for learning and planning in the MiniGrid world. All the reviewers agree that this is an interesting and interesting idea. The main concern is that the presentation in the paper leave a lot to be desired in terms of positioning the paper with prior work. There is no mention of the neural network architectures for the VFs. The authors do not explicitly address this connection, and I would like to see them do that. I quite like the idea of action abstractions, or action oriented constructions of state abstractions, especially for robotic domains, and this paper does propose a simple but potentially","This paper is good but at a borderline. One reviewer increased the score during the discussions. However, no reviewer was in strong favor. So that this paper is still a borderline one, and it is up to the SAC to decide.",0.8421384692192078
"This paper presents the first non-trivial sublinear time result for L regression problem. The paper also presents a result for the general A case (where the improvement is about a quadratic improvement). The paper is pretty well-written (some minor typos at the end of this section). Most of my complaints are more from the more practical aspects of the results in the paper: (-) The improvements in this paper only take effect for d>n2/p, which for fixed p is still pretty large. The paper presents some motivation for Lp regression: it would be useful to clarify which of those need p>4.","Dear Authors, The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted: - The new result about query complexity of regression problem that the authors have added. Along with the result on for (noisy) Vandemonde matrix, these make the paper lie above the accept bar. - The authors have providing satisfying clarifications during the rebuttal that convinced reviewers to increase further their scores. The current consensus is that the paper deserves publication. Best AC",0.8302608728408813
"The paper contains useful ablation experiments that control for various aspects of the method. The presentation is very clear. The paper contains easy to understand diagrams and Algorithm 1 was particularly helpful for the training and inference procedure (and how they relate to each other). I believe it would be more useful to readers if the section can explain how all the works relate to each other. In general, there will be many programs that can satisfy the input-output examples, but only some of them may accurately reflect the user's intent.","This paper addresses the problem of program synthesis given input/output examples and a domain-specific language using a bottom-up approach. The paper proposes the use of a neural architecture that exploits the search context (all the programs considered so far and their execution results) to decide which program to evaluate next. The model is trained on-policy using beam-aware training and the method is evaluated on string manipulation and inductive logic programming benchmarks. The results show that the proposed method outperforms previous work in terms of the number of programs evaluated and accuracy. Overall, the reviewers found the paper to be well-written and the idea proposed to be significantly novel and interesting to be presented at the conference and I agree.",0.8452082872390747
"The main weakness of the paper is the lack of thorough benchmarking. For image data, it would be nice to see the benchmarking done on at least one more complex dataset (maybe CIFAR-10). One minor comment is that the authors should consider moving algorithm 1 to the main paper. The problem studied in this paper is important and needs to be solved in data synthesis. The editorial quality of this paper is not always satisfactory. It contains quite a lot of inconsistent/non-precise descriptions, as also reflected in the above comments.",The paper presents a new framework of synthesizing differential private data using deep generative models. Reviewers liked the significance of the problem. They raised some concerns which was appropriately addressed in the rebuttal. We hope the authors will take feedback into account and prepare a stronger camera ready version.,0.8546139597892761
"The authors show that adding divisive normalization in addition to another normalization technique (batch normalization) produces the best performing image classification model on two datasets (ImageNet, CIFAR-100) The authors perform an extensive set of visualization, representational, and Fourier analyses to understand why divisive normalization gives the above performance boost on image classification.","This paper explores addition of a version of divisive normalization to AlexNets and compares performance and other measures of these networks to those with more commmonly used normalization schemes (batch, group, and layer norm). Various tests are performed to explore the effect of their divisive normalization. Scores were initially mixed but after clarifications for design and experiment decisions, and experiments run in response to comments by the reviewers the paper improved significantly. While reviewers still had several suggestions for further improvements, after the authors' revisions reviewers were in favor of acceptance which I support.",0.8517276644706726
"The paper is well-written and clear. The experiments were easy to follow and the analysis was clear. The main findings related to underestimation, comparison of training-set and test-set dynamics during training, and dependence on training data were interesting and not obvious.","This paper presents a very interesting study of using an artificial language (generated using a specific algorithm via a transformer model) and training SOTA transformer and LSTM language models on that language; the authors show that these LMs underestimate the probability of sequences from this language and overestimate the probability of ill formed sentences, among other observations. This is a very interesting study that captures the behavior of recent LMs. All reviewers are supportive of accepting this paper and it is good to see the engagement between reviewers and authors of this paper.",0.8494972586631775
"This paper proposes a method to produce samples that are uniformly distributed on the learned manifold, regardless of the training set distribution. The method is interesting and theoretically well-motivated. The sampling algorithm itself is very straightforward. The authors use the colloquial understanding of ""uniform"" side-by-side with the differential geometric / measure theoretic understanding of ""uniform"". This creates the false impression that the present technique is capable of neutralizing the negative ""implications"" on fairness, data augmentation, anomaly detection, domain adaptation, and beyond. In addition, the provided samples of the generated images are not clear enough to provide a good justification for their","The paper proposes a simple method for uniform sampling from generative manifold using change of variables formula. The method works by first sampling a much larger number of samples (N) from uniform distribution in the latent space and then does sampling by replacement (using probability proportional to change in volume) to generate a smaller number of final samples (k  N) that are seen as approximately sampled from a uniform distribution from the generative manifold. Reviewers had some questions/concerns about the confusing language in the abstract and introduction around the use of the term ""uniform"" which the authors have addressed satisfactorily. Authors have also provided results on quality (FID metric) of the generated samples as asked by the reviewers.",0.8507765531539917
"This paper proposes a new neural contextual bandit algorithm. The proposed algorithm significantly reduces the computation costs when compared to existing neural contextual bandit algorithms. The authors show that their proposed algorithm outperforms the existing algorithms. The main weakness is that the algorithm proposed is not particularly novel. There is also some concern about the requirement on the width of the neural network in thm 4.4 being so large in order for the main O(sqrt(T) bound to hold, but the authors do at least provide some empirical evidence in the appendix that suggests the performance of their algorithm is not heavily reliant on very large values of the network width.","This paper tackles the neural contextual bandit problem, for which existing approaches consists rely on bandit algorithms based on deep neural networks to learn reward functions. In these existing strategies, exploration takes place over the entire network parameter space, which can be inefficient for the large-size networks typically used in NTK-based approaches. In this work, the authors address this by building on an existing technique of shallow exploration, which consists in exploring over the final layer of the network only, allowing to decouple the deep neural network feature representation learning from most of the exploration of the network parameters. More specifically, they propose a simple and effective UCB-based strategy using this shallow exploration scheme, for which they provide a",0.8471455574035645
"The proposed method is well-described and introduced experimental settings are properly designed with multiple public datasets. The experiments show that the performance of the method is comparable or slightly better than competitive baselines. The paper addresses the import issue of uncertainty, which can hugely affect real-world applications. The introduced experimental settings seem to be reasonable to show the effectiveness of its method in calculating prediction intervals and the OOD identification capability. I believe it will be better if the authors perform studies with different parameters and more datasets with different data structures (such as images or time-series data) to generalize their findings.","The paper proposes a novel method, PI3NN, for estimating prediction intervals (PIs) for quantifying the uncertainty of neural network predictions. The method is based on independently training three neural networks with different loss functions which are then combined via a linear combination where the coefficients for a given confidence level can be found by the root-finding algorithm. A specific initialization scheme allows to employ the method to OOD detection. Reviewers agreed on the importance of the problem of producing reliable confidence estimates. The proposed method addressed some of the limitations of the existing approaches, and reviewers valued that a theoretical as well as an empirical analysis is provided. On of the main criticisms was that the theoretical derivation of the method is based",0.8466320037841797
This paper presents a learnable similarity function that adaptively adjusts the weights of the similarity between one sample and the samples of some class. The experimental results of the proposed algorithm outperform that of the other comparable clustering algorithms.,"The authors provide a framework for unsupervised clarification based on minimizing a between-cluster discriminative similarity. It is more flexible than existing methods whose kernel similarity implicitly assumes uniform weights, and the authors connect to ideas such as max-margin and weighted kernel approaches. This yields a clustering algorithm naturally that alternates between updating class labels and similarity weights. Moreover the reviewers (and I) appreciate the analysis of generalization error through Rademacher complexity arguments and detailed author responses. I might add while the paper draws connections to weighted kernel methods and have since added references to sparse subspace clustering etc, there is recent interest in using similar arguments to derive error bounds and uniform concentration results for center-based methods that might",0.8504234552383423
"This paper proposes a method for goal-based automatic curriculum learning. The reviewers found the paper well-motivated and easy to understand. The method would be more powerful if it can be used for non-goals based settings, such as those discussed in the paper. Overall I found the paper written well and easy to understand. The main question that arose was: Where does the difference in Alice and Bob's performance come from and are the same algorithms. The main question that arose was: Why is SAC used to propose goals? The results look decent, but do not perform much better than uniform (domain randomization) on most","While one reviewer remained concerned about the possibility of convergence to bad equilibria and felt that the proposed method appears to be four minor changes from prior work (PAIRED), the authors demonstrate empirically that the proposed changes make a significant difference in their evaluation. Other reviewers were positive about this work and all others rated this work as an accept. Post rebuttal the most positive reviewer increased their score to an 8 and felt did a good job answering their concerns. They wanted to see an analysis of systems with larger numbers of agents, but felt that the current manuscript was more than sufficient to warrant acceptance, and fell into the category of a good paper with the additional ablations provided during the rebuttal. The AC",0.8294639587402344
"This work essentially extends certification methods existing in supervised learning, to reinforcement learning. Although the idea of certification is new, the essence of robustness guarantees together with function perturbation is not new in the RL literature. The two proposed certificates are based on a state perturbation to which either the Q-function or the policy is applied. As far as I understand, this does not affect the model itself but only the ordering in either the Q-values or the policy. The results obtained from CROP are problem-dependent. This may say something about the domain, but also about the certificate itself and how reliable it is. It is not clear which","The authors propose a framework for for the certification of reinforcement learning agents against adversarial observation/state perturbations based on randomized smoothing. They develop the theory of the framework, demonstrating that the framework can be used to certify lower bounds on the worst-case cumulative reward of an agent. They validate their theoretical bounds experimentally. The paper is well written and reviewers were mostly in agreement that the contributions are worthy of acceptance. The technical concerns from reviewer zGtv were addressed during the discussion phase, but I strongly encourage the authors to revise the manuscript to address the points raised in the discussion.",0.8395714163780212
"This paper presents a novel approach to model the latent subgraph from embeddings. The proposed method follows the framework of the SEAL algorithm, and the authors should exphasize the differences between the two methods and what makes the propose method more advantageous. The reviewers found the paper to be well-written, the introduction of the previous work is very insightful and clear, and the proposed method to be interesting and effective.","This paper proposes a new link prediction algorithm based on a pooling scheme called WalkPool. The main idea is to jointly encode node representations and graph topology information into node features and conduct the learning end-to-end. The paper shows the superiority of the method against the baselines. Strength * The paper is generally clearly written. * A new method is proposed, which is technically sound. * Many experiments are conducted to verify the effectiveness of the proposed method. Weakness * The novelty of the work might not be so significant. There is a similarity with the SEAL algorithm. The authors have addressed most of the problems pointed out by the reviewers. They have also conducted additional experiments.",0.8694658279418945
"The paper studies a very essential and relevant problem. The mathematical steps look correct to me. Overall, I believe the paper does not introduce the literature thoroughly, includes multiple typos, and has mistakes in the terminology. I will list some of those at the end of my review, but I think in general the paper should undergo a major review (language, terminology, positioning, presentation of the existing literature, presentation of the main theorem, and the proofs).","Verifying robustness of neural networks is an important application in machine learning. The submission takes on this challenge via the interval bound propagation (IBP) framework and provides a theoretical analysis on the training procedure. They establish, in the large network with case, that the certification via IBP reflects the robustness of the neural network. Despite the tensions between the changing architecture and the required accuracy, the results are insightful. The AC recommends the authors to revise the paper, correcting the significant amounts of typos and improve the presentation for its final version.",0.8443941473960876
The paper made a significant contribution to idea of using adversarial training as part of the self-supervision signal for language learning. The experiments in the paper demonstrated the superiority of adding adversarial training to a self-supervision framework. In many cases it is not clear where the improvements come from.,"This paper received six reviews, consisting of three 8s two 6s and one 3. The reviewers generally felt that the proposed Electra-like pretraining provided fairly significant downstream improvements. Additional ablations were provided to during the author response period and other author responses were sufficient to cause scores to rise during the discussion period. The vast majority of reviewers recommended accepting this paper and the AC also recommends acceptance.",0.84581059217453
"This paper proposes a method for training large scale graph neural networks (GNNs) on large datasets. The approach is well-motivated and well-explained. The results on the OpenCatalyst benchmark are compelling. It would be great if the authors could include a discussion of the applicability of their approach in the context of other graph neural networks for molecular structure. The paper does not put the scaling results in context with other approaches for parallel training. The paper should be compared with other distributed training methods, including data and model parallelism. The paper states that model parallelism can be combined with the proposed approach, but I think this paper","The reviewers were split about this paper: on one hand they appreciated the clarity and the experimental improvments in the paper, on the other they were concerned about the novelty of the work. After going through it and the discussion I have decided to vote to accept this paper for the following reasons: (a) the potential impact of the work, (b) the simplicity of the idea, and (c) promise of release of open source code. I think these things make the paper a strong contribution to ICLR. The only thing I would like to see added, apart from the suggestions detailed by the reviewers, is a small discussion on the carbon footprint of training such largescale graph networks. The",0.8447211980819702
"The paper shows that even in the over parameterized setting, the two approaches may result in different fixed-points. The proposed regularizers are based on the weaknesses revealed by previous theoretical analysis and show good performance in experiments.","This paper presents a study of the over parametrization of linear representations in the context of recursive value estimation. The reviewers could not reach a consensus over the quality of the paper, with a fairly wide range of scores even after the rebuttal. After considering the paper, the rebuttal, and the discussion, I lean towards accepting the paper. Despite the concerns voiced by some of the reviewers, the topic and analysis of the manuscript are novel and interesting, and it is my expectation that this manuscript will prove a valuable source of inspiration for future work. I invite the authors to carefully consider the feedback received by all the reviewers (and in particular Reviewers xq3y and gT5o and)",0.8437404036521912
"The motivation of the paper is very clear and convincing. The effort to provide generalization and optimization results beyond this regime is one of the main challenges of the theoretical machine learning community when studying deep learning. The paper suggests three possible improvements on NTK analysis. The main contribution of the paper is in presenting a complexity measure that gives generalization bounds that are better than those that can be obtained by the complexity measure of the NTK (the RKHS norm of the NTK). However, it is not clear to me why the new complexity measure offered in the paper improves over the NTK. The proposed algorithm has an additional projection step that makes","This paper goes beyond the NTK setting in analyzing optimization and generalization in ReLU networks. It nicely generalizes NTK by showing that generalization depends on a family of kernels rather than the single NTK. The reviewers appreciated the results. One thing that is missing is a clear separation between NTK results and the ones proposed here. Although it is ok to defer this to future work, a discussion of this point in the paper would be helpful.",0.8607956767082214
"This paper proposes a method for training non-parallel text transfer models. The method is based on the assumption that there are parallel pairs in the original corpora of two styles. In those cases where only small unsupervised corpora are there and there are no sentences pairs sharing the same content, this method would not work. The empirical results establish the effectiveness of this method. The paper is relatively well written with justifications and reasons for imitation learning. The authors could have given examples to better maintain the content between the source and the target style corpus. However, the authors assume that parallel sentences inherently exist between the source and the target style corpus","The paper proposes a new method for unsupervised text style transfer by assuming there exist some pseudo-parallal sentences pairs in the data. The method thus first mines and constructs a synthetic parallel corpus with certain similarity metrics, and then trains the model via imitation learning. Reviewers have found the method is sound and the empiricial results are decent. The assumption on pseudo-parallal pairs would limited the application of the methods in other settings where the source/target text distributions are very different. The authors have added discussion on this limitation during rebuttal.",0.8741930723190308
"This paper proposes a simple but effective approach for video action recognition by casting the problem as an image recognition task. It provides solid experiments and ablation studies to evaluate the effectiveness of the proposed approach. The authors have provided extra ablation experiments to evaluate the importance of the grid layout, APE and temporal order of the frames in the grid, as well as activation using visualizations. The experimental setup is clear and results seem convincing.","This paper regards video understanding as an image classification task, and reports promising performance against state of the arts on several standard benchmarks. Though the method is quite simple, it achieves good results. The visualization in this paper also provides good insight. All reviewers give positive recommendations for this paper.",0.8710724115371704
"The proposed algorithm is presented in the background section together with the original algorithm by [Deng, 2020]. The authors do not compare their algorithm against CSGLD (algorithm by [Deng, 2020] in one of the main experiments: Bayesian networks on CIFAR-100 and SVHN. The idea of Monte Carlo average of random-field functions from multiple chains in this paper is straightforward and easy to implement. The theoretical analysis seem to be solid too. However, I anticipate more discussion on the connection between empirical results and theoretical implications.","This paper proposes a new variant of a stochastic gradient Langevin dynamics sampler that relies on two key ideas: approximation of the target density with a simpler function (as in [Deng, 2020]) and the parallel simulation of many chains. The authors also prove that their approach can be theoretically more efficient than a single-chain algorithm. The reviewers see the contribution as significant although they did raise some concerns regarding the clarity of the paper. Since these concerns do not appear to be major, I recommend acceptance but I advise the authors to address the comments of the reviewers to maximize the impact of the paper.",0.8595391511917114
"The main idea is to optimize a single conditional network to learn and represent diverse policies, and reduce the computational costs which are very expensive in prior methods) to self-play. The paper is well written and the derivations look correct to me. The experiments are strong, the authors illustrate that NeuPL can expected results of existing population learning algorithms on classical games. The proposed NeuPL framework is very general and can realize a lot of current mainstream population-based training algorithms. The ablation study is very detailed, which clearly demonstrate NeuPL’s effectiveness. The paper may not be accessible to a wider range of audience since it contains too",The authors propose a new framework of population learning that optimizes a single conditional model to learn and represent multiple diverse policies in real-world games. All reviewers agree the ideas are interesting and the empirical results are strong. The meta reviewer agrees and recommends acceptance.,0.8766718506813049
"This paper proposes a new learning and control method for nonlinear dynamical systems. The proposed approach outperforms existing models and opens up a new perspective for the problem. The empirical evaluations are extensive and convincing, with the method performing favorably against strong baselines. The paper is well written and very clear.","The paper was seen positively by all reviewers. The strength of the paper are: - Intuitive and interesting combination of Koopman Operators and Optimal Control for Reinforcement Learning - Convincing experiments on challenging benchmark tasks - All of the issues of the reviewers (advantages to SAC, gaps in the theory and missing references) have been properly addressed in the rebuttal. I therefore recommend acceptance of the paper.",0.8581675887107849
"The paper presents a mathematically solid analysis of neural networks (seen as tropical rational mappings), including a bound for the approximation error of the compressed network. The presented algorithm seems to outperform a recent tropical baseline in terms of test accuracy of the pruned network.","The submission introduces an algorithm for structured pruning of fully connected ReLU layers using ideas from tropical geometry. The paper begins with a very accessible overview of key concepts from tropical geometry, and shows how ReLU networks can be thought of as tropical polynomials. It gives an efficient K-means-based algorithm for pruning units in a way that approximately minimizes the Hausdorff distance between certain polytopes. Experiments show that the method outperforms other methods based on tropical geometry and is competitive with SOTA methods from a few years ago. I think the reviewers, authors and I all agree on the following points: tropical geometry is a mathematical topic not commonly used in our field and for which it is difficult to",0.8518573045730591
"This paper presents some practical methods regarding classifier margins. They, however, are less novel and their effectiveness are not sufficiently validated in the experiments. The paper presents some practical methods. They, however, are less novel and their effectiveness are not sufficiently validated in the experiments. The paper lacks analysis and/or discussion about the case that some classes are correlated as frequently observed in real-world tasks. In that case, the paper's contribution would be rather theoretical materials. Though those materials seem not to be so effective for further understanding/analyzing the large-margin losses, it is hard to recognize the effectiveness of the proposed method","This work presents a principled objective function for large margin learning. Specifically, it introduces class margin and sample margin, both of which it aims to promote. It also derives a generalized margin softmax loss which to draw general conclusions on the existing margin-based losses. The effectiveness of the proposed theory is empirically verified in visual classification, imbalanced classification, person re-identification, and face verification. The reviewers initially raised some concerns, but most of them were well addressed in the rebuttal and convinced the reviewers. Specifically, pU1u was satisfied by authors' reply on Theorem 3.2 and the practical methods. pGzf appreciated clarifications around the evaluation metric used on IJB-C and believes",0.8356727361679077
"This paper proposes an algorithm to attack vision transformers by considering the attention mechanism. This paper successfully identifies the vulnerability of ViT against dense patch attacks, while ViT was more robust than traditional Lp perturbations or natural perturbations.","This paper provides an interesting study on the adversarial robustness comparisons between ViTs and CNNs, and successfully challenges the previous belief that ViTs are always more robust than CNNs on defending against adversarial attacks. Specifically, as revealed in this paper, when the attacker considers the attention mechanisms, the resulting patch attack can hurt ViTs more. Overall, all the reviewers enjoy reading this paper and appreciate the comprehensive robustness comparisons between ViTs and CNNs. The reviewers were concerned about the missing experiments about adversarial training, vague statements about the inspiration for future defenses, visualization of adversarial examples, etc. All these concerns are well addressed during the discussion period, and all reviewers reach a consensus on accepting",0.8594276309013367
"This paper proposes a neat algorithm that extends SSL to address the DA problems. The proposed method is technically sound, simple yet highly effective. The proposed method significantly extends the state-of-the-art performance. I like this paper, but there are some minor issues that I hope the authors can clarify or improve in future versions.","Thanks for your submission to ICLR! This paper presents a novel way to combine domain adaptation with semi-supervised learning. The reviewers were, on the whole, quite happy with the paper. On the positive side, the results are very extensive and impressive, it's a clever way to combine domain adaptation and semi-supervised learning, and it's a fairly general approach in that it works in several settings (e. g., unsupervised vs semi-supervised domain adaptation). On the negative side, the approach itself is somewhat limited technically. After discussion, the one somewhat negative reviewer agreed that the paper has sufficient merit and should be accepted; thus, everyone was ultimately in agreement. I also",0.8476515412330627
"In general I like the directions that this paper is taking to push the state-of-the-art of neural network verification. The contributions are novel to my knowledge. The two ideas on branching constraints are also interesting. After reading the previous sections, I expect the experimental evaluation to demonstrate two points: the GPU-based dual algorithm is better than an (almost equivalent) MILP-encoding. The experiment on ResNets suggest does seem to show that CAB is beneficial for both BABSR and ACS. However, I'm not fully convinced of the efficacy of the rest of the proposed techniques. The paper is clearly","The authors improve upon existing algorithms for complete neural network verification by combining recent advances in bounding algorithms (better bounding algorithms under branching constraints and relaxations involving multiple neurons) and developing novel branching heuristics. They show the efficacy of their method on a number of rigorous experiments, outperforming SOTA solvers for neural network verification on several benchmark datasets. All reviewers agree that the paper makes valuable contributions and minor concerns were addressed adequately during the rebuttal phase. Hence I recommend that the paper be accepted.",0.8544665575027466
The paper proposes a new method using non-Markovian hierarchical generative models with the pre-trained diffusion-based generative models. The paper discusses practical techniques to support the scaling of the proposed method.,"The paper tackles a very interesting problem in the context of diffusion-based generative models and provides empirical improvements. Pre-rebuttal, reviewers' main concerns lie in the motivation and clarification of the method, while after rebuttal, all reviewers satisfied the response and gave positive scores. The authors should include the additional results to well address the reviewers' concerns in the final version.",0.8710653185844421
"The authors put a lot of thought into designing the architectures of NODE-GAM and NODE-GA 2 M. The authors similarly put careful thought into extracting the interactions. I especially appreciate the application of the ""purification"" technique (Lengerich et al. 2020), which addresses an ambiguity problem that has bothered me about approaches like these in the past. In reading this paper, I felt there was an ""elephant in the room"" question about why a much simpler neural-network-based architecture, such as the ""univariate"" networks present in the ""Neural Interaction Detection"" (NID) paper for modeling","The paper proposes two new generalized additive models (GAM) based on neural networks and referred to as NODE-GAM and NODE-GA2M. An empirical analysis shows that the proposed and carefully designed architectures perform comparably to several baselines on medium-sized datasets while outperforming them on larger datasets. Moreover, it is shown that the differentiability of the proposed models allows them to benefit from self-supervised learning. Reviewers agreed on the technical significance and novelty of the proposed models and valued the clever design of the new architectures. Most concerns and open questions could be answered in the rebuttal and by changes in the revised manuscript. Based one the suggestions of one reviewer new experiments comparing the proposed models to",0.8436555862426758
"This paper presents a theoretical analysis of convolutional neural networks (CNNs) in a convolutional neural network setting. The paper shows that the convergence rates are determined by the smoothness of the target functions and independent of input dimension. The paper is quite dense with notation, and I believe the presentation of the paper could be significantly improved. In particular, the authors suggest that the xi for large i could correspond to different locations of the frequencies, but in practice these probably just correspond do different locations of the frequencies. In this sense, the final result showing adaptivity to different orderings of the convolutional networks is much more compelling than","This work studies the approximation and estimation errors of using neural networks (NNs) to fit functions on infinite-dimensional inputs that admit smoothness constraints. By considering a certain notion of anisotropic smoothness, the authors show that convolutional neural networks avoid the curse of dimensionality. Reviewers all agreed that this is a strong submission, tackling a core question in the mathematics of DL, namely developing functional spaces that are compatible with efficient learning in high-dimensional structured data. The AC thus recommends acceptance.",0.8567935824394226
"This paper contributes a lot to the research area of 1-Lipschitz CNNs, especially the class of piecewise linear GNP activation functions HH. The authors present their findings and current problems and introduce the proposed methods very clear from a high level to a very detailed level. In the Introduction, the authors present the three methods in the order: HH, LLN, CR. However, the order in the method sections is LLN, CR, HH. The logic is a little bit messed up. It is not clearly explained why the original robustness certificate (Eq 4.) has a problem when the number of class k is","The paper provides a procedure for certifying L2 robustness in image classification. The paper shows that the technique indeed works in practice by demonstrating it's accuracy on CIFAR-10 and CIFAR-100 datasets. The reviewers are positive about the paper. Please do incorporate feedback, especially around experimental setup to ensure that the work compares various methods fairly and provides a clear picture to the reader.",0.8369260430335999
"This paper provides a new perspective on why the wider neural networks tend to be linear, which looks interesting and also consistent with previous results. The paper provides a rigorous theoretical understanding of the phenomenon, while it naturally lacks empirical contributions and demonstrations overall.","The authors provide in this manuscript a theoretical analysis to explain why deep neural networks become linear in the neighbourhood of the initial optimisation point as their width tends to infinity. They approach this question by viewing the network as a multi-level assembly model. All reviewers agree that this is an interesting, novel, and relevant study. The paper is very well-written. Initially, a weak point raised by a reviewer was that an empirical evaluation of the theory was missing. The authors addressed this issue in a satisfactory manner in their response. In conclusion, this is a strong contribution worth publication.",0.8720179200172424
This paper proposes a continuous learning method that doesn't grow the model size over time or rely on external memory. The proposed method distills information from the past networks to form a synthetic set of past concepts. The paper is well written and the structure is clear. The clarity of Modeling the Network Output Space can be improved though as it's unclear how exactly to synthesize images from the past weights.,"This paper presents a zero-shot incremental learning approach that does not store past samples for experience replay. The idea is novel and well motivated, and the paper is well written. Reviewers' comments were mainly about missing baselines, missing ablation studies, and clarifications about the proposed method. In the revised paper, the authors provided more justifications and added new experimental results on large benchmark datasets as well as ablation studies. After discussion, all the reviewers are positive about this submission. Thus, I recommend to accept this paper. I encourage the authors to take the review feedback into account in the final version.",0.8577999472618103
The authors propose a reformulation of the problem as an inverted open-domain Question-Answers (QA) The authors used dense retriever in the first phase of their algorithm (retrieving top K entities) and then in the second phase authors trained reader mouse for ranking and extracting entity mentions from the output of the retriever. The main advantages of the proposed method (besides outperforming previous strategies) is the data efficiency gained by eliminating dependency on hardcoded mention-candidate dictionary. Instead it finds entities from text passages and then finds lined mentions in test passages. The proposed QA paradigm allows EntQA to take advantage of existing question answering datasets.,"This paper casts entity linking in a retrieve-then-read framework by first retrieving entity candidates and then finding their mentions via reading comprehension. All reviewers agree that the proposed approach is novel, well-motivated, and simple yet performant. The authors have done a good job of addressing all the concerns raised, and the reviewers are unanimous in their recommendation for accepting the paper. I hope the authors will also incorporate the feedback and their responses in the final version.",0.8435207605361938
"This work uses a projection onto a certain hyperplane technique with an adaptive step size. Then, the authors show that it includes a non-adaptive variant, named CEG+, which covers both EG+ and FBF.","The paper considers the saddle point problem of finding non-convex/non-concave minimax solutions. Building onEG+ of Diakonikolas et al., 2021 that works under weak MVI conditions, the work presents a new algorithm CurvatureEG+ that works for a larger range of weak MVI condition compared to previous work and also works for the constrained and composite cases. The authors show cases where this algorithm converges while the previous algorithms can be shown to reach limit cycles. Overall, this theoretical work seems strong. Most reviewers seem to agree that the contribution is good enough for publication. Compared to EG+ the additional contribution is to expand the range of weak MVI condition",0.8400298357009888
This paper presents a comprehensive analysis of the limit of multi-head attention. The proposed compositional attention mechanism is rational. The paper points out the shortcomings of rigid search-and-retrieval coupling in standard multi-head attention. The paper should also compare with other variants of multi-head attention mechanisms. The paper should explore the effect of the proposed compositional attention mechanism on the latest pre-trained language models.,"This paper identifies a limitation with current attention in transformers where they scoring with query-key pairs is strongly tied to retrieving the value and proposes a more flexible configuration that subsumes the previous setup but provides more flexibility. The authors shows this leads to improvements in various settings. Overall, all reviewers seem to agree there is interesting insight and results in this paper and it merits publication. Also the discussion helped stress important points regarding weight sharing and more. One concern is that the model was not evaluated on standard NLP/vision datasets (I assume alluding to GLUE/SuperGlue/SQuAD, etc.), and authors seem to hint that pre-training this is an issue for them computationally. This",0.8411021828651428
"The paper demonstrates that overclustering is a viable approach to hyper-parameter optimization that can achieve better fine-grained clustering due to learning dense features. The proposed method is straightforward and the paper is easy to follow. The performance boost compared to SOTA for clustering seems to be very good (Table 1). The experiments are well executed, and clearly communicated. The quantitative results for clustering, and the qualitative figures in the supplementary material are both impressive, and they support the authors' claims The method has novel components which are provably attributed to these results. The paper is well rounded, with all aspects detailed to completion. There are no","All the reviewers liked the paper. The proposed method contains novel ideas of learning feature representation to maixmize the mutral informatio nbetween the latent code and its corresponding observation for fine-grained class clustering. The model seems to successfully avoid mode collapse while training generators and able to generate various object (foregrounds) with varying backgrounds. The foreground and background control ability is an outstanding feature of the paper. Please incorporate the comments of the reviewers in the final version. BTW, the real score of this paper should be 7.0 as Reviewer 5wFE commented that he/she would raise the score from 5 to 6 but at the time of this meta review, ths core was",0.8398131132125854
"The proposed MEME is a straightforward extension of an existing semi-supervised VAE to a multimodal setting, which is easy to understand and can be easily adapted to partial train settings. The authors do not explain why MEME shows a better trend than MVAE or MMVAE in the relatedness experiments. The fact that MEME is essentially not scalable to more than two is a fatal problem in the study of multimodal VAEs.","PAPER: This paper introduces a new method to learn joint representations from multimodal data, with potentially missing data. The primary novelty builds from the idea of semi-supervised VAE, introducing the concept of bi-directional information flow, which is termed “mutual supervision”. This approach brings the same advantages of semi-supervised VAE to the multimodal setting, allowing the cross-modal interactions to be modeled in the latent space. DISCUSSION: The discussion brought many important issues, addressed by both reviewers and authors. In general, it seems that most reviewers appreciate the technical novelty of the paper, related to the mutual supervision. While some concerns were expressed about the similarity with semi-supervised VAE (Joy",0.8554995656013489
"This paper investigates a novel area, which seems very important at a high level. it also proposes novel methods to address this area. There are lots of different ideas and methods presented, but the takeaways are unclear. The paper is very strong, well-motivated, and empirically sound.","Exploration can happen at various levels of granularity and at different times during an episode, and this work performs a study of the problem of exploration (when to explore/when to switch between exploring and exploitation, at what time-scale to do so, and what signals would be good triggers to switch). The study is performed on atari games. Strenghts: ------------ The study is well motivated and the manuscript is overall well written Studies a new problem area, and proposes an initial novel method for this problem extensive study on atari problems Weaknesses -------------- some clarity issues as pointed out by the reviewers no illustrative task is given to give a more intuitive exposition of the ""when to explore""",0.8511828780174255
"The paper proposes a method to estimate the gap between the true model and the learned model. The experiments verify such measurements are closely related to the performance. I think this problem is important and should be studied in-depth. The experiments in section 4.2 are a bit confusing. The non-penalty methods cause overestimation and poor performance, and the resulting policy may be similar to a random policy. I suggest using the trajectories from the medium-replay dataset to evaluate since it covers a large state-action space. The aim of section 5.3 is to study the effect of rollout horizon. It is unclear how the results presented in table","This paper empirically studies various design choices in offline model-based RL algorithms, with a focus on MOPO (Model-based Offline Policy Optimization). Among the key design choices is the uncertainty measure used in MOPO that provides an (approximate) lower bound on the performance, the horizon rollout length, and the number of model used in ensemble. The reviewers are positive about the paper, found the experiments thorough, and the results filling a gap in the current literature. They have raised several issues in their reviews, many of which are addressed in the rebuttal and the revised paper. I would like to recommend acceptance of the paper. Also since the results of this work might be of interest to many researchers",0.8418238759040833
"The paper proposes a framework for automated extraction of NAS architectures with high efficiency and fidelity. The authors propose a new analysis method to precisely recover the exact hyper- parameters without any prior knowledge. The experiments mainly focus on extracting NAS models. However, in real-world scenarios, the variants of standard models are widely used. The claims are supported by a proper empirical study considering various scenarios. The paper is very well organized and the writing is very clear. Especially, explanations regarding rather technical or hardware-related aspects are very well done and understandable. The color schemes of figures should be adapted to be more inclusive (for red-green blindness",All reviewers agree on acceptance and I agree with them. I recommend a spotlight.,0.829274594783783
"The authors propose to optimize the state-action stationary distribution directly, which avoids the instability caused by triple optimization problems for the actor, the critic and the cost Lagrange multiplier with three different objective functions in the existing work that manipulates both Q-function and policy.","This paper presents a new technique for constrained offline RL. The proposed method is based on reducing a nested constrained optimization problem to a single unconstrained optimization problem that can be efficiently represented with a neural network. The proposed algorithm is tested against several baselines on both random grid-worlds and continuous environments. Results clearly show that the proposed algorithm outperforms baselines while keep the provided constraints satisfied. The reviewers agree that the paper is well-written, the proposed algorithm is novel and technically sound, and the empirical evaluation clearly supports the claims of the paper. There were some concerns regarding the novelty of this idea, but these concerns were properly addressed by the authors in the discussion.",0.8304873108863831
"The paper proposes a novel architecture that can train visual-locomotion policies end-to-end, and demonstrated good navigation/obstacle avoidance/uneven terrain walking results in the simulation. The main weakness of the paper: Not enough baselines to compare with. The authors may also shed light on under which scenarios we should choose RL-trained robots over Spot. The authors may also shed light on under which scenarios we should choose RL-trained robots over Spot.","The paper addresses vision-based and proprioception-based policies for learning quadrupedal locomotion, using simulation and real-robot experiments with the A1 robot dog. The reviewers agree on the significance of the algorithmic, simulation, and real-world results. Given that there are also real-robot evaluations, and an interesting sim-to-real transfer, the paper appears to be an important acceptance to ICLR.",0.8434208035469055
"The paper presents comprehensive ablation studies and promising results compared to CNN-based benchmarks. The results in Table 2 also show that using a powerful CNN-based discriminator better than Transformer-based discriminator. The authors also point out the limitation of current method on high-resolution image generation. The challenges of extending transformer to such a scenario may arise from several aspects, including computational overhead and effectiveness of using global context, as the low-level features in high-resolution is supposed to be naturally local. The authors claim to design a new GAN with these state-of-the-art GANs on high-resolution","The paper proposes a GAN architecture with a ViT-based discriminator and a ViT-based generator. The paper initially received a mixed rating with two ""slightly above the acceptance threshold"" ratings and ""three slightly below the acceptance threshold"" ratings. Several concerns were raised in the reviews, including whether there are advantages of using a ViT-based GAN architecture over the CNN-based GAN and whether the proposed method can be extended to high-resolution image synthesis. These concerns are well-addressed in the rebuttal with most of the reviewers increasing their ratings to be above the bar. The meta-reviewer agrees with the reviewers' assessments and would like to recommend acceptance of the",0.8539479374885559
The paper is well written and the problem well motivated. The key contribution seems to be extension of soft decision trees which is a nice and clinically useful contribution. I am not sure how partial observability plays a role and the contributions from the partial observability perspective are unclear.,"This paper proposes a tree-based method for interpretable policy learning, for fully-offline and partially-observable clinical decision environments. The models are trained incrementally, as patient information becomes available. The method was overall deemed novel by the reviewers, and the interpretability of the model well validated by clinicians. Numerous points of clarification were brought up by reviewers, related to the notation, learning process and result reporting. All of the concerns were responded to by the authors in great detail and the manuscript was appropriately revised. All the reviewers have raised their scores as a result of the updates. Thus, the paper is ready for acceptance.",0.8431058526039124
This paper addresses an important problem with a good solution is theoretically sound is well written and easy to follow. The problem of forward knowledge transfer is an unexplored problem in the continual learning literature. The paper reads well. Experiments are not extensive enough and important benchmarks are missing. It would be more than welcome if more convincing proof could be provided.,"The submission addresses the problem of whether or not to update weights for a previous task in continual learning. The approach is to specify a trust region based on task similarity and update weights only in the direction of the tasks that are similar enough to the current one. The paper was on the balance well received (3/4 reviewers recommended acceptance, 2 with scores of 8) and complemented for its simple but effective approach, and good discussion of related literature. The submission attracted a reasonable amount of engagement and discussion between reviewers and authors, which should be taken into account in the final version of the paper.",0.8453702330589294
"The paper considers the true data-generating mechanism and attempts to recover the latent based on assumptions on knowledge of the mechanism. The paper is well written and, despite being quite technical in nature, accessible and not too hard to follow. The paper is well written and, despite being quite technical in nature, accessible and not too hard to follow. The paper does not present any empirical results or experimental analysis. This is partly justified by being clearly a theory paper, but some aspects could also be probed empirically: e. g., by rendering some data using one of multiple known mechanisms, training a (Slow)VAE-type","The paper provides new insights about how to identify latent variable distributions, making explicit assumptions about invariances. A lot of this is studied in the literature of non-linear ICA, although the emphasis here is on dropping the ""I"". I think more could be said about how allowing for dependencies among latents truly change the nature of the problem since any distribution can be built out of independent latents, by some more explicit contrast against the recent references given by the reviewers. In any case, the role of allowing for dependencies in the context of the invariances adopted is discussed, and despite no experimentation, the theoretical results are of general interest to the ICLR community and a worthwhile contribution to be",0.8189033269882202
"The authors propose three layer fusion methods which have been explored in previous methods in my understanding. The authors should carefully claim that the fusion methods are ""proposed"" in this paper. The over-smoothing problem is neither well theoretically analyzed nor empirically proved to be addressed via layer fusion methods.","This paper has a deep analysis of the over-smoothing phenomenon in BERT from the perspective of graph. Over-smoothing refers to token uniformity problem in BERT, different input patches mapping to similar latent representation in ViT and the problem of shallower representation better than deeper (overthinking). The authors build a relationship between Transformer blocks and graphs. Namely, self-attention matrix can be regarded as a normalized adjacency matrix of a weighted graph. They prove that if the standard deviation in layer normalization is sufficiently large, the outputs of the transformer stack will converge to a low-rank subspace, resulting in over-smoothing. In this paper, they also provide theoretical proof why higher",0.8379138708114624
"This paper gives a nice overview of the techniques used and improves on them in comparison to previous work. The sketch of proofs are quite detailed. This a quite dense paper, with many definitions (for all architectures studied) but I found it clear and well written. The main concern is that Assumption 1 and Assumption 2 are restrictive and even a bit unrealistic for some architectures. For a non-homogeneous network architecture, scaling up each entry of the weight can in fact drastically change the correctness of the prediction. The final result (the convergence to a rank one solution) is new for the deep non-linear case.",*Summary:* Low-rank bias in nonlinear architectures. *Strengths:* - Significant theoretical contribution. - Well written; detailed sketch of proofs. *Weaknesses:* - More intuitions desired. - Restrictive assumptions. *Discussion:* Authors made efforts to improve the discussion in response to 6P7z. Authors agree with eeoo about Assumption 2 being relatively restrictive but point out that main results do not need it. They discuss Assumption 1 and revised it formulation. Reviewer eeoo was satisfied with this. Following the discussion udhX raised their score (after authors acknowledged an early problems and improved them) and found the paper well written with novel,0.8433651328086853
This paper addresses the important delayed reward problem in RL. The proposed RRD method is simple but somewhat novel. The empirical results show visible improvement of RRD over baseline methods and demonstrate the effectiveness of RRD.,"Description of paper content: The paper addresses the problem of credit assignment for delayed reward problems. Their method, Randomized Return Decomposition, learns a reward function that provides immediate reward. The algorithm works by randomly subsampling trajectories and predicting the empirical return by regression using a sum of rewards on the included states. The method is compared to a variety of existing methods on Mujoco problems in “episodic reward” settings, where the reward is zero except for the final step of the episode, where it is the sum of rewards from the original task. Theoretical argument suggests the method is an interpolation of return decomposition (regress based on all states, not a subsample) and uniform reward distribution (send",0.847863495349884
This paper studies the implicit regularization of SGD in the manifold of zero training loss using 2 scaling to separate the fast and slow motion of SGD.,All the reviewers agree that this paper made a solid contribution of understanding the algorithmic regularization of SGD noise (in particular the label noise for regression) after reaching zero loss. The framework is novel and has the potential to extend to other settings.,0.8778000473976135
"This paper proposes a new method to detect anomalies in time-series data. The authors propose a combination of the RNN, the DAG to capture the dependency structure, and the normalizing flow. The experimental results suggest that the joint learning, with enforcement of an acyclic graph, can lead to better performance. The authors provide an ablation study to demonstrate that all of the components of the algorithm contribute to the improved performance. There are no measures of variability or confidence intervals for the provided results. The experiments on the traffic dataset seem incomplete and it is difficult to conclude much from them. There are no tests to determine if the performance differences","The paper tackles the problem of detecting anomalies in multiple time-series. All the reviewers agreed that the methodology is novel, sound and very interesting. Initially, there were some concerns regarding the experimental evaluation, however, the rebuttal and subsequent discussion cleared up these concerns to some extent and all reviewers are eventually supporting or strongly supporting acceptance.",0.8522673845291138
"The paper introduces complex topics well. The analyses of the UCI datasets are comprehensive. Overall, the proposed autoregressive quantile flow is a reasonable application of normalizing flow in quantile regression. It is also extendable to various different flow transformations and applicable to a wide variety of regression problems. However, this paper has not provided a very clear and strong motivation of why using normalizing flow (or autoregressive flow in particular) for uncertainty estimation, given there are many models that can be trained by gradient-based methods. In addition, the authors may also want to explain the procedure more clearly by adding an algorithm box into the main paper","The paper proposes a framework for training autoregressive flows based on proper scoring rules. The proposed framework is shown to be a computationally appealing alternative to maximum-likelihood training, and is empirically validated in a wide variety of applications. All three reviewers are positive about the paper and recommend acceptance (one weak, two strong). The reviewers describe the paper as well written and well motivated, and recognize the paper's contribution as significant. Overall, this is a nice and promising methodological exploration of flow-model training that is worth communicating to the ICLR community.",0.8581899404525757
"This paper proposes a method to reduce the small-sample bias of a few-shot classification. The proposed approach is compared against sensible baselines (L2 regularization, label smoothing), and considers multiple benchmarks and backbone architectures. The reviewers found the paper to be clear, well-written, and the experimental results convincing. Overall, the paper is theoretically grounded and the experimental results are convincing.","This work starts from the observation that maximum likelihood estimation, while consistent, has a bias on a finite sample which is likely to hurt for small sample sizes. From this, they apply Firth bias reduction to the few-shot learning setting and demonstrate its empirical benefits, notably relatively to L2 regularization or label smoothing alternatives. After some discussion with the authors, all reviewers are supportive of this work being accepted. Two are also suggesting this work be featured as a spotlight. The proposed method is simple, well motivated, and appears to be effective. Therefore, I'm happy to recommend this work be accepted and receive a spotlight presentation.",0.8745492100715637
"This paper is well written and easy to follow in general. However, I have some comments and questions for the authors, which are listed below. The paper also considers mini-batch updates and constant step-sizes modifications. These can also be easily applied to the classical MARL algorithm in (Zhang et al., 2018). The authors should also compare the results with the recent globality actor-critic methods and numerical experiments that the authors need to address.",This paper provides actor-critic method for fully decentralized MARL. The results remove some of the restrictions from existing results and have also obtained a sample bound that matches with the bound in single agent RL. The authors also give detailed responses to the reviewers' concerns. The overall opinions from the reviewers are positive.,0.8696771860122681
"This paper considers extremely high alpha values, which is not even considered in the original mixup paper (standard one is alpha=1, and maybe variant the authors considered is alpha=2). I think the authors are setting extremely large alpha values to exaggerate the effect they want to show. The approach the paper takes in their study is a meaningful one, and the theory and experiments are well motivated. The paper goes beyond the observations of related works to provide concrete conditions on which Mixup training can fail. The paper takes a curious approach to existing works for understanding the generalization and robustness of the Mixup-","This paper presents an interesting analysis of mixup, discussing when it works and when it fails. The theory is further illustrated with small but intuitive examples, which facilitates understanding the underlying phenomena and verifies correctness of the predictions made by the theory. The submission has received three reviews with high variance ranging from 3 to 8: mn55 favoring rejection while eGEK recommending accept. I read all the reviews and authors' response. Unfortunately, mn55 did not follow up to express how convinced they are with author's reply, but I do find the responses to mn55 very solid and convincing. In concordance with eGEK, I do find the provided analysis important and helpful, and the presentation of the theory",0.835785448551178
The paper is well motivated and the method is clearly presented with all its technique points delivered. Several concerns during my reading are well addressed. The discussion about traditional methods performing poorly on feature 1 seems important enough to be included in the core manuscript. The paper is well motivated and the method is clearly presented with all its technique points delivered. The discussion about traditional methods performing poorly on feature 1 seems important enough to be included in the core manuscript. The paper is well motivated and the method is clearly presented with all its technique points delivered. The discussion about traditional methods performing poorly on feature 1 seems important enough to be included in the core manuscript.,"This paper proposes a feature selection method to identify features for downstream supervised tasks, focused on addressing challenges with sample scarcity and feature correlations. The proposed approach is highly motivating in biological and medical applications. Reviewers pointed out various strengths including potential high impacts in biomedical applications, technical novelty and significance, and comprehensive and illustrative experiments. The authors adequately addressed major concerns raised by reviewers.",0.8246738910675049
"The paper proposes a novel class of continuous-time diffusion-based generative models, called critically-damped Langevine diffusion (CLD). The authors show that a score matching objective for the proposed method requires only vtlogpt(vt|xt), not (xt,vt)pt(xt,vt).","The paper develops a diffusion-process based generative model that perturbs the data using a critically damped Langevin diffusion. The diffusion is set up through an auxiliary velocity term like in Hamiltonian dynamics. The idea is that picking a process that diffuses faster will lead to better results. The paper then constructs a new score matching objective adapted to this diffusion, along with a sampling scheme for critically damped Langevin score based generative models. The idea of a faster diffusion to make generative models is a good one. The paper is a solid accept. Reviewer tK3A was lukewarm as evidenced by their original 2 for empirical novelty that moved to a 3. From my",0.8440765142440796
"The paper presents a simple, but principled and novel way of constructing orthogonal classifier for any non-linear classifiers. The effectiveness of the proposed technique has been confirmed on various applications, including style transfer, domain adaptation, and fairness. The proposed approach appears to yield improvements for both the controlled style transfer and domain adaptation problems. There are quite a few missing references to methods that also learn multiple complementary classifiers. So it's difficult to evaluate the novelty/advantage of the proposed method. The concepts of ""principal classifier"" and ""principal variables"" are used in the abstract and intro without being defined or given a reference about. The","This paper introduces the concept of classifier orthogonalization. This is a generalization of orthogonality of linear classifiers (linear classifiers with orthogonal weights) to the non-linear setting. It introduces the notion of a full and principal classifier, where the full classifier is one that minimizes the empirical risk, and the principal classifier is one that uses only partial information. The orthogonalization procedure assumes that the input domain, X can be divided into two sets of latent random variables Z1 and Z2 via a bijective mapping. The random variables Z1 are the principal random variables, and Z2 contains all other information. Z1 and Z2 are assumed to be conditionally independent given the target label.",0.83855140209198
"This paper proposes an approach for relational pathfinding that can be applied to large-scale learning tasks in noisy domains. The main weakness of the approach is the grounding problem, which needs to search for some trickyness before training. Besides, the proposed method seems more efficient than what approach? The reviewers agree that the proposed approach is reasonable and effective for the targeted 2 tasks.","A novel method is described that uses RL to search for a rule set which predicts multiple relations at once for KBC-like problems. The rules can include latent predicates, which reduces the complexity of individual rules, similar to Cropper & Muggleton's (2015) meta-interpretive learning framework, which is usual for rule-learning systems. Another novel aspect is use of a cache memory for rules. Pros - the idea of using RL instead of carefully-designed discrete search for symbolic learning systems is a very nice novel idea - the experimental results are strong Cons - the benchmarks are synthetic (although GraphLog does at least include noise) - although the Cropper and Muggleton work is",0.8418993949890137
"This paper is well-written, and the technical part of this paper is easy-to-follow. The REP-UCB algorithm is conceptually very simple. It alternates between learning a model (using MLE) and finding an optimistic policy. In terms of the complexity, the REP-UCB algorithm is oracle efficient, using the same oracle as FLAMBE. The paper significantly reduces the sota sample complexity in the online setting to a reasonable level. Although technically speaking, FLAMBE focuses on reward-free exploration, where this paper focus on online setting. In contrast, previous results in offline setting assumes a known","In this paper, the authors extend the FLAMBE to the infinite-horizon MDP and largely improved the sample complexity of the representation learning in FLAMBE. Meanwhile, the authors also consider the offline representation learning with the same framework. Although there is still some computational issue in MLE for the linear MDP, the paper completes a solid step towards making linear MDP for practice. The paper could be impactful for the RL community. As the reviewers suggested, there are still several minors to be addressed: - The extension of the proposed algorithm for finite-horizon MDP should be added. - The directly comparison between the sample complexity of FLAMBE and the proposed algorithm in infinite-",0.851164698600769
"This paper provides a new probabilistic modelling framework named PC for lossless compression. This framework is new in AI and can be used in the context of deep generative models. The paper presents a new neural compression method that can be used in the context of deep generative models. The method is shown to be significantly faster than existing methods. The paper presents this as though a significant breakthrough has been made in terms of runtime, but the PC method is only compared to slow implementations of existing methods, which were not optimized for speed. One example of a faster implementation of a neural compression method is BB-ANS with a small VAE, implemented","The paper revisits lossless compression using deep architecture. In contrast to main stream approaches, it suggests to make use of probabilistic circuits, introducing a novel class of tractable lossless compression models. Overall, the reviews agree that this is an interesting direction and a novel approach. I fully agree. Actually, I like that the paper is not just saying well, we could use a probabilistic circuit for ensure tractability but also shows that there is still a benefit of different variable orderings for encoding and decoding. In any case, adding probabilistic circuits to the ""compression family"" is valuable and also paves the way to novel hybrid approaches, combining neural networks and probabilistic circuits. I have enjoyed reading the paper,",0.8483808636665344
"The paper presents a theoretical analysis of sim-to-real deployment via domain randomization. The submission is limited to 9 pages which the authors comply to. The main limitation of the paper is in assuming no fine-tuning with real-world samples. In the current stage I have three major concerns. First, its motivation and contribution seem to be detached from sim-to-real practice. The authors should spell out which simulator & environment combinations currently satisfy such assumptions. And if there are none (which may be okay for an otherwise interesting theoretical work), there should be at least a discussion on what are the main limiting factors of the","This manuscript introduces a theoretical framework to analyze the sim2real transfer gap of policies learned via domain randomization algorithms. This work focusses on understanding the success of existing domain randomization algorithms through providing a theoretical analysis. The theoretical sim2real gap analysis requires two critical components: *uniform sampling* and *use of memory* **Strengths** All reviewers agree that this manuscript provides a strong theoretical analysis for an important problem (understanding sim2real gap) well written manuscript, and well motivated Intuitive understanding for theoretical analysis is provided **Weaknesses** analysis is limited to sim2real transfer without fine-tuning in the real world the manuscript doesn't provide a novel experimental evaluation lack of take",0.8397059440612793
"This paper presents a sound theory of group-invariant MDP. The theory also characterizes both the invariance and equivariance cases. The main strength of the paper is the experimental results which are very supportive in a range of different tasks. The main weakness is lack of novelty, on the theoretical side. This is because, as noted by the paper itself, equivariant networks have been used for learning equivariant policies and invariant Q functions in previous work. However, given the importance of this domain, I believe the focus on this group and proposed methodology still has interesting contributions (e. g., in learning from demonstrations).","The paper investigates the use of equivariant neural network architectures for model-free reinforcement learning in the context of visuomotor robot manipulation tasks, exploiting rotational symmetries in an effort to improve sample efficiency. The paper first provides a formal definition and theoretical evaluation of a class of MDPs for which the reward and transition are invariant to group elements (""group-invariant MDPs""). It goes on to describe equivariant versions DQN, SAC, and learning from demonstration (LfD). Experiments on a set of different manipulation tasks reveal that the proposed architectures outperform contemporary baselines in terms of sample complexity and generalizability, while ablations demonstrate the contribution of the different model",0.8465163111686707
"The paper is well written, and the method is represented well. The experimental results are very sufficient and the authors provide a wide range of comparison methods. They experimented with 69 datasets and repeated 30 times for each run. There has been works in computer vision that have studied applying SSL under label-noise and semi-supervised setting, but this is the first time I have seen for the datasets. For instance, corrupting one view is better than corrupting both the views for the image.",The paper explores self-supervised learning on tabular data and proposes a novel augmentation method via corrupting a random subset of features. The idea is simple but effective. Experiments include 69 datasets and compare with a number of methods. The result shows its superiority. It would be inspiring more work for SSL on the tabular domain.,0.8634657263755798
"This paper presents a novel approach to fingerprinting a model. The paper is well written and easy to follow. The main concern is that the definition of detection and attribution of synthesised images is somewhat misleading. The application side of the proposed method is rather limited. It does not solve the current deepfake problem (strictly speaking, GAN synthesised images are not deepfake which is a different terminology. The experiments are carefully designed, and the results showed that the proposed method could provide the same level of fingerprint detection accuracy and maintain the same level of quality in the generated images as the model from Yu et al., 2021. The","The paper proposes and studies a method for the responsible disclosure of a fingerprint along with samples generated by a generative model, which has important applications in identifying ""deep fakes"". The authors establish both the detectability of their fingerprint-without significant loss of fidelity-as well as the robustness to perturbations. The reviewers found the problem and contributions to be important and significant, well substantiated by an extensive experimental study.",0.8511673212051392
This paper proposes a new way to explore large neighborhoods via auxiliary paths. The paper uses these ideas in a new way to efficiently explore large neighborhoods via auxiliary paths. The proposed is more efficient (in terms of the likelihood) than naively expanding the neighborhood using a locally-informed proposal. Using a balanced function the resulting acceptance rate takes a very simple form and can be computed efficiently. The paper uses these ideas in a new way to explore large neighborhoods via auxiliary paths. The paper uses these ideas in a new way to explore large neighborhoods via auxiliary paths. The paper uses these ideas in a new way to explore large neighborhoods via auxiliary paths,"This paper provides a novel path auxiliary algorithm for more efficiently exploring discrete state spaces within a Metropolis-Hastings sampler for energy based models. In particular, it essentially replaces the ""single site update"" by instead proposing an entire path using local information, thus enabling the chain to take larger steps, which can improve acceptance/mixing significantly as they demonstrate. The work is a timely contribution that improves upon exciting recent work. After much discussion among several knowledgeable reviewers and clarifications regarding some details of the main theorem from the authors, there is consensus that the contributions are correct, novel, and likely of impact to the machine learning community. Since the revision period, the empirical evaluations have also been improved and the",0.8401461243629456
"The paper proposes an ""achievement context"" trick that is used to filter out false-negative samples during affordance classification. The paper presents experiments that demonstrate the effectiveness of the proposed method and provides extensive ablation studies.","This paper proposes a hierarchical reinforcement learning approach that exploits affordances to better explore/prune the subtasks, and thus making the overall learning more efficient. The idea of the paper is novel and interesting. After the rebuttal, all the reviewers agree that the paper is a solid contribution. Therefore, I recommend acceptance of this paper.",0.8661391139030457
The proposed model is a novel unsupervised technique that is competitive or state of the art using a multiscale variational autoencoder framework. The paper provides extensive experiments and ablation analysis that provide interesting insights into the properties of the model.,"A multi-scale hierarchical variational autoencoder based technique is developed for unsupervised image denoising and artefact removal. The method is shown to achieve state of the art performance on several datasets. Further, the multi-scale latent representation leads to an interpretable visualization of the denoising process. The reviewers unanimously recommend acceptance.",0.8834288120269775
The paper presents an efficient method for training neural networks for physical simulations. The idea is simple and easy to follow. The computational cost seems high. The written and organization of paper is clear and easy to follow. The manuscript has shown convincing theoretical intuition and validated observations on valuable physical problems.,"Thank you for your submission to ICLR. The reviewers and I are in agreement that the paper presents a substantial contribution to the field at the intersection of differentiable simulation and ML methods. In particular, the half-inverse method is compelling, non-obvious, and hints of a nice path forward towards the goal of practical differentiable simulations within models. Overall I'm happy to recommend the paper be accepted.",0.8574753403663635
"This paper uses neural networks to learn the complicated function. This step makes the reviewer confused. What’s the point of training a neural network to learn it instead of learning the unknown parts in that equation? Also, why do we use the gradients as the input for the neural network instead of putting more known items in the input, e. g., A_t in the lemma D.1 (does this give more info to the training process)? The proposed algorithm uses a novel and interesting network architecture, EE-Net, which is different from prior UCB-based [Zhou et al. 2020] or Thompson sampling","Summary: This paper studies the neural contextual bandit problem, and proposes a neural-based bandit approach with a novel exploration strategy, called EE-Net. Besides utilizing a neural network (Exploitation network) to learn the reward function, EE-Net also uses another neural network (Exploration network) to adaptively learn potential gains compared to currently estimated reward. Discussions: The reviewers appreciated the novelty and the quality of the ideas and results in this paper. Most questions were about details in algorithm design choices and in the analysis. The authors have addressed these questions and updated their draft. The reviewers have now reached a consensus and recommend accepting this paper. Recommendation: Accept.",0.8483745455741882
"The proposed method is very practical and the ideas are organized logically. The empirical methodology appears standard, and is reported in sufficient detail to recreate the results. The writing is clear and succinct. The paper is very clearly written and has an extensive introduction that allows to introduce the scientific problem. The authors should test their idea on spiking neurons (eg LIF) as opposed to LSTMs. Only the Google Speech Command dataset is not toy, and the accuracy they get on this dataset is below the SOTA. The paper is very clearly written and has an extensive introduction that allows to introduce the scientific problem. The method is briefly described and then","The authors propose a rank coding scheme for recurrent neural networks (RNNs) - inspired by spiking neural networks - in order to improve inference times at the classification of sequential data. The basic idea is to train the RNN to classify the sequence early - even before the full sequence has been observed. They also introduce a regularisation term that allows for a speed-accuracy trade-off. The method is tested on two toy-tasks as well as on temporal MNIST and Google Speech Commands. The results are very good, typically improving inference time with very little loss in accuracy. Furthermore, the idea seems novel and the paper is well written. An initial criticism was that experiments with spiking",0.845805823802948
"This paper proposes a novel method to tackle the problem of query-based black-box defense. The proposed ZO-AE-DS paradigm is novel to me. The idea of black-box defense is very interesting, which is actually a very important real-world application scenario. The experiments are all compared with FO-AE-DS and its variants. Then they demonstrate it with experiments that AE can also improve robustness. The paper is overall easy to follow, and clearly written. Experiments are conducted on CIFAR-10 and STL-10. Also, It can be observed that the larger the picture, the worse the defense effect is. The","All reviewers have converged to an unanimous rating of the paper, highlighting, in the paper or during the discussion, many strengths, including a compelling approach clearly relevant to applications and its solid range of experiments. A clear accept, and I would encourage the authors to push in the final version the experiments and discussions following the threads with reviewers (in particular, Vo8C and ULvk). Thanks also to authors and reviewers for a thorough discussion which helped to strengthen further the paper's content. AC.",0.8278445601463318
"This paper proposed a relax loss to defend privacy leakage, since the authors find that membership privacy risks can be reduced by narrowing the gap between the loss distributions. The authors give an analysis for ""Relax increases the variance of the training loss distribution"" in appendix A.",The paper proposes an approach and specific training algorithm to defend against membership inference attacks (MIA) in machine learning models. Existing MIA attacks are relatively simple and rely on the test loss distribution at the query point and therefore the proposed algorithm sets a positive target mean training loss value and applies gradient ascent if the average loss of current training batch is smaller than it (in addition to the standard gradient descent step). The submission gives extensive experimental results demonstrating advantage over existing defense methods on several benchmarks. The primary limitation of the work is that it defends only against rather naive existing attacks which do not examine the model (but rely only on the loss functions).,0.8484803438186646
This paper studies the Contrastive Learning (what happens when the discrimination task is too easy) and Generative Modelling (what happens when the data and noise are too far from each other). The authors give a concrete reason and example to explain why NCE can fail in practice in certain situation. The paper is overall clearly written and easy to follow. I am overall happy with the paper. Please see the detailed comments below for all questions / wishes for the authors.,"This contribution investigates and takes a step back on an important problem in recent ML, namely the impact of the noise distribution in density estimation using Noise Contrastive Estimation. The work offers both theoretical insights and convincing experiments. For these reasons, this work should be endorsed for publication at ICLR 2022.",0.8536890745162964
"This paper addresses an important gap in differentiable physics engines. The paper takes an optimal transport based approach, which is theoretically (physically) motivated and has advantages of simplicity. The proposed approach significantly outperforms a gradient descent baseline. The paper is well-written and easy to follow.",The reviewers are unanimous that this is a strong submission that deserves to be accepted.,0.8577490448951721
"This paper proposes a simple method to fine-tune existing code translation models without the need to manually curate parallel data. The method does not require any changes to the training procedure of the model, and can be independently applied in the fine-tuning stage. The authors propose to utilize an automated test generation suite to build this parallel data from the model itself. It requires an automated unit test generation tool to be available for at least one of the languages. I have 2 concerns with this proposal: It requires an automated unit test generation tool to be available for at least one of the languages. The proposed method therefore, in my opinion, is","This paper is about unsupervised translation between programming languages. The main positive is that it introduces the idea of using a form of unit test generation and execution behavior within a programming language back-translation setup, and it puts together together a number of pieces in an interesting way: text-to-text transformers, unit test generation, execution and code coverage. Results show a substantial improvement. The main weaknesses are that there are some caveats that need to be made, such as the (heuristic, not learned) way that test cases are translated across languages is not fully general, and that limits the applicability. There are also some cases where I find that the authors are stretching claims a bit beyond what experiments",0.8444051146507263
The rejection-sampling-based method is definitely the main focus of the paper. The paper makes substantial progress in scaling up NDPPs and opens up new avenues for applying NDPPs to various real-world scenarios.,"This is an exciting paper that provide the efficient algorithms for exact sampling from NDPPs along with theoretical results that are very pertinent in and out themselves. The AC agree with the reviewers that the authors satisfactorily addressed the concerns raised in the reviews, and is convinced that the revised version will be greatly appreciated by the community. We very much encourage the authors to pursue this line of work and in particular to overcome the practical restriction to the ONDPP subclass.",0.8625884652137756
"This work is an extension of the work by Bradshaw, Gottipatti and Horwood, which addresses some of the limitations of these models. The paper should be clearly presented as an investigation and extension of the work by the authors. There is no mention of the RetroDog model by Bradshaw. This would not detract from its quality or value, which in my opinion is considerable.","After much back and forth about prior work, 3 reviewers score this paper as an 8 and one scores it as a 3. Other reviewers have written to the 3 and told them they believe that their review is now too harsh, in light of clarifications w. r. t. related work. I tend to agree, though I must admit that I am not an expert on this topic. Given that there is almost unanimous support for accepting and it's possible that the one hold-out has not seen some of the extra information, I recommend acceptance. Given the praise from the other three reviewers, I moreover recommend a spotlight.",0.8348283171653748
"This paper studies the problem of computationally designing antibody CDRs. The generation method is flexible and the authors adapt it for conditional generation. The paper is well-written and easy to follow. The proposed refinement method has high novelty and outperform state-of-the-art baseline methods. The experimental evaluation may be problematic, it is not convincing to use machine learning methods to predict the neutralization ability based on CDR H3.","This paper proposes use of a novel generative modelling approach, over both sequences and structure of proteins, to co-design the CDR region of antibodies so achieve good binding/neutralization. The reviewers are in agreement that the problem is one of importance, and that the technical and empirical contributions are strong. There are concerns over the relevance of evaluating the method by using a predictive model as ground truth. Still, the overall contributions remain.",0.8747814297676086
"This paper presents a new method to predict the future churn of a model. The authors provide theoretical guarantees to justify the use of their algorithm. The authors provide empirical justifications of the proposed algorithm using many datasets and baselines. There is some citation on these in the related work, however the problem introduction needs some justification e. g. literature on churn being hurtful in production settings etc. The paper can be improved by addressing a few points as below: The authors presented the claim that their process corresponds to a value of  that constrains the churn in the primal setup. While possibly trivial, the methods to use this distillation process","The paper introduces a procedure to control the churn (i. e. differences in the predictive model due o retraining) using distillation. This is a strong paper, with novel technique which is clearly presented, and is backed by sound theory. The experimental results were also deemed extremely convincing by reviewers TJ4g and pZBb. Reviewer nqfu raised a question about the similarities between churn reduction and domain adaptation. The authors have addressed this by pointing out similarities to their work but also noting that, in the settings mentioned by the reviewer, alternative approaches such as completely retraining the model might be more appropriate. This part of the rebuttal is convincing. Reviewer TJ4g has pointed out several",0.8474727869033813
"The paper presents a method that is novel in terms of the motivation, theory and experiments, in order to learn the structural function. The authors present error analyses for the density ratio and the causal function, as well as experiment results comparing their work to recent algorithms in the field. In particular, as a minor point, I think the abstract is a bit too vague and would benefit from a more precise description of what the paper is about. I appreciate the honest assessment of the relevance of the chosen settings for the proposed method. The experiments are well described and motivated. I appreciate the honest assessment of the relevance of the chosen settings for the","This paper proposes a method that uses conditional moment restriction methods to estimate causal parameters in non-parametric instrumental variable settings. This is done by converting to an unconditional moment restriction setting common in the econometrics causal inference literature. The paper was reviewed quite favorably by reviewers, and the authors updated the manuscript to address specific issues raised by reviewers.",0.8493472933769226
"This paper proposes a new algorithm to improve the performance of generative models. The paper proposes a new algorithm to improve the performance of generative models. The authors provide a number of clarifications and illustration figures. The writing is overall clear. There are abundant experiments, with many comparisons among different methods and ablation studies of the proposed method. There are some vague parts in the current version. I would raise my score if the authors could properly address the questions.","The paper provides a method for with tuning continuous hyperparameters (HPs). It is closely related to a previous work (Lorraine, 2019) that was limited to certain HPs, and in particular could not be applied to HPs controlling the learning such as learning rate, momentum, and are known to be influential to the convergence and overall performance (for non-convex objectives). The reviews indicate a uniform opinion that the paper tackles an important problem, that its methods provide a non-trivial improvement over previous techniques and in particular those of (Lorrain, 2019), and that the provided experiments are extensive and convincing. The initial reviews had several concerns about technical details in the paper such as",0.8437319397926331
"This paper proposes a new method for down-weighting by variance and using probabilistic ensembles to improve the performance of Reinforcement Learning. The reviewers found the paper to be very well written and the approach very well embedded in the current literature. There were some concerns about the lack of clear math and pseudocode applying the method to the RL setting, and the analysis of the sources of uncertainty (sec 3.1.2) is perhaps novel. Despite the lack of novelty, if the theoretical or empirical results were strong, then acceptance could be justified. The proposed IV-RL method seems to be an improvement without drawbacks (except for additional hyper-",The reviewers unanimously appreciated the clarity of the work as well as the framing of the proposed method. Congratulations.,0.8533623814582825
This work fails to discuss a whole body of relevant literature that also aims to train neural networks. As a result the chosen baselines for the experiments section fail to show how effective this method actually is.,"This work considers one-shot pruning in deep neural networks. The main departure from previous work is to consider stochastic Frank-Wolfe. The reported results are convincing although a number of baselines were missing from the initial submission. The authors provide a balanced account of the strengths and weaknesses of the proposed approach. The authors adequately addressed the concerns of the reviewers. For instance they ran additional experiments to compare to missing pruning baselines. I would encourage the authors to revise the manuscript by including the missing related work, the additional clarification discussions (e. g., motivation for K-sparse constraints, follow-up analysis, and cost per iteration) and to include the additional experiments that were conducted (",0.8518803715705872
"This paper presents a novel approach to the problem of skill discovery for RL and continuous control. The proposed model integrates both discrete and continuous latent variables into a policy. The proposed architecture seems to work, and I was particularly interested in the comparison with a hierarchical behaviour cloning model. Results show the proposed architecture seems to work, and I was particularly interested in the comparison with a hierarchical behaviour cloning model. This area is of particular interest as we move beyond ""look we discovered some skills and re-used them"" to more realistic online learning settings. The paper is in some need of smoothing, and I found this a difficult read, despite being","This work propose to learn hierarchical skill representations that, as opposed to prior work, consist of both discrete and continuous latent variables. Specifically, this work proposes to learn 3 level hierarchy via a hierarchical mixture latent variable model from offline data. For test time usage and adaptation on down-stream tasks, the manuscript proposes two ways of utilizing the learned hierarchy in RL settings. **Strengths** A novel method to learn hierarchical representations with mixed (discrete/continuous) latent variables is proposed Detailed experimental evaluation, and baseline comparisons, show promising results **Weaknesses** There were various clarity issues as pointed out by the reviewers (fixed in rebuttal phase) The related work was missing relevant work, and the",0.8481714129447937
"This paper is well written and organized. Overall, this paper is well written and organized. However, there are no comparisons made to those presented in Yuan et al., 2020, which introduced the AUCM loss. However, there are no comparisons made to those presented in this paper. The paper is well written and organized. Overall, this paper is well written and organized.",I recommend this paper to be accepted. All reviewers are in agreement that this paper is above the bar.,0.859978199005127
"The proposed method- using Bivariate Shapley values to study feature interactions- is novel and reasonable. Considering data in DAG format is reasonable and their connections are represented as edges. The paper provides extensive empirical results, which indicate that the approach is a good explanation for measuring the influence of features on each other. The authors provide both theoretical justification as well as good illustrative examples. The proposed method performed better in both types of tasks. One important aspect is that there is quite some noise in the graph edge from Shapley estimation. The authors claim that their method can be extended into high-order scenarios, and provide a method to calculate","In this paper, the authors generalize the univariate Shapley method to bivariate Shapley method. The authors first build a directly graph based on the asymmetric bivariate Shapley value (adding feature j to all sets contained feature i). Then several graph algorithms are applied to analyze the directly graph to derive (1) univariate feature importance available in univariate approach and (2) relations like mutually redundancy only available in bivariate approaches. Experiments on several datasets with comparison to existing methods demonstrated the superiority of the proposed method. All reviews are positive.",0.8502970933914185
"The paper is an impressive empirical study covering a vast range of variations, baselines and ablations. All experiments are repeated multiple times and the paper is well summarized. The necessary background and related work is briefly, but well summarized. Experiments are and findings are well presented - though given the sheer number of experiments and results some parts of the paper appear a bit crammed (but there is no easy solution to this given the limitations of a conference-format paper). The paper lays the groundwork for now digging deeper into some of the findings. I have some small suggestions for improvements, but am currently in favor of accepting the paper.","This paper studies the Lottery Ticket hypothesis in reinforcement learning for identifying good sparse representations for low-dimensional tasks. The paper received initial reviews tended towards acceptance. However, the reviewers had some clarification questions and concerns. The authors provided a thoughtful rebuttal. The paper was discussed and most reviewers updated their reviews in the post-rebuttal phase. Reviewers generally agree that the paper should be accepted but still have good feedback. AC agrees with the reviewers and suggests acceptance. However, the authors are urged to look at reviewers' feedback and incorporate their comments in the camera-ready.",0.8374848365783691
This paper focuses on class-imbalanced learning and shows that self-supervised training yields that are more robust to imbalance datasets. The paper is well written in general and its motivation is clear. The only task considered in the paper is image classification using the standard accuracy as the evaluation metric. Is the observed robustness also valid for other tasks?,"This paper is proposed to investigate the robustness of self-supervised learning (SSL) and supervised learning (SL) in both balanced (in domain) and imbalanced (out of domain) settings. It can be concluded that SL can regularly learn better representations than SSL, and representations are better from balanced than from imbalanced datasets. The SSL is more robust than SL in the imbalanced settings, which is the crucial of this paper. Expect the experimental results, the authors also provided theoretical analysis to support their claims. The authors also extend a well-established method SAM into the Reweighted SAM as the technical contribution to better address the imbalanced setting. The paper is well written with clear logic to follow",0.8599340915679932
"This paper presents an ab initio method that does not require a training dataset. It is unclear how the PESNet compares to the the trained methods when they are trained on very large and diverse datasets like OC20. The paper uses a lot of terminology that is difficult to follow for somebody without a background in chemistry. I would urge the authors to change the text to make it easier to follow. The manuscript is well written and theory and experiments are presented in a concise fashion. The idea of coupling neural wavefunctions for different nuclear configurations with a meta-GNN is promising, since this allows for simultaneous solution with VMC","This paper builds on the success of the FermiNet neural wave function framework by pairing it with a graph neural network which predicts the parameters of neural wave function from the geometry. The resulting PESNet trains significantly faster, with no loss of accuracy. This method constitutes an important advance in ML-powered quantum mechanical calculations. The reviewers unanimously recommend acceptance.",0.857977032661438
"The proposed solution to the technical challenge of the Bernstein-type analysis under this scenario is novel. Moreover, the paper is well organized. However, several issues/limitations preclude me from fully accepting this paper.","This paper addresses the reward-free exploration problem with function approximation under linear mixture MDP assumption. The analysis shows that the proposed algorithm is (nearly) minimax optimal. The proposed approach can work with any planning solver to provide an ( +opt )-optimal policy for any reward function. After reading the authors' feedback and discussing their concerns, the reviewers agree that the contributions in this paper are valuable and that this paper deserves publication. I encourage the authors to follow the reviewers' suggestions as they will prepare the camera-ready version.",0.859584391117096
"This paper considers the problem of NCD in a new perspective and rigorously formulates the NCD setting. A theoretical analysis is provided to demonstrate when the NCD setting can be solved. In addition, they reveal that NCD can be solved if known and novel classes are related. The authors give detailed algorithms and the source code, which can help readers better understand and reproduce the proposed method. All the results include the standard deviation, which is important for reflecting the real improvements on the CIFAR and SVHN. The work contributes to some intuitive notions of learnability in terms of epsilon-separation and consistent semantic transformation sets.","All reviewers believe that this paper is valuable, and the authors have made a significant, careful contribution. Some suggestions from the area chair: - ""in causality"" is not a standard technical term and also not non-technical idiomatic English, so it should be explained the first time it is used. - The authors should briefly cite and discuss research on so-called positive and unlabeled (PU) learning. This seems like the special case where there is exactly one known class and one novel class. The distinction between sampling in causality and labeling in causality appears in the PU literature, though not under this name. - The authors could also mention the obvious but surprising point that if data are generated by two",0.8305627703666687
"The paper presents an argument that model-based RL should have special value in situations when safety and efficiency is a concern. The paper raises interesting questions, but ultimately cannot compellingly make the case for the proposed approach. The paper does not systematically address where such a model would come from, or how errors in the supplied model might affect performance. This is a critical issue in differentiating between model-free and model-based RL.",The paper describes a new model-based RL technique for constrained MDPs based on Bayesian world models. It improves sample efficiency and safety. The reviewers are unanimous in their recommendation for acceptance. This represents an important advance in RL. Great work!,0.8748064041137695
"This paper observes that when the encoder and decoder belong to the exponential family, only general linear models satisfy ‘consistency’ or even approximate consistency. The paper also shows in Corollary 1 that a flexible encoder network cannot make up for the lack of flexibility in encoder distribution.",Wide agreement from the reviewers. Interesting theorems. Empirical work illustrates the theory. Claim and insight: failure of VAEs is caused by the inherent limitations of ELBO learning with inflexible encoder distribution. Good discussion pointed out related work and insights from the experiments.,0.843722939491272
"This paper consolidates a number of existing techniques, including TDMs, Learning from Play, GCSL, and Decision Transformer (DT), into the same umbrella of ""hindsight information matching"" algorithms in the spirit of hindsight relabeling.","The paper describes a framework that unifies several previous lines under hindsight information matching. Within that framework, the paper also describes variants of the decision transformer (DT) called categorical DT and unsupervised DT. The rebuttal was quite effective and the reviewers confirmed that their concerns are addressed. The revised version of the paper is significantly improved and consists of an important contribution that should interested many researchers. Well done!",0.8559441566467285
"The paper builds up a variety of optimization methods, building on a line of work in the LFI literature. This leads to a lot of approaches to compare, and there is no clear indication as to what practitioners should use in practice. The proposed algorithms are all sensible tasks, and the connection between the two fields seems like a fruitful area for further exploration.","The paper investigates various approaches, and a unifying framework, for sequence design. There were a variety of opinions about the paper. It was felt, after discussion, that the paper would benefit from a sharper focus, and somewhat suffers from being overwhelmed by various approaches, lacking a clear narrative. But overall all reviewers had a positive sentiment, and the paper makes a nice contribution to the growing body of work on protein design.",0.8701437711715698
The paper proposes a new decoupled formulation of time series forecasting and formulate the forecasting problem in the modelTS by introducing a prior estimated hidden periodic state and a expansion framework based on residue learning. The proposed model achieves better performances across all datasets compared with state-of-the-art N-BEATS. Especially I find out that DEPTS has a 10% improvement for M4 competition compared with state-of-the-art N-BEATS.,"The paper proposed a novel deep learning model specifically designed for periodic time series forecasting problems. The approach includes lay-by-layer expansion, residual learning, and periodic parametrization. The model outperforms state-of-the-art baselines on several time series forecasting benchmarks. The reviewers appreciate the extensive experimental results, but also suggested improvement on writing and comparison regarding the parameter efficiency of the model.",0.8722603917121887
This paper is generally clear and well written. It could be helpful to add citations in a few places to help the reader who is not already familiar enough with the graph generative models community.,"This paper provides an overview of evaluating graph generative models (GGMs). It systematically evaluates one of the more popular metrics, maximum mean discrepancy (MMD). It highlights some challenges and pitfalls for practitioners and suggests some ways to mitigate them. The reviewers found the paper practically relevant and several reviewers upgraded their scores through the discussion process. The authors acknowledged there are still some remaining issues regarding (i) considering other metrics & descriptor functions; ii) evaluating node/edge attributes and iii) addressing molecule generation. I am satisfied that these areas are beyond the scope of the current work and that the clarification improvements in the paper are adequate. It stands well enough on its own to accept in its present form.",0.8433172106742859
"The paper proposes a novel method for non-classification tasks. The novelty seems solid in terms of novelty, although there might be related work that I’m not aware of. The overall contribution seems solid in terms of novelty, although there might be related work that I’m not aware of. The paper is clear regarding the proposed method, although I found it helpful to also read the PostNet paper. However, the dataset is too dense, presumably due to the space constraints. Overall, the experiments convince me of the main goal of the paper, namely that NatPN provides well-calibrated uncertainty estimates for multiple data types.","This paper presents a method for producing higher quality uncertainty estimates by mapping the predictions from an arbitrary (e. g. deep learning) model to an exponential family distribution. This is achieved by using the model to map from the inputs to a low-dimensional latent space and then using a normalizing flow to map to the parameters of the distribution. The authors show empirically that this improves over a variety of baselines on a number of OOD and uncertainty quantification tasks. This paper received 5 reviews who all agreed that the paper should be accepted (6, 6, 8, 8, 8). The reviewers in general found the method novel compared to existing literature, compelling and the results strong. Multiple reviewers asked for experiments with higher dimensional",0.8311043977737427
"The main idea is quite conceptually simple and an interesting approach to develop altruism for MARL. The experimental section of the paper is well executed, and the analysis of the results is thorough. The analysis of the failure case is also quite insightful. I also like the proposed approach because it seems quite general and doesn't assume access to the leader's reward function, state goal, policy, or trajectories (unlike other methods in this space). The method is new as far as I know, although some of its elements have been used in other contexts, but I think the authors do a good job at explaining the connections to related work","This work proposed a method for encouraging an agent showing altruistic behaviour towards another agent (leader) without having access to the leader's reward function. The basic idea is based on the hypothesis that having the ability to reach many future states (i. e., called choice) is useful for the leader agent, no matter what it reward function is. The altruistic agent learns a policy that maximizes the choice of the leader agent. The paper defines three notions of choice, and evaluates them on four environments. The reviewers believe that this work attempts to solve an important problem, proposes a novel approach, and performs reasonably good experiments. The reviewers are all on the positive side at the end of the discussion phase",0.8459218144416809
"This paper provides a novel alternative for the problem of coordination graph learning. While the paper doesn't mention it, the payoff variance based edge selection method might be an interesting case for resolving some of the future work. The paper has an expansive set of experiments with fair number of agents. While there are some ablations, so it's not a big weakness, but it would be useful to know for example whether the action encoder trick helps saymix or other value factorization methods. I would expect sparsity to help even more in larger number of agent environments and it would be great if MACO allows to quickly evaluate that","All reviewers found that the paper offers interesting contributions for multi-agent RL and favour acceptance of the paper. The strengths of the paper are summarized below: - Good algorithmic contribution - Offers a new set of benchmark tasks for coordination in MARL settings - Exhaustive experiments on complex tasks with a reasonable number of agents - All the issues raised by the reviewers (missing references, missing discussion of limitations...) have been satisfactorliy addressed. I therefore join the reviewers in the recommendation to accept the paper.",0.835715651512146
"This paper studies the approximation properties of RNN encoder-decoders, in the perspective of linear and continuous time. The authors show that the temporal product structure may be the intrinsic structure arising from the encoder-decoder architecture. The impact is somewhat limited by assuming linear and continuous-time for proving the approximation properties of RNN encoder-decoder, which is not the case in reality.","This paper presents a theoretical analysis of the approximation properties of linear recurrent encoder-decoder architectures, obtaining universal approximation results and subsequently showing approximation rates of targets for RNN encoder-decoders. It introduces a notion of ""temporal products,"" which helps to characterize the types of temporal relationships that can be efficiently learned in this setting. Overall, the reviewers and I all agree that this paper makes important theoretical contributions to the important problem of the approximation capabilities of encoder-decoder architectures. The main weaknesses involve the rather simplified linear problem setup, but this limitation is easily forgiven in this first-of-its-kind rigorous analysis. I recommend acceptance.",0.875468373298645
"The paper is well-written and easy to understand. The authors provide rigorous of properties for all Pixelated Butterfly components. In general, the conducted experiments are quite extensive and validate the claims made in the paper. The actual hardware resources used in the main experiments are not described. This affects both the choice of the block size (which might influence the findings) and the availability of fine-grained sparsity patterns, such as 2:4 structured sparsity on NVIDIA Ampere architectures. One possible way to do so would be to study how the final task performance changes for a range of compute budgets and a single base architecture with a","This is an intriguing work that introduces a novel sparse training technique. The core insight is a novel reparametrization or sparsity pattern based on the so-called butterfly matrices that enables fast training and good generalization. The theory is solid and useful. Most importantly, the method is novel and is likely to become impactful. Understanding better what contributes to the excellent performance is an interesting question for future work. In agreement with all the reviewers, it is my pleasure to accept the work.",0.8373446464538574
"This paper proposes blockwise quantization (dynamic) to reduce the states smaller (momentum and second moment) in diagonal first order embeddings. The paper proposes a 8-bit Adam implementation that works as well as its f32 while being memory efficient. The paper shows very impressive empirical results and has an open-sourced which makes it very valuable to the community. The paper includes the f32 variant, but do not include the bfloat16 variant comparison.","This paper proposes Adam and Momentum optimizers, where the optimizer state variables are quantized to 8bit using block dynamics quantization. These modifications significantly improve the memory requirements of training models with many parameters (mainly, NLP models). These are useful contributions which will enable training even larger models than possible today. All reviewers were positive.",0.8545474410057068
"Two representations with the same null-space are not necessarily the same. The authors say that the representation of robust nets is similar to the representations of the models of peripheral vision. However, this is far from being the same as saying that the representations are equivalent or similar. The paper has a number of points that should be clarified by the authors before it can be accepted for publication at ICLR.","This paper shows that images synthesized to match adversarially robust representations are similar to original images to humans when viewed peripherally. This was not true for adversarially non-robust representations. Additionally the adversarially robust representations were similar to the texform model image from a model of human peripheral vision. Reviewers increased their score a lot during the rebuttal period as the authors provided more details on the experiments and agreed to tone down some of the claims (especially the strong claim that the robust representations capture peripheral computation similar to current SOA texture peripheral vision models). As well stated by reviewer s6dV, two representations with the same null-space are not necessarily the same. With reviewer scores of 8",0.8685188889503479
"In the past about ten days, we carefully improved the experiments, the clarifications and the discussions of our work to address the concerns, the questions and the requests by all four reviewers. We will release the whole source code package of our work in the near future, and hope it could advance the research in dynamic convolution. The authors also provide a wide range of ablation studies, which help a better understanding of various aspects necessary when deploying the technique to other applications. The results indicate that the lighter model (ODConv 1x) offers comparable or slightly better accuracies with less compute, while the larger model (ODConv","This paper presents ODConv, a convolution pattern which uses attention in the convolutions across all dimensions of the weight tensor. The paper is well motivated and well explained, easy to follow. This work is built on top of previous work, but reviewers all agree that the contributions of this paper are significant. The experimental section is comprehensive, with several benchmarks, and show clear improvements. The reviewers suggested a few additional remarks, and discussions to add to the paper, which the authors have addressed in the rebuttal. Reviewers seem in general happy with the authors answers to their concerns. This seems like a sound and meaningful paper. I am fully in favour of acceptance, and I recommend this paper to be presented",0.8450838923454285
This paper proposes a method to identify image tokens based on a category influence. The class token adaptively removes image tokens based on the corresponding category influence. The discarded tokens are reorganized/fused into one token to facilitate network training at the initial training stages. The experiments on the efficiency and accuracy evaluations show the proposed method is effective for current Vit models.,"The paper presents an approach to select visual tokens in images and reorganize them for the object classification, within Transformers. All four reviewers find the paper interesting and novel, and they are also very positive about the experimental results. The authors also addressed minor concerns of the reviewers successfully through the discussion phase, clarifying details and adding experiments. We recommend accepting the paper.",0.849503219127655
The paper is well-motivated and easy to follow. The proposed variational approach is sound for by-passing time-derivatives of noisy observations. The proposed framework (D-CODE) shows that D-CODE is the right way to recover ODEs under some undesirable but unavoidable measurement errors.,"This paper introduces a new technique for discovering closed-form functional forms (ordinary differential equations) that explain noisy observed trajectories x(t) where the ""label"" x'(t) = f(x(t), t) is not observed, but without trying to approximate it. The method first tries to approximate a smoother trajector xhat(t), then relies on a variational formulation using a loss function over functionals C_j_j, defined in terms of an orthonormal basis g_1,..., g_S of sampling functions such that the sum of squares of all the C_j approximates the theoretical distance between f(",0.8228383660316467
"This paper presents a novel method to represent the molecular graph as a sequence of decisions according to a novel spanning tree-based grammar. The method is general and can be applied to other graph structures. The experiments are comprehensive and well written. I think this work has a lot of novelty and merit. The idea of using a transformer which can view earlier decisions with attention mechanisms increases the possibility of valid molecular generation. This also forms a conceptual departure from highly sequential or reinforcement learning-based methods. The authors also perform ablation tests which show the improvements with several of their design choices including the use of graph structure, the use of a transformer,","This paper proposes a spanning tree-based graph generation framework for molecular graph generation, which is an interesting problem. The tree-based approach is efficient and relatively effective in molecular graph generation tasks, and the empirical results are convincing. There were some concerns during the initial reviews, but all of them have been addressed during the discussion phase. Thus, I recommend this work be accepted.",0.8671871423721313
The paper proposes a series of principled improvements to state-of-the-art MCTS planning algorithms. The proposed improvements are particularly effective in low simulation budget settings.,"The paper presents improvements to AlphaZero and MuZero for settings where one is restricted in the number of rollouts. The initial response from reviewers was generally favorable but the reviewers wanted more details and clarifications of multiple parts of the paper, and further intuition about the Gumbel distribution. The authors’ responses were detailed and convinced or maintained strong positive support of most reviewers. The authors also stated that they plan to provide a release of the code and also provided a policy improvement proof. Overall this is an interesting approach that is likely to be of significant interest to many.",0.8595541715621948
"The paper builds a convincing case for the proposed method by calling attention to the mismatch of goals in traditional training procedures and conformal prediction. The experiments appear to be quite thorough. The extension to other structure-inducing losses is interesting, although its actual deployment may warrant caution.","In this paper, a new learning scheme for minimizing the confidence set by conformal prediction is proposed. Most of the reviewers agree that the idea is interesting and novel. This is an important contribution to trustworthy ML, with theoretically sound considerations and thorough experimental validation.",0.8733163475990295
"This paper presents a multi-task NLP model that learns to answer well given various domains of questions/tasks. In particular, QA, NLI, sentiment classification, summarization etc., and the zero-shot evaluation tasks, which are of different categories than in the training set. The idea of converting all the tasks to an unified format is not new, and has been explored in earlier works like Decagressive, UFO-Entail, T5 and more recently in the CLUES few-shot benchmark. One of the primary contributions of the work is in aggregating resources across 12 tasks, 54 datasets and a large","This paper studies constructing text2text transformer models that are good at zero-shot task generalization via multi-task learning over a diverse set of NLP tasks. One main contribution of the work is to create prompt templates for various NLP tasks (that are of different task formats) such that all tasks can be framed into text2text learning format and that is ""natural"" to the pretrained T6 model. The paper conducts extensive experiments to demonstrate the promising zero-shot generalization ability of such multi-task learner. Strength: - Important problem setup that has broad applications - Extensive experiments to validate the claims - Useful resources are developed for the problem Weakness: - Good to study the effect of using different combination",0.8494895696640015
"This paper presents an efficient way to compute meta-gradients in discrete-time setting. The paper is well written, well organized and easy to follow. I believe the method proposed is an interesting addition to the large family of meta-learning algorithms that target the (by-now-classical) few-shot learning setting. The main weakness of the proposed approach is that it is quite specific to the setting treated in the paper, mainly few-shot classification with a small number of classes, where the loss is (unregularized) cross-entropy. In my view, the main bottleneck is computation rather than memory","This paper addresses a continuous-time formulation of gradient-based meta-learning (COMLN) where the adaptation is the solution of a differential equations. In general, outer loop optimization requires backpropagating over trajectories involving gradient updates in the inner loop optimization. It is claimed that one of main advantages of COMLN is able to compute the exact meta-gradients in a memory-efficient way, regardless of the length of adaptation trajectory. To this end, the forward-mode differentiation is used, with exploiting the Jacobian matrix decomposition. All the reviewers agree that the derivation of memory-efficient forward-mode differentiation is a significant contribution in the few-shot learning. The paper is well",0.8499348163604736
"This paper studies a problem that lies in the intersection between perception and statistics. The reviewers agree that there is an interesting and important problem that lies in the intersection between perception and statistics. The reviewers also agree that there is a lack of experiments/discussions on how useful these observations are for designing better generative models and/or better perceptual distances. Overall, the paper was nice to read. There are six (6) observations and 3 Eqns in main body of the paper, which formed the core part of the work. The reviewers also agree that there is an interesting and important problem that lies in the intersection of perception and statistics.","All reviewers suggest acceptance of this paper, which reports the relationship between perceptual distances, data distributions, and contemporary unsupervised machine learning methods. I believe this paper will be of broad interest to different communities at ICLR.",0.8475464582443237
"This paper could be improved by addressing the following points. The main result of this paper needs to be formulated much more cleanly. The paper is well-organized and well-presented. The reviewer is a little concerned over the novelty. The paper is not yet ready for acceptance. However, I think that in its current state the paper is not yet ready for acceptance. However, I think that in its current state the paper is not yet ready for acceptance. However, I think that in its current state the paper is not yet ready for acceptance. However, I think that in its current state the paper is not yet ready for acceptance","The paper looks at subspace recovery in the presence of outliers, of which there have been many formulations. They study a recent formulation, DPCP, but relax the requirement that the dimension of the subspace is known -- obviously very important in practice. The approach is quite clever: they exploit the fact that for this non-convex problem, starting a simple algorithm at a randomly chosen starting point will converge to a local minimizer, and they can run an ensemble of these algorithms (each with different starting points) and be guaranteed the solutions will span an appropriate subspace. This idea alone is a nice contribution. The paper has theory and experiments. Most reviewers were positive about the paper. The",0.8078038096427917
The paper improves the upper bound on the number of parameters required to memorize N points from O(N2/3) to O(N1/2) and closes the gap between the lower bound (N1/2) to O(N1/2) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -,This paper studies the memorization power of Relu Neural networks and obtains sharp bounds in terms of parameters. The writing is very clear and the results very interesting.,0.7793238162994385
"This paper presents an effective way to implement the idea of conducting a program search on a differentiable architecture space with policy gradient. The experimental results show that the proposed method outperforms or achieves competitive performance compared to RL baselines such as HIRO, TRPO, PPO.","This paper presents an approach to synthesize programmatic policies, utilizing a continuous relaxation of program semantics and a parameterization of the full program derivation tree, to make it possible to learn both the program parameters and program structures jointly using policy gradient without the need to imitate an oracle. The parameterization of the full program derivation tree that can represent all programs up to a certain depth is interesting and novel. In its current form this won’t scale to large programs that require large tree depth, but is a promising first step in this direction. The learned programmatic policies are more structured and interpretable, and also demonstrated competitive performance against other commonly used RL algorithms. During the reviewing process the authors have actively engaged in the",0.8484256267547607
"The paper studies the effect of SAM on ResNet, ViT and computationally-Mixer training and its interplay with dataset size and augmentation. The main result is that SAM can more or less replace the heavy image augmentations for ViT and-Mixer (Result1) and improve accuracy on clean and corrupted test set. The authors provide a number of claims that are not justified by the evidence. However, I am not convinced of why it works. The main concern about this paper is that the authors provide a number of claims that are not justified by the evidence.","### Description The paper demonstrates that efficient architectures such as transformers and MLP-mixers, which do not utilize translational equivariance in the design, when regularized with SAM (sharpness aware minimization) can achieve same or better performance as convolutional networks, in the vision problems where the convolutional networks were traditionally superior (with data augmentation or not, regularized or not). The paper demonstrates it very thoroughly through many experiments and analysis of the loss surfaces. ### Decision + Discussion I find the paper to be very timely in its context. It has a remarkable coverage of experimental studies and different use cases: SAM + augmentation, +contrastive, +adversarial, +transfer learning; as",0.8382706642150879
"The proposed method is well-motivated, and the paper is clearly written. The authors evaluate the performance on two image classification datasets. It might be more convincing if the authors can evaluate the performance on more kinds of tasks (e. g., regression tasks). The empirical experiments are rigorous and well supports the claims made in the paper.","This paper presents a novel methodology for performing meta learning for gradient-based hyperparameter optimization. The approach overcomes limitations (scaling, e. g.) of previous methods through distilling the gradients of the hyperparameters. The paper received 4 reviews, of which all were positive (6, 6, 8, 8). The reviewers appreciated the technical clarity of the paper and found the proposed approach sensible, novel, technically sophisticated and effective. The main concerns were regarding the comprehensiveness of the experiments and technical presentation of the dataset distillation. It seems that the reviewers found the author response (lots of results were added) satisfactory regarding these points. Thus the recommendation is to accept.",0.8528818488121033
The presented tighter approximation bounds for two-layer ReLU neural networks are of interest. The authors discuss its tightness and links with Fourier approximation bounds in Theorems 3 and 4. The non-uniqueness of neural network representations is studied in Theorem 5.,The authors extend the result of Ongie et al. (2019) and derive sparseneural network approximation bounds that refine previous results. The reuslts are quite ineteresting and relevant to ICLR. All the reviewers were positive about this paper.,0.839471161365509
The paper presents experimental results on a variety of benchmarks with different types of data. All benchmarks are small-scale (which should be sufficient for the purposes of this paper). The results are very good across the board with LEM beating LSTM across the board. This brings me to my main objections to this paper.,The paper proposes a new recurrent architecture based on discretization of ODEs which allow for learning multi-scale representations and help with the vanishing gradient problem. The reviewers all agree this architecture is novel and provide substantial theoretical and empirical evidence. A strong accept.,0.8593565821647644
"The authors have done a good job at it but in my humble opinion literature is still missing some important papers. The term robustness is very loosely used in the paper. In my humble opinion, the term robustness is very loosely used in the paper. The paper is well written and easy to follow. The reviewers should consider making the code for their method and empirical experiments available to the reviewers and later release it publicly. I believe that the authors should consider making the code for their method and empirical experiments available to the reviewers and later release it publicly.","The paper proposed a novel approach that leverages the discrepancies between the (global) series association and the (local) prior association for detecting anomalies in time series. The authors provided detailed empirical support to motivate the above detection criterion, and introduced a two-branch attention architecture for modeling the discrepancies and establishing an anomaly score. All reviewers acknowledge the technical novelty of this work (including the key insight of modeling anomalousness with Transformer’s self-attention and concrete training mechanism via a minimax optimization process) as well as the comprehensiveness of the empirical study. Meanwhile, there were some concerns in the positioning of the work, in particular in the clarity in connection to related work, and some reviews concern the clarity",0.825910747051239
"This paper proposes a new method for training diffusion models, where the model is distilled to as small as 4 or 8 steps while retaining high image quality. The proposed progressive distillation is novel and interesting. Overall, the presentation is clear and easy to understand, though there exist some minor issues with typos and confusing sentences (see comments below). The main claim is that the method can reduce the number of model evaluations in sampling to as small as 4 or 8 steps while retaining high image quality. It would also be interesting to see the effect of choosing a different schedule for the progressive distillation, for example reducing the number of integration steps by a factor","This paper presents a faster sampling method for diffusion based generative models which are usually slow in practice. The key idea is based a progressive distillation approach (e. g., how to distill a 4 step sampler into a 1 step sampler). The paper studies the various design choices for diffusion models which existing work hasn't looked at that deeply and sheds light on the effects of these choices. The paper also shows that DDIM can be seen as a numerical integrator for probability flow ODE. The experimental results are impressive. There were some concerns such as the effect of progressive distillation and the overhead of distilling the diffusion model but the authors provided a satisfactory response and backed it up with additional results. Overall",0.8563598394393921
"The proposed QMC-based algorithm is an extension of random reshuffling to the scenario of training. This is not building a new sampling strategy, but extending a known method to a different setting. The assumption 2, which is the condition on average gradient error, remains a mathematical condition. So far, it is not quite clear how one should build an optimal sampling method, and how one can build a better sampling method than current ones.","This paper studies the dependency of SGD convergence on order of examples. The main observation of the paper is: if the averages of consecutive stochastic gradients converge faster to the full gradient, then the SGD with the corresponding sampling strategy will have a faster convergence rate. For different sampling strategies, sampling with replacement has slower convergence in stochastic gradient, where sampling without replacement can converge faster. The paper also proposes two new algorithms that can improve convergence rates in some interesting settings. The reviewers find the analysis clean and the new algorithms are interesting. There is some concern on the dependency on n or d for the faster rate, which should be discussed more clearly in the final version of the paper. Overall this is a solid contribution",0.8435818552970886
"This paper shows a novel connection between calibration and generalization. The theoretical results are sound. Experiments are through and sound. The paper is well written and easy to follow. The authors are encouraged to provide more intuition in the main body of the paper regarding the key insights in the proofs. The main weakness of the paper is that it doesn't really attempt to present a strong case for the usefulness of the main observation in practice. That being said, the novelty of their empirical result and conceptual explanation are enough to make the paper worthy of acceptance in my opinion. I recommend the acceptance of this paper without any reservation.","This article introduces an interesting variant of the work of Nakkiran & Bansal (2020). It shows empirically that the test error of deep models can be approximated from the disagreement on the unlabelled test data between two different trainings on the same data. The authors then show theoretically that a calibration property can explain such behaviour, and they report experiments showing that the relationship does exist in practical situations. All reviewers agree on the practical and theoretical value of the article, which is very well organised and written. The ideas developed here are likely to lead to further work in the future, and they clearly deserve to be published at ICLR. I agree with one of the reviewers that the title is somewhat misleading",0.8470398783683777
"The paper is well-written and easy to follow. The pictorial demonstration is also very clear. The qualitative results are helpful to understand the exploration behaviors and are intuitive and interesting. I think there are some issues that could be improved to make the paper more clear. The main weaknesses of this paper are that it does not demonstrate strong empirical results, that the related works section is missing some important discussion, and that the method is missing some important details.","This paper proposes an alternative approach to epsilon-greedy exploration by instead generating multi-step plans from an RNN, and then stochastically determining whether to continue with the plan or re-plan. The reviewers agreed that this idea is novel and interesting, that the paper is well-written, and that the evaluations are convincing, showing large improvements over epsilon-greedy exploration and more consistently strong performance than other baselines. While the original reviews contained some questions around discussion of related work and the simplicity of the evaluation environments, the reviewers felt these concerns were adequately addressed by the rebuttal. I agree the paper explores a very interesting idea and convincingly demonstrates its potential, and should be",0.857437789440155
The paper provides the theoretic interpretation of their uncertainty penalization. The proposed method outperforms various offline RL algorithms including model-based algorithms and uncertainty-aware methods.,"This paper makes significant advances in offline reinforcement learning by proposing a new approach of being pessimistic to deal with uncertainties in the offline data. The proposed approach uses bootstrapped Q-functions to quantify the uncertainty, which by itself is not new, and introduces additional data based on the pseudo-target that is penalized by the uncertainty quantification. The use of such additional data is the first of a kind, and the paper provides theoretical support for the case of linear MDP and empirical support with the D4RL benchmark. The reviewers had originally raised concerns or confusions regarding theoretical analysis and experiments. The authors have well responded to them, and no major concerns remain.",0.8604698181152344
"This paper gives solid theoretical formulation and analysis of graph representation learning over bag of subgraphs. The technical contribution of this paper is sound and significant. The paper gives detailed analysis of design choices such as base graph encoders and subgraph selection policies. The proposed method shows (small) improvements over the base encoder in many cases. Hence, it seems rather unlikely that it will be widely adopted in its current form. The paper does not provide details about the size of the models (e. g. in terms of number of trainable parameters). This problematic since it remains unclear if the performance improvement stems from the increases or is simply due","Improving the expressiveness of GNN is an important problem in the current graph learning community. Its key idea is to generate subgraphs from the original graph, then encode the subgraphs into the message passing process of GNN. The proposed method is proven to be strictly more powerful than 1-WL. The authors also quantize how design choices such as the subgraph selection policy and equivariant neural architecture can affect the architecture’s expressive power. After the rebuttal, all reviewers are glad to accept this submission. During the discussion, while reviewer B3oK has shown some concerns on the concurrent works in NeurIPS 2021, it should not affect the decision of the submission once the authors have discussed them in",0.8466086983680725
"The paper focuses on addressing important problem and draws interesting insights. The architecture called AlterNet, shows better performance and robustness on ImageNet and CIFAR-100. The paper proposes a new Alter-ResNet architecture that is both more accurate and robust.","The paper presents an empirical analysis of Vision Transformers - and in particular multi-headed self-attention - and ConvNets, with a focus on optimization-related properties (loss landscape, Hessian eigenvalues). The paper shows that both classes of models have their strengths and weaknesses and proposes a hybrid model that takes the best of both worlds and demonstrates good empirical performance. Reviewers are mostly very positive about the paper. Main pro is that analysis is important and this paper does a thorough job at it and draws some useful insights. There are several smaller issues with the presentation and the details of the content, but the authors did a good job addressing these in their responses. Overall, it's a",0.8520554900169373
"The paper proposes a method to improve the efficiency of sequential SBI methods. The paper proposes several methods to improve the inference quality, e. g., SIR, IWAE,  -divergence, but the reviewer concerns more about the accuracy in likelihood estimation. The novelty of the paper is not very high. The major difference between the proposed method and standard variational inference algorithm (and its variant) is not very high. The paper proposes to replace the unknown likelihood with an estimated likelihood, which is natural but the novelty is not very high. The paper proposes several methods to improve the inference quality, e. g","The authors propose a well-presented approach to likelihood-free inference. The reviewers are all in alignment in recommending this paper for acceptance. There was a healthy discussion between authors and reviewers, where the authors have already incorporated many of their recommendations. The potential for this methodology to be applied to situations with expensive simulators should be intriguing to a broad audience. As a result, I recommend for this paper to be accepted as a spotlight.",0.84601229429245
"This paper proposes a good idea to solve the trilemma in generative learning, which accelerates the sampling in denoising diffusion models by using conditional GANs. The paper motivates the problem through the mental image of a trilemma. The approach is relatively simple and provides good results. Experiments convincingly demonstrate that the approach indeed strikes a good balance of the three desired goals. The paper should address previous work that also attempts to solve the trilemma by reducing the number of steps required for a trained DDPM model. The results on CIFAR-10, CelebA-HQ and LSUN Churches are competitive with previous state-of-the","The paper modifies DPMs by replacing the denoising L2 losses with GANs to learn the iterative denoising process. This leads to excellent results using a small number of refinement steps. In some sense, this also takes away one of the key advantages of DPMs over GANs, which is DPMs minimize a well-defined objective function. Nevertheless, the results are convincing, but not spectacular. I am not convinced that we should continue to report training FID on CIFAR-10. I would have like to see class-conditional ImageNet results. Also, it is not clear whether the proposed technique provides additional gains on top of SoTA GANs. Overall,",0.8522281646728516
"This paper studies a very important and interesting topic, continuous depth neural networks. It may be one way to reach the framework of Explainable AI. I appreciate the authors providing a very detailed derivation and implementation for the InImNet architecture. In particular, I would like to see the comparison of performance for different models under the same training time cost.","The authors presents an alternative view of Neural ODEs, offering a novel understanding of depth in neural networks. The reviewers were overall impressed by the novelty and potential for insights this work brings. There was some disappointment that the empirical results were not stronger (both in terms of pure performance and computational cost) and that it wasn't clear how the theoretical insights into depth actually translated into a practical insight. Nevertheless, I agree with the reviewers that this is a good submission and would I think make for an interesting addition to the conference programme.",0.8546003103256226
"This is a very well written and very insightful work. The writing is flawless, the structure is very clear and the presentation of ideas unfolds naturally. The contributions are significant, their evaluation is convincing, both in the method and in the results, and the discussion has an appreciable depth. First, some evaluation aspects are omitted, and I question their effect on the mechanism of InFeR. Second, some parts of the discussion could maybe be condensed to develop other aspects more in-depth, both in the main text or as references to the appendix. The following remarks list the aspects upon which I would like to see more details and/","The paper analyzes the learning behavior of deep networks inside RL algorithms, and proposes an interesting hypothesis: that many of the observed difficulties in deep RL methods stem from _capacity loss_ of the trained network (that is, the network loses the ability to adapt quickly to fit new functions). As the paper points out, some of these difficulties have popularly been attributed to other causes (such as difficulties in exploration) or to less-specific causes (such as reward sparsity: the paper proposes that capacity loss mediates observed problems due to sparsity). The paper investigates its hypothesis two ways: first by attempting to measure how capacity varies over time during training of existing deep RL methods, and second",0.8230463862419128
"This paper addresses the interesting novel problem of measurement shifts, a subset of general domain shifts that occur due to changes in the measurement systems. To approximate the source feature distributions, this work uses a softly-binned histogram and provides a novel differentiable implementation that is used in a loss function to adapt the model to the target domain. The improvements of BUFR are demonstrated against prior SFDA works on the proposed EMNIST-DA dataset.","The paper aims to solve the source-free domain adaptation, specifically on measurement shift. The proposed method lowers the domain gap via restoring the source feature distribution with a lightweight approximation. The effectiveness and performance are well validated by extensive experiments on various datasets compared with other recent methods, and the ablation analysis supports the claim of the paper well. The paper is well written with clear logic to follow.",0.8633178472518921
"This paper proposes a method to improve LMs' ability to make cross-example connections by constructing examples out of interleaved texts. In general, dependencies within a text are more important than dependencies between random semantically related sentences from different texts. The authors propose to include related texts retrieved by the kNN method in a single training sample, which is proved effective in solving sentence similarity tasks. The empirical part of the paper shows improved performance of adding similar sentences to the context of LM training. The authors should conduct experiments on more types of sentence pair tasks. I will be more convinced if evaluation is done on a wider range of tasks.","This paper presents a novel framing of what's at stake when selecting/segmenting text for use in language model pretraining. Four reviewers with experience working with these models agreed that the conceptual and theoretical work here is insightful and worth sharing. The empirical work is fairly small-scale and does not yet support broad conclusions, but reviewers did not see such conclusions as necessary for the paper to be valuable.",0.8397473692893982
"This paper proposes a method to scale up emergent communication experiments. The main paper considers |C| = 16, 64, 256, 1024 (the appendix considers also |C| = 4096) and N = 1, 10, 50. It is hard to justify conclusions such as ""increased task complexity systematically improves generalization"" and ""there is no consistent pattern between TopSim and |C| with a non-significant Spearman correlation"" with 4 datapoints. It is not clear that starting with complex tasks such as pointing and pantomiming is the best model of human language.",This manuscript expands the range of recent work in reinforcement learning for language games to much larger and more realistic datasets. A timely and relevant contribution and one that is well evaluated. Further work in stabilizing RL approaches for such large-scale problems is likely to have other far-reaching consequences. Reviewers were unanimous that this is a strong submission after the author discussion period.,0.8273253440856934
"The paper addresses the problem of few-shot multi-granularity adaptation. The main focus of the proposed approach is learning embeddings, but the evaluations are focused on classification. The paper includes comprehensive evaluation across various datasets and demonstrate the effectiveness and stability of the proposed approach. The authors should make their procedure for computing confidence intervals and determining superiority/non-inferiority clear, and should ideally provide appropriate statistical tests/analysis for these claims. The paper does not provide any such analysis in its current state. It also appears (at least as far as this reviewer understands it) that the current formulation imposes a constraint that subclass","This work presents an approach to learning good representations for few-shot learning when supervision is provided at the super-class level and is otherwise missing at the sub-class level. After some discussion with the authors, all reviewers are supportive of this work being accepted. Two reviewers were even supportive of this work being presented at least as a spotlight. The approach presented is well motivated, experiments demonstrate its value and include a nice application in the medical domain, making the work stand out relatively to most work in few-shot classification. Therefore, I'm happy to recommend this work be accepted and receive a spotlight presentation.",0.8436523079872131
"This paper studies the computational bottleneck in hypergradient computation. The proposed method borrows ideas from L-BFGS and Broyden’s method, which appears to be natural choices to consider. The authors establish various convergence results showing that the approximate hypergradients converge to the true hypergradients. The proposed method is interesting and can be viewed as a complementary method to the recent Jacobian-Free method. The theoretical part of the paper is written very well for someone unfamiliar with the literature. Some of the experimental results are not convincing. For the proposed method to make practical sense, the author should show the wide","The paper considers the setting of bi-level optimization and proposes a quasi-Newton scheme to reduce the cost of Jacobian inversion, which is the main bottleneck of bi-level optimization methods. The paper proves that the proposed scheme correctly estimates the true implicit gradient. The theoretical results are supported by numerical experiments, which are encouraging and show that the proposed method is either competitive with or outperforms the Jacobian Free method recently proposed in the literature. Even though the reviews expressed some initial concerns regarding the empirical performance of the proposed method, the authors adequately addressed those concerns and provided additional experiments. Thus, a consensus was reached that the paper should be accepted.",0.8744451403617859
"This paper studies deployment efficient RL from the theoretical perspective. It shows when MDP is linear, the lower bound of deployment complexity for deterministic policy and arbitrary policy is (dH) and (H) respectively.","The authors’ present a precise definition of deployment efficient RL, where each new update of the policy may be costly, and theoretically analyze this for finite-horizon linear MDPs. The authors include an information-theoretic lower bound for the number of deployments required. The reviewers found this an important setting of interest and appreciated the theoretical contributions. The authors’ carefully addressed the raised points and also addressed questions about deployment complexity and sample complexity in their revised work. One weakness of the paper is that it does not provide empirical results and the linear MDP assumption, while quite popular in theoretical RL over the last few years, is quite restrictive. However,the paper still provides a very interesting theoretical contribution for",0.8539155125617981
"The paper is well-written and comprehensive. Although the paper employed many notations, they are intuitive and easy to follow. Overall I think this is a good paper. In ICML 2021 -- After rebuttal -- I'm ok with the answers provided by the authors, I keep my score.","This paper provides generalization bounds for meta-learning based on a notion of task-relatedness. The result is natural and interesting--intuitively, when tasks are similar, then meta-learning algorithms should be able to utilize all data points across all tasks. The theoretical contribution is novel, and the results also provide more practical insight into the performances of some models.",0.8395311236381531
"This paper presents a new compression algorithm for distributed machine learning. The algorithm is similar to QSGD, but uses a global scaling factor that is known to all parties. This adds additional benefits that the compressed results on each worker are addable. The authors find a clever scaling factor that makes the convergence proof work. In the experiments, they show that their method works best on one task, but not as well as well as PowerSGD on another task. The authors should explain this in the main text.",This paper proposes a theoretically sound and practically effective method to compress quantized gradients and reduce communication in distributed optimization. The method is interesting and worth publication.,0.8607165813446045
"This paper addresses an interesting and important problem. The proposed PIB is novel. The experiments show that the proposed tildeI(w;S) correlates with the generalization gap, and helps improving the performance. The experiments are well done, thorough, and support the main claims of the paper. The main weaknesses of this paper are language and clarity. The graphs should report means and standard error intervals over multiple random seeds.","This paper revisits the information bottleneck principle, but in terms of the compression inherent in the weights of a neural network, rather than the representation. This gives the resulting IB principle a PAC-Bayes flavor. The key contribution is a generalization bound based on optimizing the objective dictated by this principle, which is then tractably approximated and experimentally verified. Reviews raise concerns about assumptions made to achieve the tractable version and a public discussion debates whether this is truly a PAC-Bayes bound. The authors address these adequately. Another concern is whether improvements in experiments can be ascribed to the new objective. Authors add new experiments in support of this. Additional concerns about the clarity of certain aspects of the paper",0.8447404503822327
"This paper presents a new method to improve the performance of Byzantine-robust distributed learning over non-iid data. The authors propose resampling to reduce the heterogeneity across the workers. In the previous version of this paper, the authors propose resampling to reduce the heterogeneity across the workers. The authors claim that “none of these (non-iid Byzantine-robust) methods are applicable to the standard federated learning. The authors do not compare the proposed methods with any other non-iid Byzantine-robust methods. Related to the above comment, the authors do not compare the proposed methods with any other non-ii","This manuscript proposes and analyses a bucketing method for Byzantine-robustness in non-iid federated learning. The manuscript shows how existing Byzantine-robust methods suffer vulnerabilities when the devices are non-iid, and describe a simple coordinated attack that defeats many existing defenses. In response, the primary algorithmic contribution is a bucketing approach that aggregates subgroups of devices before robust aggregation. This approach is also easily composed with existing Byzantine-robust methods. The manuscript includes an analysis of the performance of the proposed approach, including an information-theoretic lower bound for certain settings. During the review, the main concerns are related to the clarity of the technical contributions, and unclear technical statements. The",0.862765908241272
"The authors present an excellent analysis of how attention maps are distributed across layers, and find interesting patterns. The authors are able to reduce the inference by a factor of 1.96 for a 30 second audio, with marginal improvements in WER.","The paper conducted a thorough experimental analysis of the attention map in the Conformer models for CTC based speech recognition models and connected it with phonetic and linguistic information in the speech. Using these insights, the paper presented some computation improvement and marginal quality gains. The authors actively conducted additional experiments to further justify the claims. The paper is strong in terms of the systematic way of in-depth analysis and further development (i. e. sharing the attention map across layers for speedup). But as pointed out by the reviewers, it lacks some comparisons with other alternatives to justify the importance of sharing attention maps in reducing computations. Also it would be better if there's justifications on how the observations generalize to other",0.8580668568611145
The paper is well written with a lot of information useful for the reproduction of the experiments. The theory of the binary-encoded classification label is well described with clarifying examples. The authors introduced an estimation of the error probability for BEL-J and BEL-U and an interesting relation between the two methods (in terms of the percentage difference between errors). The paper is generally well written but I suggest to revise the organization because some parts are introduced after they are effectively used and this can make confusion. The paper is generally well written but I suggest to revise the organization because some parts are introduced after they are effectively used and this can make confusion,"The paper focused on deep regression problems and proposed a label encoding technique which can be thought as a sibling of the famous error-correcting output codes but designed for regression problems. The main idea is well illustrated in Figure 1 at the top of page 3, where the encoder and decoder are the main objects of the proposal (and a quantizer is also needed for using the encoder/decoder which is a uniform quantizer in the paper). The idea/proposal is supported by solid theoretical arguments and convincing empirical evidences (not only the paper but also the rebuttal). While there were some concerns in the beginning, the authors have successfully clarified all the concerns and then the average score has been increased",0.8343280553817749
"This paper presents a novel transformer model for atomic graphs. The paper includes detailed analysis of the learned attention weights. The presented model has good runtime performance compared to models like Dimenet++ that require higher order interactions, while achieving good performance. The paper is well structured and the proposed method is described clearly. The use of the modified attention mechanism could be better motivated. The analysis of attention scores is interesting, however, it does not become clear from the paper how to interpret the results, e. g. in Figs 3 and 4. In the current form, the shown results are not too useful to ""shed light into the black","The paper proposes a rotationally equivariant transformer architecture for predicting molecular properties. The proposed architecture demonstrates good computational efficiency and good results on three benchmarks. All four reviewers recommend acceptance (two weak, two strong), citing the novelty of the architecture, the good computational efficiency of the model and the good empirical results as the main strengths of the paper. The reviewers expressed minor criticisms and recommendations for improvement, some of which were addressed by the authors during the reviewing process, which led to an increase in scores. Overall, this is a nice contribution of machine learning to science, and I'm happy to recommend acceptance to ICLR.",0.8513448238372803
"This paper proposes a neural network training method with a large step-size. The reviewers found the paper to be interesting and suitable for ICLR. The reviewers found the authors' claims to be non-mathematical and not appropriate for ICLR. The authors' theory does not cover this important part of the literature, so their claims are - at best - incomplete in this regard. The authors' theory does not ""run counter to the established wisdom of the field"", the reviewers found the paper to be interesting and suitable for ICLR. The reviewers found the authors' claims to be non-mathematical and not appropriate","Overall, the paper provides interesting counter examples for the SGD with constant step-size (that relies on a relative noise model that diminishes at the critical points), which provide critical (counter) insights into what we consider as good convergence metrics, such as expected norm of the gradient. The initial submission took a controversial position between the mathematical statements and the presentation of the statements on the behavior of the SGD method in non-convex optimization problems. While the mathematical is sufficient for acceptance at ICLR, the presentation was inferring conclusions that could have been misread by the community. I am really happy to state that the review as well as the rebuttal processes helped improved the presentation of the results that I am excited to",0.8318657279014587
"The paper adds a new dimension to the federated learning setup by combining local and decentralized SGD approaches. The theoretical results are intuitive and the convergence analysis explores and highlights the important tradeoffs between intra-cluster connectivity, device sampling rate and convergence rate. The paper proposes to use a hybrid model aggregation to conduct federated learning leveraging both high-speed D2D network and low-speed D2S network. The proposed algorithm takes good advantage of the fast local connectivity among the nodes to improve the convergence. The main concern I have for the paper is that the proposed idea lacks novelty.","The paper contributes to the literature on federated learning by introducing a hybrid local SGD (HL-SGD) method. HL-SGD is motivated by the setups where edge devices are grouped into clusters with fast connections within the cluster, but slower connection between the devices and the server. HL-SGD uses hybrid updates: decentralized updates within the clusters and federated averaging steps between the clusters and the server. Initially, the reviews expressed concerns regarding comparison to prior work, empirical results, and privacy of the proposed scheme. However, the authors adequately addressed all of the concerns, added relevant discussions and results to the paper, and a consensus was reached that the paper should be accepted.",0.869684636592865
"This paper proposes a novel defense focusing on delaying the model stealing process. The paper is clear, well organized and pleasant to read. The experimental section is extensive, and the experimental protocol seems appropriate. The paper is clear, well organized and pleasant to read. A detailed reasoning or comparison to other options should improve the paper.",the paper proposed a novel idea of requiring users to complete a proof-of-work before they can read the model's prediction to prevent model extraction attacks. Reviewers were excited about the paper and ideas. Some misunderstanding raised by reviewers were sufficiently clarified by the authors in the rebuttal.,0.8612660765647888
"This paper proposes a new graph neural net (GNN) method to learn the dynamics of a system from data. The authors propose to do that using the finite element method (FEM). The proposed method builds on using: basis function approximation for the (unknown) time field u, Galerkin method with the assumption that discrepancy between the dynamics F and basis function is an approximation to (finite) basis functions, method of lines, and message passing GNN as a proxy for the dynamics. The authors also propose a method to incorporate inductive bias into model learning for models that are assumed to contain a convection component.","This paper introduces a graph neural network (GNN) based on the finite element method (FEM) for learning partial differential equations from data. The proposed finite element network is based on a piecewise linear function approximation and a message passing GNN for dynamics' prediction. The authors also propose a method to incorporate inductive bias when learning the dynamical model, e. g. including a convection component. The paper received three clear accept and one weak accept recommendations. The reviewers discussed the possible extensions of the method, and also raise several concerns regarding experiments, e. g. the added value of a synthetic dataset, implementation tricks or hyper-parameter settings. The rebuttal did a good job in answering reviewers'",0.8753842711448669
The paper proposes a new method for the training of 3D shape representations based on sparse voxel embeddings and a Distance Field method from Chibane et al. The paper proposes a new method for the training of 3D shape representations based on sparse voxel embeddings and a Distance Field method from Chibane et al. The authors do not make it clear how the other methods' performance was assessed - was it from the reported number or retrained with their preprocessing. The authors do not provide a model without mode seeking. It would be interesting to see how the authors managed to train GCA on such large scenes.,"This paper introduced a probabilistic extension to a pipeline for 3D scene geometry reconstruction from large-scale point clouds. All reviewers recognized the significance of the proposed approach and praised the simplicity of deriving a probabilistic version of Generative Cellular Automata that performs well in a number of reconstruction benchmarks. Authors were responsive during rebuttal and managed to clarify the concerns raised about the limited scope of the experiments and certain parameters involved, and also raise one reviewer's scores.",0.8444195985794067
This paper proposes a novel framework to train a face recognition network for the federated learning setting. The proposed approach has been compared only against a centrally trained model and the proposed model without differential privacy. The experimental validation is weak. The details of these setting and other experimental results and discussion are totally missing from the paper.,"This paper received 4 quality reviews. The rebuttal and discussions were effective. All reviewers raised their ratings after the rebuttal. It finally received 3 ratings of 8, and 1 rating of 5. The AC concurs with the contributions made by this work and recommend acceptance.",0.8337554335594177
"The paper addresses an important problem in theoretical neuroscience: while trained deep networks appear to be good models of the visual stream, the amount of training they need to get there is incompatible with biology. In particular, they show that very little supervision is required for strong matches to neural responses. The paper presents a novel set of results for an important problem, and supports its claims with a significant number of experiments. Overall, the paper presents a novel set of results for an important problem, and supports its claims with a significant number of experiments.","This paper experiments with what is required for a deep neural network to be similar to the visual activity in the ventral stream (as judged by the brainscore benchmark). The authors have several interesting contributions, such as showing that a small number of supervised updates are required to predict most of the variance in the brain activity, or that models with randomized synaptic weights can also predict a significant portion of the variance. These different points serve to better connect deep learning to important questions in neuroscience and the presence of the paper at ICLR would create good questions. The discussion between authors and reviewers resulted in a unanimous vote for acceptance, and the authors already made clarifications to the paper. I recommend acceptance.",0.8601267337799072
"This paper proposes a new method of comparing reward function without training policies. The authors take a previous method (EPIC) and improve it by making it consider only feasible state transitions. The proposed idea is techincally sound and well presented. The empirical results could be more comprehensive. The paper motivates the problem well by clearly identifying the issue with the prior work (EPIC) and proposes a reasonable solution. The experiments are carefully designed to verify the hypotheses of the paper. The numerical results show that DARD is more powerful compared to EPIC. In particular, the experiments demonstrate that samples ""out of distribution"" would harm the performance of","The paper proposes a new pseudometric, DARD, for comparing reward functions that avoid policy optimization. DARD builds on a recent work by Gleave et al. 2020 where the pseudometric EPIC was proposed. In contrast to EPIC, DARD operates on an approximate transition model and evaluates reward functions only on transitions close to their training distribution. Empirical experiments in different domains demonstrate the effectiveness of the proposed pseudometric. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers' questions and increased their overall assessment of the paper. At the end of the discussion phase, there was",0.8632595539093018
"The discussion on previous stack RNNs was a strong segment of the paper. The modifications to the NS-RNN are well motivated by appealing to toy problems or an explanation of what happens numerically during training. The discussion from formal languages to natural languages was a good motivation for introducing the memory-limited technique. It would have been good to explain what Hardest CFL (Greibach, 1973) is in the paper.","This paper advances the long running thread of sequence modelling research focussed on differentiable instantiations of stack based models. In particular it builds upon recent work on the Nondeterministic Stack RNN (NS-RNN) by introducing three extensions. The first is to relax the need for a normalised distribution over the state and action distribution and allow unnormalised weights, this mostly serves to facilitate gradient flow and thus easier training. The second extension allows the RNN to condition on the top stack state as well as the symbol, improving expressiveness. The third improvement introduces a method for limiting the memory required to run the proposed model on long sequences, thus allowing its application to practical language modelling tasks",0.8371678590774536
This paper introduces an infinitesimal generator for a Markov process that has an equilibrium density p. The authors construct the generators via the mirror maps associated with the constraints. The authors show that their algorithm mixes quicker in simulated experiments. The authors try to give a lot of information about gradient flows that are often repetitive and not particularly friendly to readers. The paper is overall well-written and makes reasonable attempts to provide sufficient background to understand the proposed algorithms. The literature review is thorough. The paper is very complete and introduces all the different notions needed to understand the theory of the proposed methods. The paper is very complete and introduces all the different,The paper proposes to extend mirror descent to sampling with stein operator when the density is defined on a constrained domain and non euclidean geometry. All reviewers agreed on the novelty and the merits of the paper. Accept,0.8349719643592834
"This paper proposes an algorithm that learns to be fully deterministic on Go-Zero. All reviewers agree that this is an interesting contribution to the field of generative learning, and the paper is well-written and well-executed. The main concerns are mostly with the way results are evaluated and reported. There are two references that were not mentioned as part of the work and empirical results. This is unacceptable for experiments with random outcomes, and it considerably weakens support for the paper's main claim.","The paper extends MuZero to stochastic (but observable) MDPs. To represent stochastic dynamics, it splits transitions into two parts: a deterministic transition to an afterstate (incorporating all observations and actions up to the current time), followed by a stochastic outcome (accounting for new randomness that follows the last action). The transition to an afterstate is similar in spirit to ordinary MuZero's dynamics model; the stochastic outcome is learned by a VQ-VAE. At planning time, MuZero retains the MCTS lookahead from ordinary MuZero. Stochastic MuZero achieves impressive results: e. g., it maintains the original MuZero's strong performance on the deterministic",0.8233847618103027
"The paper proposes a method to handle conflicting gradients in Multi-task Learning (MTL) and demonstrates proof-of-concept on synthetic optimization problems, as well as good results on real datasets (CIFAR10, CelebA, NYUv2). Overall, the proposed RotoGrad approach seems quite promising and could become a standard tool for multi-task neural network training. It would be interesting to see results on larger-scale problems (both larger networks and larger datasets), as well as examine how RotoGrad might be applicable in transfer-learning and meta-learning scenarios.","The paper addresses the problem of inconsistent gradients in multi-task learning, proposing ways to handle both their magnitude nd direction. Gradient directions are aligned by introducing a rotation layer between the shared backbone and task-specific branches. Reviewers appreciated the technical approach, higlighting the novelty of the rotation layers in this context. The empirical evaluations are systematic fair and insightful, and the presentation is polished. Reviewers unanimously supported accepting the paper.",0.8516366481781006
"The paper proposes two novel strategies about how to enhance the transferability of vision transformers (ViT) models, i. e. e. refined tokens and self-ensemble. The experiments conducted on ImageNet/COCO dataset, and PASCAL demonstrate that the proposed method can outperform the baselines on some experimental settings. The paper also provides a detailed analysis of the model and experimental results.","In this paper, the authors enhance the adversarial transferability of vision transformers by introducing two novel strategies specific to the architecture of ViT models: Self-Ensemble and Token Refinement method. Comprehensive experiments on various models (including CNN's and ViT's variants) and tasks (classification, detection, and segmentation) successfully verify the effectiveness of the proposed method. In general, the problem studied is relevant and important. The paper is well-written and well-motivated with empirical findings. The proposed two strategies are novel, simple to implement, and effective in practice. Following the author's response and discussion, the average score increases from 6 to 7.5, with most concerns well addressed",0.8724930286407471
"The paper is well-written, the results are important and convincing, and the related work is covered well. The proposed method is novel. As far as I know, the proposed method is novel. The most critical weakness of this paper is that it doesn't specify how many samples are used to compute the synthetic accuracy. It is not clear to the readers how to compute the synthetic accuracy and how many samples are enough. The authors do not provide any statistical analysis of the proposed method. For example, each experiment is only conducted once instead of providing the mean and standard deviation of multiple runs. It is not clear how the performance","The paper demonstrates that test error of image classification models can be accurately estimated using samples generated by a GAN. Surprisingly, this relatively simple proposed method outperforms existing approaches including ones from recent competitions. All reviewers agree this is a very interesting finding, even though theoretical analysis is lacking. Given the importance of the problem of predicting generalization, I recommend acceptance.",0.8440008163452148
This paper builds the connection between local attention and dynamic depth-wise convolution. The proposed inhomogeneous dynamic depth-wise convolution variant is computationally (slightly) better than local attention module.,"All three reviewers recommend acceptance. The paper introduces an interesting study and insights on the connection between local attention and dynamic depth-wise convolution, in terms of sparse connectivity, weight sharing, and dynamic weight. The reviews included questions such as the novelty over [Cordonnier et al 2020] and the connection to Multi-scale vision longformer, which were adequately addressed by the authors. The findings in this paper should be interesting to the ICLR community.",0.8682191371917725
"The paper proposes a slightly different approach than previous works, finding an analytical solution to the noise shape of SGD in simple scenario. This approach allows to highlight the importance of some factor, such as the loss level, that were neglected in previous works. The analysis also provides some insight on more applicative issue. It highlights why the linear scaling of the learning rate might not work for small batches or large learning rate. It also provides justification for the use of negative weight decay or insight about stability in second order approaches. The paper is well written, and the main technical results are well presented. The results are novel and close a few","All the reviewers think that the work is significant and new. Therefore, they support the paper to be published at ICLR 2022. Given the strong results and the “accept” consensus from the reviewers, I accept the paper as “spotlight”. The authors should implement all the reviewers’ suggestions into the final version.",0.8367961645126343
The paper is well motivated. The shortcomings of existing unsupervised skill discovery algorithms are clearly explained. The derivation of the proposed objective is intuitively appealing and theoretically sound. The final form of the reward bonus is a simple one (a desirable trait) -- entropy of the mean discriminator minus the mean entropy of the ensemble of discriminators.,"This paper tackles the problem of exploration using intrinsic rewards in RL in states that have never been encountered before. The authors derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement, which estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples. The intrinsic reward resulting from the so-called DISDAIN (discriminator disagreement intrinsic reward) exploration bonus is more tailored to the true objective compared to pseudocount-based methods. Reviewers agree that the paper is well-motivated and well-written, that the proposed DISDAIN exploration method is simple and practical, and that experiments are convincing. Experiments on continuous",0.8484954833984375
"This paper proposes a method to account for the other agents' influences on the observations of a focal agent. The method is evaluated against some very sensible benchmarks, including a PSR method that omits the information sharing between agents part, and a direct independent actor critic that doesn't incorporate PSRs at all. The performance of the MAPSR is analized both theoretically and empirically. The quality of the learned policies are also verified by experiments. It would be better to see some extensions or future thoughts on the unknown interaction structures which are not always trivial and need to be learned. The paper would benefit from a quick grammar check. The","This paper presents an extension of the Predictive State Representation (PSRs) to multi-agent systems, with a dynamic interaction graph represents each agent’s predictive state based on its “neighborhood” agents. Three types of agent networks are considered: static complete graphs (all agents affect all others experience); static non-complete graphs (only some agents affect one another); and dynamic non-complete graphs (agents affect one another in a time varying way). A number of theoretical results are presented, including PAC bounds for the approximations in the framework. The paper also contains a number of experiments that clearly show the advantages of the proposed technique over some related methods. The reviewers unanimously agree that this",0.8454136252403259
"This paper proposes a novel support (pseudo-)divergence (not a true divergence because D = 0 means the supports match but possibly the distributions do not) to measure the difference in supports between distributions. The main concern is that one solution to support alignment is distribution alignment. The algorithm (as shown by mini-batch part) seems naturally collapse to distribution alignment (as evidenced by the mini-batch issue). This would also be much simpler and more grounded than the history idea. The paper should make a clearer distinction between ""continuous"" support alignment and discrete support alignment. The idea of ""constraints"" being","Thanks for your submission to ICLR. Three of the four reviewers are ultimately (particularly after discussion) very enthusiastic about the paper, and feel that their concerns have been adequately addressed. The fourth reviewer has not updated his/her score but has indicated that their concerns were at least somewhat addressed. I took a look at their review and agree that the authors have addressed these concerns sufficiently. I am happy to recommend this paper for acceptance at this point. Note that I really appreciate the time and effort that the authors went into adding additional results and clarifications for the reviewers.",0.816625714302063
The paper presents a novel way of combining information from text and a KB in a bidirectional way. The results presented in the paper show strong gains against baseline methods on 3 different datasets.,"The paper presents a novel method of fusion of information from two modalities: text (context and question) and a Knowledge Base, for the task of question answering. The proposed method looks quite simple and clear, while the results show strong gains against baseline methods on 3 different datasets. Ablation studies show that the model achieves good performance on more complex questions. While the reviewers raise some concerns, e. g., on the sensitivity of the proposed method, the technical novelty against prior works, they see values in this paper in general. And the authors did a good job in their rebuttal. After several rounds of interactions, some reviewers were convinced to raise their scores by a little bit. As a result",0.8871796131134033
"The proposed method is based on the idea of extracting the features of the target data as meta-features using optimal transport. The proposed method's speed is acceptable, and the meta-features provide practical hints for using ML algorithms. The proposed method's effectiveness has been shown empirically, mainly through experiments, but not sufficiently theoretically. Therefore, the limitations of the proposed method are not clear.","This paper addresses an important issue of AutoML systems, specifically their ability to ""cold start"" on a new problem. Some of the reviewers initially had concerns about the experimental validation and the theoretical foundations of the method, but during the discussion phase the authors addressed concerns extremely well. The authors already included most of the feedback of reviewers, further strengthening the paper.",0.8618373870849609
"The proposed method uses the transformer to model both social and temporal information in both the encoder and decoder. The ability to generate scene-consistent predictions are verified with both qualitative and quantitative results on TrajNet. The experiments are extensive, where the proposed method has been evaluated on three motion prediction datasets and one stroke completion dataset. The novelty is a bit weak considering other works have similar transformer-based motion prediction methods, especially [A1] as mentioned in section 5. The ability to generate scene-consistent predictions are verified with both qualitative and quantitative results on TrajNet. The experiments on TrajNet and Omniglot are not very",This paper studies the problem of motion prediction for multiple agents in a scene using transformer-based VAE like architecture. The paper received mixed reviews initially which generally tended towards borderline acceptance. All reviews appreciated extensive experiments but had some clarifications and requests for ablations. The authors provided a strong rebuttal that addressed many of the reviewers' concerns. The paper was discussed and all the reviewers updated their reviews in the post-rebuttal phase. Reviewers unanimously agree that the paper should be accepted. AC agrees with the reviewers and suggests strong acceptance. The authors are urged to incorporate reviewers' comments in the camera-ready.,0.8263819217681885
The paper presents sound (I only checked the proof of Thm. 1 in detail and could not find any major flaws) theory proving identifiability of both shared and private components for the assumed multi-view setting. A finite sample analysis and result is provided which is rare in this area and thus appreciated. The experimental evaluation is relatively diverse (multiple datasets and baselines),"All reviewers agreed that this is a strong paper, that the methodological contributions are both relevant and significant, and that the experimental validation is convincing. I fully share this viewpoint!",0.8481338024139404
"This paper presents an interesting and useful question of introspecting and analyzing different inductive biases. The experiments and analysis are well written and thorough, and the break down of how each type of inductive bias might help or hurt is interesting and easy to follow. The experiments related to the MuJoCo benchmarks help ground the need for this type of analysis, and open the door to some interesting future work.","This paper examined physics-inspired inductive biases in neural networks, in particular Hamiltonian and Lagrangian dynamics. The work separated the benefits arising from incorporating energy conservation, the symplectic bias, the coordinate systems, and second-order dynamics. Through a set of experiments, the paper showed the most important factor for improved performance in the test domains was the second-order dynamics, and not the more common explanation of energy conservation or the other factors. The increased generality of this approach was demonstrated with better predictions on Mujoco tasks that did not conserve energy. All reviewers liked the insights provided by the paper. They agreed that the paper clearly laid out several hypotheses and systematically tested them. The reviewers",0.848076343536377
"The paper proposes novel method to connect memory with transformers for language modeling. The idea seems somewhat novel and evaluated empirically. There is no ablation study in terms of modeling. The paper writing ignores many details that are helpful for reproducing the results. The paper proposes a non-differentiable cache mechanism to reuse keys and values coming from prior training steps. The results on PG-19 doesn't seem to replicate previous work (original paper shows 33.6 for Compressive transformer, whereas this work shows 19 for plain transformers). There are some benchmarks that require models to be able to handle long inputs, such as document-level QA datasets","This paper studies the problem of dealing with long contexts within a Transformer architecture. The key contribution is a kNN memory module that works in concert with a Transformer by integrating upper layers with additional retrieved context. The idea is simple but the execution is good. While the idea is reminiscent of other recent work on this topic, and novelty is somewhat borderline, it is practically useful. Overall, though ambivalent, my recommendation is that the paper should probably be accepted",0.8449854850769043
"The paper is well-written and motivated. The numerical part is thorough. It is unclear to me if such a predicator(s) exist in practice. I appreciate that, at least, based on the way things are presented the current framework seems to be more ""practical"" than the SSAC framework.","One might assume that the k-means problem has already been beaten to death, but this paper shows there are still remaining questions. And rather interesting ones at that, with a novel angle of having additional help from a prediction algorithm of cluster memberships. This connects to learning-augmented algorithms research. The reviewers agreed that the problem is interesting and gives a novel angle, and the interestingness stems from novelty, and the ability to ""escape"" from NP-hardness. The reviewers and authors had nice discussions about details and conclusions, on how limiting is it that the authors focus on reasonably accurate predictors, for instance, and where could the predictors come from. This is a good paper, and",0.8390680551528931
"This paper investigates the computational barriers of calculating the partition function and model selection, both of which are fundamental machine learning tasks. The paper is generally well written with a good computation. One glaring exception (for me) is the 2nd paragraph of 2.4 where the reduction from neural sequence to formal languages is presented. This is crucial for understanding the paper, yet it is done quickly and I found it hard to follow. This (and Theorem 1) are key, after all, theorem 2 (for instance) is quite straightforward once the setup is established. It would be nice to connect the'recommendations' of Section 7 to actual EBM","This is a deep theoretical paper with results that I consider very interesting. I have *not* had time to check them myself, but I have background in these theoretical matters and the results seem reasonable to me - the hardness of even checking the quality of a solution is well known for partition functions (as well as hardness of any reasonable approximation), but the undecidability seems new - I assume it comes naturally and it is a very interesting result - I have seen similar decidability issues for #P: general probabilistic polynomial time Turing machines (it is unclear if a connection was sought here). Reviewers are all positive about the content, and authors have acknowledge some points for improvement.",0.8319964408874512
"This paper introduces a generic architecture for coping with different tasks with various input and output lengths. The authors present a thorough study of Perceiver IO across different application scenarios, such as language, spatial vision, multi-modality. The proposed method can achieve comparable or even better performance than the baselines. The main strength of Perceiver IO is its high flexibility to various input domains. While keeping the main architecture, we can focus on engineering the read-process-write process. One question is whether we should reformulate all these tasks as the way shown in this paper. The results are comparable to baseline methods in specific domains","This paper proposes Perceiver IO, a general neural architecture that handles general purpose inputs and outputs. It operates directly in the raw input domains, and thus does away with modality specific architecture components. The paper contains extensive experiments showing the capabilities of this architecture in different domains. The paper received very positive reviews from all reviewers. Some concerns included a need for additional details such as a missing task from GLUE, FLOPs comparisons to past works, nomenclature for the versions of Perceiver IO, etc. These concerns were well addressed by the authors. Others concerns by reviewers were the lack of experiments in a multi task setting. However, it was acknowledged by the authors and reviewers that this is an",0.8581692576408386
"The paper is well written and the problem at stake is really interesting, but I find the presentation of experimental results misleading. The paper is overall well-written and the claims are quite well supported by the experiments. The paper is well written and the problem at stake is really interesting, but I find the presentation of experimental results misleading. The paper is well written and the problem at stake is really interesting, but I find the presentation of experimental results misleading. The paper is well written and the problem at stake is really interesting, but I find the presentation of experimental results misleading. The paper is well written and the problem at stake is really","The paper proposes an interesting hypothesis about deep nets' generalization behavior inside RL methods: it suggests that the nets' implicit regularization favors a particular form of degeneracy, in which there is excessive aliasing of state-action pairs that tend to co-occur. It proposes a new regularizer to mitigate this problem. It evaluates the hypothesis and the regularizer empirically, and it provides suggestive derivations to motivate both. The reviewers praised the comprehensive empirical analysis, the insights into learning, and the combination of empirical and theoretical evidence. The authors participated responsively and helpfully in the discussion period, and addressed any concerns raised by the reviewers. This is a strong paper: it derives and motivates a",0.8141999840736389
"This paper proposes a new evaluation metric for multi-instrument AMT. The authors propose MV2H. The paper proposes a novel, musically relevant evaluation metric for multi-instrument AMT. The authors should contrast their metrics choice and the novel metric in the paper with the novel metric in MV2H.","This work concerns Automatic Music Transcription (AMT) -- transcribing notes given the audio of the music. The paper demonstrates that a single general-purpose transformer model can perform AMT for many instruments across several different transcription datasets. The method represents the first unified AMT model that can transcribe music audio with an arbitrary number of instruments. All reviewers rated this paper highly and are excited about seeing it at the conference. One reviewer noted that ""This paper seems to be a great milestone in the AMT research. It is probably the first unified AMT model that can take music audio with an arbitrary number of instruments."" The reviewers had some suggestions and comments, which appear to be addressed by the authors.",0.8424774408340454
"This paper proposes a new approach for the classification of graph-based data for classification and regression tasks. The proposed approach is an end-to-end, bilevel, combination of label propagation and boosting. The authors contribute not only an empirical analysis of the proposed approach on 8 datasets demonstrating the effectiveness of the proposed approach as well as a theoretical analysis. The proposed approach has a convergence guarantee and is shown to be very effective empirically. Overall, this seems to be a strong result. Compared to the best previous method BGNN, combining boosting and GNNs, EBBS achieves stronger empirical results. The algorithms and theoretical results are discussed","The paper addresses a problem encountered in many real-world applications, i. e. the treatment of tabular data, composed of heterogeneous feature types, where samples are not i. i. d. In this case, learning is more effective if the typically successful approach for i. i. d. data (boosted decision trees + committee techniques) is combined with GNN to take into account the dependencies between samples. The main contribution of the paper with respect to previous work in the field is the introduction of a principled approach to pursue such integration. One important component of the proposed approach is played by the definition of a specific bi-level loss (efficient bilevel boosted smoothing) that allows for convergence",0.852536678314209
The main strengths of this paper are the strong empirical results on recently introduced benchmark (OC20) for learning on small molecules. The authors put down a good effort to jot connection between various convolutional and message passing algorithms in the main paper. Although I enjoyed reading the manuscript I think it would benefit from a clearer presentation of the novel technical contributions. The overall contribution and novelty is minimal.,"This work combines steerable MLPs with equivariant message passing layers to form Steerable E(3) Equivariant GNNs (SEGNNs). It extends previous work such as Schnet and EGNNs, by allowing equivariant tensor messages (in contrast to scalar or vector messages). The paper also provides a unifying view of related work which is a nice overview for the ML community. It is overall well written, but would benefit from further revision to improve readability in some parts (in particular section 3, cf. reviews). It shows strong empirical results on the IS2RE task of the OC20 dataset and mixed results on the QM9 dataset.",0.853399395942688
"The paper is well written with nice presentation and clear description. The proposed method has sound motivation, i. e., bridging the gap between the training process and the actual test protocol when deployed for open-set applications. The new loss formulation also benefits parallel training setup which are widely adopted nowadays. In addition to new formulation of the binary classification training protocol, there are multiple components contributing to the final performance, including sample balancing, easy/hard sample mining, angular margin and similarity adjustment. The experiments are comprehensive, and show the state-of-the-art performance of the proposed method. The paper is well-written and easy",All reviewers agree that the proposed SphereFace2 approach - training face recognition models by using multiple binary classification losses - is interesting and innovative. The reviewers agree that the paper is well written and are satisfied with the presented experimental study. The rebuttal addressed all additionally raised questions. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a spotlight.,0.8484815955162048
"This paper presents the theoretical analysis presented in section 5 along with figure 2 to motivate the use of model ensemble. The experiments conducted in this work are extensive. In most of the setups, model ensemble resulted in improvements in the certified accuracy. The main claim about the time reduction that Certify ADP provides is based on an unfair comparison with Certify. The amount of information that both algorithms provide is not the same. I am not trying to lower the importance of Certify ADP since in many practical situations, it is useful to know whether an instance is certified with a given radius or not, but the comparison against Certify needs to be","This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets. The strengths of the paper are as follows: + In terms of significance of the topic, the problem tackled in the paper is significant and highly relevant. + The motivation of using model ensemble is clearly illustrated via a figure and well justified with theoretical analysis. + Algorithmically, the paper proposes Adaptive Sampling and K-consensus algorithms to reduce the computational cost,",0.8517851233482361
This paper presents a new approach for pruning neural networks based on the second-order correlation information. The proposed approach is shown to be better than existing methods. The reviewers unanimously agree that the proposed approach is an interesting contribution to the literature.,"Four reviewers have evaluated this submission with one score 6 and three scores 8. Overall, reviewers like the work and note that *a rigorous and principled approach is taken by this work*. AC agrees and advocates an accept.",0.8528173565864563
"The proposed MetaLink is able to leverage the correlations among tasks successfully. Authors should experiment on multi-task benchmark datasets such as CityScape, NYUv2, Pascal-Context, Taskonomy, Office-31, Office-Home. The usage of the labels of the auxiliary task is well motivated. The writing could have been more precise in some parts of the paper.","The paper describes a novel learning scenario where there are many related tasks, some seen at test time, and some seen only at training time, where additionally the task labels can be hidden or present. This approach generalizes both a ""relational setting"" (where auxiliary task labels could be used as features) and a ""meta setting"" (where new tasks need to be solved in a zero-shot setting using data from related tasks only). The idea behind the method is to do MTL with a common representation and a set of task-specific heads, and build a graph where (1) tasks are nodes associated with the parameters of their task-specific ""heads"" and (2) edges link examples to tasks",0.8269243836402893
"The proposed approach seems like a novel combination of existing methods with the advantage of requiring less prior knowledge and higher sample efficiency. The problem of the growing size of transformers as we require larger time dependencies is of great concern. The trick of forwarding these representations to an LSTM layer is simple yet effective from the results provided. The paper evaluates the proposed solution in multiple hard and well known benchmarks, authors also provide relevant ablation studies.","This paper introduces a new transformer architecture for representation learning in RL. The key ingredients of the proposed architectures are a novel combination of existing methods: (1) the use of LSTMs to reduce the need for large transformers and (2) a contrastive learning procedure that doesn't require human data augmentation. The resulting approach requires less prior knowledge and provides higher sample efficiency. The paper is convincing, with comprehensive experiments on multiple challenging and well-known benchmarks and an ablation study. The reviewers did expressed concerns that parts of the paper are a very difficult read and could use improvement, especially those relying on substantial external background. The intuition behind several components could be improved, and there are some clarity issues, as detailed in the",0.877589225769043
"The authors' presentation is concise and clear, and the paper is easy to follow (modulo the additional comments I provide below). The authors still need to provide an outlook for other cases as the authors do not seem to position their work as a purely conceptual development. The authors could also consider moving the related work section to the end of the introduction to further strengthen their initial setup. The authors could also consider moving the related work section to the end of the introduction to further strengthen their initial setup. The paper is well motivated and supported by rigorous theoretical derivations, making it sufficiently worthwhile for an interested reader to spend the extra effort.","This paper is a solid contribution to researchers in this field, as it provides a new idea for the basic problem of determining the direction of causality between two variables, using the functional causal model as a dynamical system and optimal transport.",0.840290904045105
"The paper proposes Federated Robust Decoupling (FED-ROD) to excel on both generic FL and personalized FL. With extensive results, it shows that strong personalized models emerge from the local training step of generic FL algorithms. The paper demonstrates that FED-ROD enables zero-shot adaptation and much effective fine-tuning for new clients.","This manuscript proposes and analyses can approach to address the centralized and personalized tasks in federated learning jointly. Existing work has tackled this issue by developing separate tasks. Instead, this manuscript proposes a shared architecture that aims to optimize centralized and personalized models. One observation motivating this work is that local models trained during federated learning effectively optimize local task performance. The resulting approach results well when label shifts primarily drive the client variability. Here, the centralized components are trained to optimize a balanced risk, while the local components are trained to optimize the standard empirical risk. Reviewers agree that the manuscript is well-written and appropriately addresses the timely issue posed. The main concerns are the clarity of the technical contributions and technical statements during the review",0.8422124981880188
"The paper proposes a loss that has a unique optimum avoiding the shortcomings of VAML-based algorithms, thus opening the possibility to get the best of both worlds. The proposed VaGraM loss is designed to specifically remedy the two presented issues in the original VAML loss. The paper is generally well written and easy to follow. The experiments miss a few relevant baselines. One of the motivations of the paper is that training models using value estimates outside of the empirical state distribution might result in inadequate VAML losses. On the other hand, it is unclear whether it is tractable to ask for guarantees outside of the training distribution. The","This paper studies model-based RL in the setting where the model can be misspecified. In this case, MLE of model parameters is a not necessarily a good idea because the error in the model estimate compounds when the model is used for planning. The authors solve this problem by optimizing a novel objective, which takes the quality of the next state prediction into account. This paper studies an important problem and this was recognized by all reviewers. Its initial reviews were positive and improved to 8, 8, 6, and 6 after the rebuttal. The rebuttal was comprehensive and exemplary. For instance, one concern of this paper was limited empirical evaluation. The authors added 5 new benchmarks and also included stabilizing improvements in their original baseline",0.8432763814926147
"This paper contains a series of sound, controlled experiments to study the role of input representations in language modeling. The experiments are very controlled, from data, number of lines, language mappings, and segmentations. I think the experiments are well designed, and with a clear focus to answer the research questions laid out in 1.2. Overall, I do think that this paper will benefit the multilingual NLP community in thinking carefully about the choice of input representations, rather than advocating a one-choice fits all solution. However, I do have a few concerns that prevent me from giving a higher score. The claim that this paper ""renders a","The authors address a very important question pertaining to the relevance of morphological complexity in the ability of transformer based conditional language models. Through extensive (controlled) experiments using 6 languages they answer as well as raise very interesting questions about the role of morphology/segmentation/vocab size which mat spawn more work in this area. All the reviewers were positive about the paper and agreed that the paper made significant contributions which would be useful to the community. More importantly, the authors and reviewers engaged in meaningful and insightful discussions through the discussion phase. The authors did a thorough job of addressing all reviewer concerns and changing the draft of the paper accordingly. I have no hesitation in recommending that this paper should be accepted.",0.8580828905105591
"This paper contributes a useful and apparently novel algorithm for an important problem in the application of RL methods in real-world settings. The paper is very clear and well written, and the explanation of the algorithm is easy to follow. The experimental methods (selection of simulated experiments, measured metrics etc.) appear to be sound, and it is good to see experiments on a real-world robot which substantiate the claim that this method is useful for real-world RL. The discussion of related work seems to be missing a few recent papers that might be relevant. These methods should also be represented in the simulated experiments to enable a fair comparison","The authors introduce a method for improving reinforcement learning in sparse reward settings. In particular, they propose to take advantage of a suboptimal behavior policy as a guidance policy that is incorporated in a TRPO-like update. The reviewers agree that this is a novel and interesting idea and given the authors' rebuttal with additional experiments, clarifications and discussions, they agreed to accept the paper. However, they also point out several flaws (e. g. evaluation on a more challenging sparse-reward task such as Adroid) that I encourage the authors to address in the final version of the paper.",0.8512600660324097
"The authors propose a new method of evaluating ECs by training (briefly) a translation model to translate to natural language captions from EC language generated from the same images. The authors propose a new method of evaluating ECs by training (briefly) a translation model to translate to natural language captions from EC language generated from the same images. The authors propose a new method of evaluating ECs by training (briefly) a translation model to translate to natural language captions from EC language generated from the same images. The authors argue that ""intuitively, a higher translation score means the emergent and natural sentences are closer in structure and semantics","This paper explores ways in which *emergent communication* (EC) methods from representation learning can be evaluated extrinsically, by hooking them into downstream NLP tasks. Reviewers agree that the paper is thorough, and finds encouraging results. This paper is borderline, and difficult to evaluate, even after very substantial discussion (some of it private). From my reading of the reviews and pieces of the paper, I'm very sympathetic to wvqW's concern that none of the present-day applications under study seem likely to benefit from this kind of emergent communication pretraining: *Natural* language pretraining, even transferring across natural languages, is for too strong a baseline, and it's not even",0.8154651522636414
This paper proposes introduction of the multi- parameter persistence (as defined in the topological analysis) to capture the time-dependent properties of ad-temporal data and be a part of time-aware learning of a convolutional network. The paper presents a novel research direction with improved performance over state-of-the-art approaches.,"The authors introduce the Time-Aware Multiperistence Spatio-Supra Graph CN that uses multiparameter persistence to capture the latent time dependencies in spatio-temporal data. This is a novel and experimentally well-supported work. The novelty is achieved by combining research in topological analysis (multipersistence) and neural networks. Technically sound. Clear presentation and extensive experimental section. Reviewers were uniformly positive, agreeing that the approach was interesting and well-motivated, and the experiments convincing. Some concerns that were raised were successfully addressed by the authors and revised in the manuscript. Happy to recommend acceptance. A veru nice paper!",0.8591549396514893
"The paper presents MultiBERTs, a set of 25 model checkpoints, and the Multi-Bootstrap, a non-parametric method to estimate the model uncertainty. The experiments verified the proposed method on a case study of gender bias in coreference resolution.",This paper is a resource and numerical investigation into the variability of BERT checkpoints. It also provides a bootstrap method for making investigations on the checkpoints. All reviewers appreciate this contribution that can be expected to be used by the NLP community.,0.8590666651725769
"In particular, the METHOD section is very vague and not clearly explained. Notation are introduced without definition and no explanation for the background needed to understand the notation. The authors did not mention that in the paper. The ideas of the paper are interesting: the pushforward trick is appealing. The architecture, however, looks like a separate idea, which is not connected to the loss. The paper proposes two separate innovations, puts one into the title, but experiments confirm the effectiveness of only one. However, the study of the architecture lacks the analysis of the systematic convergence of the solution, given by the new architecture, with the number of","This paper proposes a message passing neural network to solve PDEs. The paper has sound motivation, clear methodology, and extensive empirical study. However, on the other hand, some reviewers also raised their concerns, especially regarding the lack of clear notations and sufficient discussions on the difference between the proposed method and previous works. Furthermore, there is no ablation study and the generalization to multiple spatial resolution is not clearly explained. The authors did a very good job during the rebuttal period: many concerns/doubts/questions from the reviewers were successfully addressed and additional experiments have been performed to support the authors' answers. As a result, several reviewers decided to raise their scores, and the overall assessment on the",0.8474766612052917
"This paper is well motivated and most parts are well written, but the main method section is written to be difficult to follow. The main contribution is an exploration strategy with an in-episode switch from an exploitation policy to one aimed at exploration. While the exploration region is less explicitly located at the edge of the known state space region, it would be valuable to investigate a bit more whether a more flexible solution is also possible here.","I thank the authors for their submission and active participation in the discussions. All reviewers are unanimously leaning towards acceptance of this paper. Reviewers in particular liked that the paper is well-written and easy to follow [186e,TAdH,Exgo], well motivated [TAdH], interesting [PsKh], novel [186e] and provides gains over baselines [186e,TAdH,PsKh] with interesting ablations [186e,Exgo]. I thus recommend accepting the paper and I encourage the authors to further improve their paper based on the reviewer feedback.",0.8209325075149536
This paper presents a meta-analysis of pre-trained NLP models. The paper is well-written and easy to follow. The paper is well-written and well-written. The paper is well-written and easy to follow. The paper is well-written and well-written. The paper is well-written and well-written. The paper is well-written and well-written. The paper is well-written and well-written. The paper is well-written and well-written. The paper is well-written and well-written. The paper is well-written and well-written. The paper,"This paper provides a very large-scale study on the pretraining of image recognition models. Specifically, three scaling factors (model sizes, dataset sizes, and training time) are extensively investigated. One important phenomenon observed by this paper is that stronger upstream accuracy may not necessarily contribute to stronger performance on downstream tasks---actually sometimes these two types of performance could even be at odds with each other Overall, all the reviewers enjoy reading this paper and highly appreciate the empirical results presented in this paper. There were only a few concerns raised by the reviewers but most were well addressed during the discussion period. All reviewers reach a consensus on accepting this paper and believe this study is worthy to be heard by the community.",0.8191849589347839
"The paper shows the first universal approximation with constraints. The general theory shows that the universal transformer can use the probabilistic approximation to project suboptimal outputs. The paper is well presented. However, there are still some typos. The authors need to training their writing. It would be a good idea to provide more examples. The paper may be applicable to practical use cases.","The paper studies an interesting question of whether neural networks can approximate the target function while keep the output in the constraint set. The constraint set is quite natural for e. g. multi-class classification, where the output has to stay on on the probability manifold. The challenge here is that traditional universal approximation theory only guarantees that f(x)f(x), but can not guarantee that f(x) lies exactly in the same constraint set as f(x). The paper made a significant contribution in the theory of deep learning -- It is shown that the neural network can indeed approximate any regular functions while keep the output stay in the regular constraint set. This gives",0.8447548747062683
"The proposed function of model capacity for encoder-decoder NMT model performance is useful for parameter allocation of NMT models training. The problem is of great value for both NMT practice and research. The authors have proposed three research questions, while the relationship among them is not clear. The scaling property of the composition bias of the train/test sets has been less studied in previous work, and is most interesting to the reviewer. This part can be more convincing if the authors can provide more experiments and analyses.","This is a strong empirical paper that studies scaling laws for NMT in terms of several new aspects, such as the model quality as a function of the encoder and decoder sizes, and how the composition of data affects scaling, etc. The extensive empricial results offer new insights to the questions and provide valuable guidance for future research on deep NMT. The datasets used in the study are non-public, which may make it hard to reproduce the evaluation.",0.868897020816803
"This paper presents a new method for learning the optimal policy of a noise-based game. The proposed method is novel, it has significant differences compared to previous methods. The authors explain the rationale and the theory behind it in details. For evaluation, the authors give a pretty detailed comparison with different types of target domains and through comparison with other methods. The results show that the AdaRL outperform other methods, but the error bounds are very large (both for AdaRL and other methods) with respect to the difference between the methods. Overall I quite liked this paper as it introduces a well-motivated idea that can be quite useful to the","The authors present a method called ""AdaRL"" that learns a structured latent representation that characterizes relationships between different variables in an RL system. The method is evaluated on modified Pong and Cart-Pole domains and it is shown to outperform other transfer learning baselines. The reviewers agree that the method makes sense and addresses an important problem of transfer in RL. The authors did a good job in the rebuttal to empirically validate their claims and provided extra experiments. The reviewers also point out that the evaluated domains are rather simple and the paper would benefit from evaluations in a more complex environment as well as better writing. Please focus on improving these aspects in the final version of the paper.",0.8608298301696777
"The proposed method achieves a promising speedup of 80-500x. The proposed method seems general and can be applied to other rigid docking scenarios, i. e., predicting a rotation and translation matrix. The algorithm can potentially be adapted for protein-ligand docking, which is also an important task in computational chemistry. The authors have put a significant amount of time into designing, developing, and testing their proposed method, and it introduces and adapts several interesting ideas. The paper is well written and the mathematical formulation of SE(3)variant rigid protein docking is very nice. The authors also promise they will release the code and datasets after reviewing the","This paper introduces a novel SE(3) equivariant graph matching network, along with a keypoint discovery and alignment approach, for the problem of protein-protein docking, with a novel loss based on optimal transport. The overall consensus is that this is an impactful solution to an important problem, whereby competitive results are achieved without the need for templates, refinement, and are achieved with substantially faster run times.",0.8563282489776611
The paper proposes a unified framework for parameter-efficent tuning that includes several state-of-the-art methods as instantiations. The paper is well organized and well-motivated with theoretical analysis and proof. The proposed method Scaled PA consistently shows its advantages against the other baselines.,"The paper reviews and draws connections between several parameter-efficient fine-tuning methods. All reviewers found the paper addresses an important research problem, and the theoretical justification and empirical analyses are convincing.",0.8889412879943848
"The authors propose an interesting hypothesis that referring to the training data could be helpful for language modeling, and they showed that the method is able to make considerable improvements over the vanilla LM.","This paper introduces a new type of language model, the GNN-LM, which uses a graph neural network to allow a language model to reference similar contexts in the training corpus in addition to the input context. The empirical results are good, and the model sets a new SOTA on the benchmark Wikitext-103 corpus, as well as improving over strong baselines on two other language modeling datasets (enwiki8 and Billion Word Benchmark). The main drawback, as noted by one reviewer, is the computational expense of the method with significant slowdowns compared to the baseline. Two reviewers voted strong accept, with a third raising several concerns. The largest concern was the lack of comparison to prior work,",0.8598130941390991
This paper proposes a method to learn new tasks on low-rank filter subspace. The proposed method achieves good performance on the benchmark dataset. The authors may need to compare the inference time among different methods. The proposed low-rank filter strategy would allow highly efficient model storage. The authors may need to compare the inference time among different methods. The proposed method is based on the over- parameterization of deep models. The authors may need to compare the inference time among different methods. The proposed method achieves good performance on the benchmark dataset. The authors may need to compare the inference time among different methods. The proposed method is based on,"The authors propose a memory-based continual learning method that decomposes the models' parameters and that shares a large number of the decomposed parameters across tasks. In other words, only a small number of parameters are task-specific and the memory usage of storing models from previous tasks is hence a fraction of the memory usage of previous approaches. The authors take advantage of their method to propose specific ensembling approaches and demonstrate the strong performance of their methods using several datasets. In the rebuttal, the authors were very reactive and provided many useful additional results during including a comparison of the computational cost of their method vs. others, results using two new datasets (CUBS & Flowers), and additional results on mini",0.8414655923843384
"The paper introduces the algorithm well and the order of information seems nice. In general, the paper introduces the algorithm well and the order of information seems nice. I focus the rest of the review on various ways I think the paper could be improved further.","This paper proposes an innovative method for continual learning that modifies the direction of gradients on a new task to minimise forgetting on previous tasks without data replay. The method is mathematically rigorous with a strong theoretical analysis and excellent empirical results across multiple continual learning benchmarks. It is a clear accept. There was good discussion between the reviewers and authors that addressed a number of minor issues, including clarifying that the method has the same computational complexity as backpropagation. The authors are encouraged to make sure that these points are addressed in the final version of the paper.",0.853913426399231
