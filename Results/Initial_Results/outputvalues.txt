generated: the the the the the the. the. the the, the.. the, the, the the and the the of the and the of the the to the the a the the paper the the- the the in the the is the
original:There was a healthy discussion with all the reviewers with a consensus that the results are somewhat expected and unlikely to shed light beyond the ntk regime, yet within the confine of ntk there is a solid and nicely written technical contribution.
generated: the the the the the the. the. the.. the the, the, the,, the the and the and the the of the and the of the of and of the the paper the
original:The main detractor of this paper feels that the paper makes a relatively small technical and empirical contribution given existing results on HER (Andrychowicz et al., NeurIPS 2017). However, several other reviewers, who had more engagement in the discussion, were strong supporters. Having looked at the paper myself I thought the selection of experimental problems undermined the results. Experiments are most compelling when many unaffiliated groups compete on the same benchmarks. But the basic idea of integrating HER with AlphaZero, and a reasonable attempt at this, seems to be interesting enough to warrant a poster.
generated: the the the the the the. the. the. the the, the, the, the the of the the and the and the and,. and the the to the of the of of the
original:This paper received six reviews, consisting of three 8s two 6s and one 3. The reviewers generally felt that the proposed Electra-like pretraining provided fairly significant downstream improvements. Additional ablations were provided to during the author response period and other author responses were sufficient to cause scores to rise during the discussion period. The vast majority of reviewers recommended accepting this paper and the AC also recommends acceptance.
generated: the the the the the. the the. the the, the.. the, the the and the, the and the the of the and the of the the paper the the to the the a the the- the the in the
original:This paper provides generalization bounds for meta-learning based on a notion of task-relatedness. The result is natural and interesting--intuitively, when tasks are similar, then meta-learning algorithms should be able to utilize all data points across all tasks. The theoretical contribution is novel, and the results also provide more practical insight into the performances of some models.
generated: the the the the the the. the. the the, the.. the, the the of the and the the and the of the,, the and the of and of the the to the
original:This paper offers a refinement of the information-theoretic characterization of the generalization of models obtained via SGD. This is assessed on some basic neural architectures and inspires the use of new regularizers. Overall, even though the perspective of this paper is not novel, the presented results appear to be clearer and tighter than prior instances of the same ideas. This was appreciated by most reviewers. The few clarity and organization concerns that were raised by the reviewers were adequately addressed by the authors. Overall, the paper deserves to be shared with the community.
generated: the the the the the the. the. the the, the, the the of the the and the,. the., the of the and the of the and of and the the to the
original:The authors analyzing the VC-dimension of a class of neural networks with hard thresholds at the hidden nodes that include a low-rank weight matrix and hard-thresholds at hidden units. The bounds are independent of the number of weights used to represent functions mapping a hidden layer to the output. They also provided some experiments supporting the practicality of networks like those treated in their theoretical analysis. There was some question about whether the VC-dimension continues to be relevant. Also, while the upper bounds have attractive properties that were highlighted by the authors, they also are not very strong in other respects. The consensus view overall, though, was that this is a "nice result", a clean illustration
generated: the the the the the. the the. the.. the, the the, the, the and the the of the the and the of the and the of and, of the to the
original:This paper explores a classification approach based on labeling pairs of inputs concurrently using a single network, rather than singletons. The authors test the approach on adversarial robustness (towards norm bounded perturbations), OOD detection next to basic standard accuracy calculations. While the key idea is potentially interesting and the paper has received positive comments from the majority of reviewers, there were also some concerns that need to be addressed in a final manuscript: * The paper does not motivate or explain theoretically why the joint classification framework is superior, beyond verbose arguments. These arguments need to be better clarified and linked with the experimental evaluation. * While the empirical results are perceived as positive by the reviewers,
generated: the the the the the. the the. the. the, the, the the,., the and the the and the of the the of the and of and the of the to the
original:The paper addresses vision-based and proprioception-based policies for learning quadrupedal locomotion, using simulation and real-robot experiments with the A1 robot dog. The reviewers agree on the significance of the algorithmic, simulation, and real-world results. Given that there are also real-robot evaluations, and an interesting sim-to-real transfer, the paper appears to be an important acceptance to ICLR.
generated: the the the the the the. the. the. the the, the, the the and the, the and the the of the and the of the of the the a the the to the the paper the the- the the in the the
original:This paper explores addition of a version of divisive normalization to AlexNets and compares performance and other measures of these networks to those with more commmonly used normalization schemes (batch, group, and layer norm). Various tests are performed to explore the effect of their divisive normalization. Scores were initially mixed but after clarifications for design and experiment decisions, and experiments run in response to comments by the reviewers the paper improved significantly. While reviewers still had several suggestions for further improvements, after the authors' revisions reviewers were in favor of acceptance which I support.
generated: the the the the the the. the. the.. the the, the the and the, the,, the and the the of the and the of the of and of the the paper the
original:The paper studies an interesting question of whether neural networks can approximate the target function while keep the output in the constraint set. The constraint set is quite natural for e. g. multi-class classification, where the output has to stay on on the probability manifold. The challenge here is that traditional universal approximation theory only guarantees that f^(x)≈f(x), but can not guarantee that f^(x) lies exactly in the same constraint set as f(x). The paper made a significant contribution in the theory of deep learning -- It is shown that the neural network can indeed approximate any regular functions while keep the output stay in the regular constraint set. This gives
generated: the the the the the. the the. the. the, the the, the, the and the the and., and the and the of the the of the of the to the the to the
original:In this paper, the authors study the decentralized empirical risk minimization problem with Reproducing Kernel Hilbert Space. I found the problem formulation and the solution quite interesting. The authors also answered the main comments of the reviewers. Even though part of the work is incremental, I feel that there is enough merit to accept this paper.
generated: the the the the the the. the. the. the the, the, the, the the and the the of the and the of the and., and the of of the the to the
original:This manuscript proposes and analyses a bucketing method for Byzantine-robustness in non-iid federated learning. The manuscript shows how existing Byzantine-robust methods suffer vulnerabilities when the devices are non-iid, and describe a simple coordinated attack that defeats many existing defenses. In response, the primary algorithmic contribution is a bucketing approach that aggregates subgroups of devices before robust aggregation. This approach is also easily composed with existing Byzantine-robust methods. The manuscript includes an analysis of the performance of the proposed approach, including an information-theoretic lower bound for certain settings. During the review, the main concerns are related to the clarity of the technical contributions,
generated: the the the the the. the the. the.. the, the the, the, the and the the of the the and the of the and the of, and of the to the
original:This paper proposes a new wavelet-based model to represent textures. The model incorporates a wide range of statistics, by computing covariances between rectified wavelets coefficients, at different scales, phases and positions. The model can synthesize textures that have a similar quality to state-of-the-art texture models using CNN structure. Qualitative results are shown to demonstrate the effectiveness of the model. The paper studies an important problem in computer vision and neuroscience, which is texture modeling. However, many important related works are missing. After rebuttal, three of four reviewers champion accepting the work because the proposed wavelet-based texture model, which produces competitive synthesis with much less parameters than
generated: the the the the the the. the. the. the the, the, the the and the, the and the the of the and the of the of the the to the the paper the the a the the- the the in the the
original:This work proposed to detect backdoor in a black-box manner, where only the model output is accessible. Most reviewers think it is a valuable task, and this work provides a novel perspective of using adversarial perturbation to diagnosis the backdoor. Some theoretical analysis for linear models and kernel models are provided. There is still huge gap to analyze the DNN model. But on the other side, it provides some insight to understand the proposed method and could inspire further studies. Besides, since there have been many advanced backdoor attack methods, and many more are coming out, I am not sure that the proposed detection criteria is well generalizable, considering only some typical attack methods are tested. However,
generated: the the the the the the. the. the the, the. the, the the and the the of the and the,... the.., the of the and and the of the
original:Strong submission that analyses the unsupervised skill discovery setting from the perspective of information geometry, which leads to some interesting conclusions. In particular, it is shown that this does not lead to skills that are optimal for all reward functions, but does provide a good initialization for methods that aim to find optimal policies. Across the board, the reviewers believe the analysis provided by this work is both important and novel. And while there were some initial concerns raised, such as lack of empirical confirmation of some of the claims and some questions about the analysis, the authors have addressed all of these concerns convincingly. Hence, I strongly recommend acceptance of this submission.
generated: the the the the the the. the. the. the the, the, the,., the the and the and the the of the and the of the of and of the the to the
original:This paper addresses an important issue of AutoML systems, specifically their ability to "cold start" on a new problem. Some of the reviewers initially had concerns about the experimental validation and the theoretical foundations of the method, but during the discussion phase the authors addressed concerns extremely well. The authors already included most of the feedback of reviewers, further strengthening the paper.
generated: the the the the the the. the. the the, the. the, the, the the and the and the the of the of the and the of., and of the the to the
original:The paper presents an approach to learn the surrogate loss for complex prediction tasks where the task loss is non-differentiable and non-decomposable. The novelty of the approach is to rely on differentiable sorting, optimizing the spearman correlation between the true loss and the surrogate. This leads to a pipeline that is simpler to integrate to existing works than approaches that try to learn a differentiable approximation to the task loss, and to better experimental results. The paper is well written and the approach clearly presented. The reviewers liked the simplicity of the approach and the promising experimental results on a variety of challenging tasks (human pose estimation and machine reading).
generated: the the the the the the. the. the the, the. the, the the and the the of the the to the the paper the the a the the- the the in the the is the the that the the The the the for the the this the the
original:The authors propose a multi-resolution pyramidal attention mechanism to capture long-range dependencies in time series forecasting, achieving linear time and space complexity. The authors conduced an extensive set of experiments and ablation studies demonstrating that the proposed method consistently outperforms the state-of-the-art and provided evidence for the various components of the architecture. They also provided a proof guarantee the linear complexity of long sequence encoding and adequately addressed the concerns raised by the reviewers. The additional additional benchmarks conducted by the author further demonstrated the strong performance of the method. All reviewers agreed that this work makes a solid contribution to the field.
generated: the the the the the. the the. the.. the, the the, the, the and the and the the of the the and, and the of the of of the to the
original:The authors introduce a novel probabilistic hierarchical clustering method for graphs. In particular they design an end-to-end gradient-based learning to optimize the Dasgupta cost and Tree Sampling Divergence cost at the same time. Overall the paper presents solid results both from a theoretical and experimental perspective so I think it is a good fit for the conference and I suggest accepting it.
generated: the the the the the. the the. the. the, the the, the and the the of the the and the,., the of the and the of and of the to the
original:*Summary:* Low-rank bias in nonlinear architectures. *Strengths:* - Significant theoretical contribution. - Well written; detailed sketch of proofs. *Weaknesses:* - More intuitions desired. - Restrictive assumptions. *Discussion:* Authors made efforts to improve the discussion in response to 6P7z. Authors agree with eeoo about Assumption 2 being relatively restrictive but point out that main results do not need it. They discuss Assumption 1 and revised it formulation. Reviewer eeoo was satisfied with this. Following the discussion udhX raised their score (after authors acknowledged an early problems and improved them) and found the paper well written
generated: the the the the the the. the. the. the the and the the, the, the the of the and the,... the.., the of the and and the of the
original:The paper proposes a new method for unsupervised text style transfer by assuming there exist some pseudo-parallal sentences pairs in the data. The method thus first mines and constructs a synthetic parallel corpus with certain similarity metrics, and then trains the model via imitation learning. Reviewers have found the method is sound and the empiricial results are decent. The assumption on pseudo-parallal pairs would limited the application of the methods in other settings where the source/target text distributions are very different. The authors have added discussion on this limitation during rebuttal.
generated: the the the the the the. the. the.. the the, the, the,, the the and the and the the of the and the of the of and of the the paper the
original:This paper proposes a new time-varying convolutional architecture (ST-GNN) for dynamic graphs. The reviewers were positive about the presentation and detailed theory, especially on the stability analysis. The shared criticism was on experimental validation synthetic datasets that the reviewers did not find appealing. The AC believes that while the lacking validation concerns are legit, there is a lack of sophisticated dynamic graph benchmarks in the community yet, so the authors did their best effort to test their method. We thus recommend to accept the paper.
generated: the the the the the the. the. the. the the and the the, the, the the of the and the,... the.., the of the and and the of the
original:This paper provides a very large-scale study on the pretraining of image recognition models. Specifically, three scaling factors (model sizes, dataset sizes, and training time) are extensively investigated. One important phenomenon observed by this paper is that stronger upstream accuracy may not necessarily contribute to stronger performance on downstream tasks---actually sometimes these two types of performance could even be at odds with each other Overall, all the reviewers enjoy reading this paper and highly appreciate the empirical results presented in this paper. There were only a few concerns raised by the reviewers but most were well addressed during the discussion period. All reviewers reach a consensus on accepting this paper and believe this study is worthy to be heard by the community.
generated: the the the the the. the the. the. the, the the, the, the and the the of the the and,. and the and the of the of of the to the
original:This paper studies online learning using SGD with momentum for nonstationary data. For the specific setting of linear regression with Gaussian noise and oscillatory covariate shift, a linear oscillator ODE is derived that describes the dynamics of the learned parameters. This then allows analysis of convergence/divergence of learning for different settings of the learning rate and momentum. The theoretical results are validated empirically, and are shown to generalize to other settings such as those with other optimizers (Adam) or other models (neural nets). The reviewers praise the clear writing and the rigorous and systematic analysis. 3 out of 4 reviewers recommend accepting the paper. The negative reviewer does not find the
generated: the the the the the the. the. the the, the. the, the the and the, the and the the of the and the of the the paper the the to the the a the the- the the in the the is the the
original:Three experts reviewed this paper and all recommended acceptance. The reviewers liked that the work addressed a common problem in prior related work that it is hard to quantitatively evaluate slide discovery methods. Moreover, the proposed method achieves superior performance over prior arts. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance. The reviewers did raise some valuable concerns, such as paper clarity, significance of the textual descriptions, that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!
generated: the the the the the the. the. the the, the. the, the the and the the of the the to the the a the the paper the the- the the in the the is the the that the the The the the for the the this the the
original:The paper introduces As-ViT, an interesting framework for searching and scaling ViTs without training. Overall, the paper received positive reviews. On the other hand, R1 rated the paper as marginally below the threshold, raising concerns about search on small datasets and issues regarding the comparison in terms of FLOPS/accuracy with other methods. The authors adequately addressed these concerns in the rebuttal, and helped clarify other questions by R2 and R3. R1 did not participate in the discussion after the author response nor updated his/her review. The AC agrees with R2 and R3 that the paper passes the acceptance bar of ICLR, as the unified approach for efficient search
generated: the the the the the the. the. the the, the.. the, the, the the and the and the the of the of the and, and the of of the the to the
original:This paper proposes an identifiable nonlinear ICA model based on volume-preserving transformations. The overall approach is very similar to the GIN method published @ ICLR 2020. There is a weak consensus among the reviewers that this paper has some merit, although none pushed for acceptance. After reviewing the paper myself, I agree that the contributions here appear to be incremental, but the results do push this growing field of identifiable latent variable models forward.
generated: the the the the the the. the. the.. the the, the, the,, the the and the and the the of the and the of the of and of the the paper the
original:The reviewers unanimously appreciated the clarity of the work as well as the framing of the proposed method. Congratulations.
generated: the the the the the the. the. the.. the the, the, the the and the the of the the paper the the to the the a the the- the the in the the is the the that the the The the the for the
original:The paper proposes a novel method, PI3NN, for estimating prediction intervals (PIs) for quantifying the uncertainty of neural network predictions. The method is based on independently training three neural networks with different loss functions which are then combined via a linear combination where the coefficients for a given confidence level can be found by the root-finding algorithm. A specific initialization scheme allows to employ the method to OOD detection. Reviewers agreed on the importance of the problem of producing reliable confidence estimates. The proposed method addressed some of the limitations of the existing approaches, and reviewers valued that a theoretical as well as an empirical analysis is provided. On of the main criticisms was that the theoretical derivation of
generated: the the the the the the. the. the the, the. the, the the and the and the the of the,., the and and the of the of of the the paper the
original:The paper describes an approach for automatically generating CAD sketches, including both the primitives that describe the drawing, as well as the constraints that describe relationships between the primitives that need to be maintained even if the primitives are changed. This is an important problem that is starting to receive a lot of attention from the literature. Overall, the paper is very well executed and the results are quite compelling. There were some concerns about the relationship with the work by Willis et al. and other papers that were published around the time when this paper was submitted. There is still some novelty in this paper relative to those works as argued in appendix H, but it would have been really good to have a more
generated: the the the the the the. the. the. the the and the the, the, the the of the, the of the and the and,. and the of the the to the the a the
original:This paper makes an important evaluation study to further the understanding of the representation capacity of DNNs. The novelty is in using the multi-order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs. The authors discovered an interesting representation bottleneck phenomenon, i. e., in a normally trained DNN, low-order and high-order interaction patterns are easy to be learned, while middle-order interaction patterns are difficult to be learned. They also propose two novel loss functions, which allow the model to encode interactions of specific orders, including middle-order interaction. All reviews are positive.
generated: the the the the the. the the. the, the the, the. the,., the and the the of the the and the and the of the of and of the to the
original:This paper presents an analytic approach for estimating the optimal reverse variance schedule given a pre-trained score-based model. The experimental results demonstrated the efficacy of the proposed method on several datasets across different sampling budgets. Given the recent interest in score-based generative models, I believe that the paper will find applications in various domains. I am pleased to recommend it for acceptance.
generated: the the the the the the. the. the. the the,., the, the, the the of the and the the and the of the and the of and.. the..
original:The authors introduce a method that improves goal-conditioned supervised learning (GCSL) by iteratively re-weighting the experience by a variable that correlates with the number of steps till the desired goal. The reviewers mention that the authors focus on an important problem, their method is simple and the empirical results are significant. However, they do point several flaws of the paper, the main ones being questionable theoretical claims and the clarity of the presentation. After an extensive discussion, most reviewers agree that the paper should be accepted but I do encourage the authors to take into account the comments by the reviewers for the final version of the paper and make the theory more clear.
generated: the the the the the. the the. the. the, the the,., the, the and the and the the of the the and and the of the of of the to the
original:The paper addresses hierarchical kernels and provides an analysis of their RKHS along with generalization bounds and cases where improved generalization can be obtained. The reviewers appreciated the analysis and its implications. There were multiple concerns regarding presentation clarity, which the authors should address in the camera ready version.
generated: the the the the the the. the. the. the the, the, the the and the the of the the to the the paper the the a the the- the the in the the is the the that the the The the the for the the this the the
original:The paper proposes monotonic splines as an improvement on current approaches to parametrising quantiles in distributional RL. The idea is an obvious, natural improvement on what exists, and yields improved experimental results.
generated: the the the the the the. the. the the, the. the, the the and the, the and the the of the and the of the of., and of the the to the
original:This is an exciting paper that provide the efficient algorithms for exact sampling from NDPPs along with theoretical results that are very pertinent in and out themselves. The AC agree with the reviewers that the authors satisfactorily addressed the concerns raised in the reviews, and is convinced that the revised version will be greatly appreciated by the community. We very much encourage the authors to pursue this line of work and in particular to overcome the practical restriction to the ONDPP subclass.
generated: the the the the the. the the. the. the, the the, the, the and the the of the the and the and the of,... the., the.. of the
original:Based on the previously observed neural collapse phenomenon that the features learned by over-parameterized classification networks show an interesting clustering property, this paper provides an explanation for this behavior by studying the transfer learning capability of foundation models for few-shot downstream tasks. Both theoretical and empirical justifications are presented to elaborate that neural collapse generalizes to new samples from the training classes, and to new classes as well. The problem that this paper delves into is important. The paper is well-motivated, and well structured with a good flow. Both theoretical and empirical analyses of the paper are solid. Preliminary ratings are mixed, but during rebuttal, multi-round responses and in-depth
generated: the the the the the the. the. the. the the, the the and the, the and the the of the, the and the of the the to the the a the the paper the the- the the in the the is the the
original:The paper addresses the interesting many-to-many assignement problem between a set of images and a set of text. Most reviewers, (and I agree with them) think that the idea and its application worth being published although the performance improvement is marginal. I request the authors to update the paper based on the discussions.
generated: the the the the the the. the. the. the the, the, the, the the and the the of the and the and,. and the of the the a the the to the the paper the
original:This work defines the new problem of lifelong few-shot language learning where the goal is to continually learn new few-shot tasks and use those to benefit future tasks while not forgetting previous tasks. With larger models, this is an important goal due to the cost of updating and retraining these models. The work also shows superiority to existing approaches like EWC and MAS. After the author's rebuttal, the experimental section is also thorough with evaluation on a good range of tasks and approaches such as adapters showing good results. While this setting appears simpler than the full lifelong-learning setting and the approach combines existing ideas, this work's contribution to the definition and thinking about this problem is valuable. However
generated: the the the the the the. the. the the, the. the, the,., the the and the the of the and the of the and the of and of the the to the
original:This paper proposes an elegant approach to object detection where an encoder network reads in an image and a decoder network outputs coordinate and category information via a sequence of textual tokens. This method does away with several object detection specific details and tricks such as region proposals and ROI pooling. The paper received positive reviews from all reviewers who agreed that this formulation of object detection was novel and provided a new perspective that may transfer to other computer vision tasks. One common concern amongst reviewers was the slow inference time due to the sequential nature of the decoder -- and this concern was a central point of discussion between the authors and reviewers. My takeaway from this discussion is that this model is certainly slower than traditional
generated: the the the the the the. the. the. the the, the, the, the the and the the of the and the and., and the of the of of the the paper the
original:All reviewers suggest acceptance of this paper, which reports the relationship between perceptual distances, data distributions, and contemporary unsupervised machine learning methods. I believe this paper will be of broad interest to different communities at ICLR.
generated: the the the the the the. the. the.. the the, the, the the and the and the the of the,, the and the of the of and of the the paper the
original:The paper addresses the problem of non-convex non-concave min-max optimization under the perspective of application of smoothed algorithms between two opponents. The paper examines a model where the max-player applied a zero-memory smooth (from differential perspective) algorithm and min-player SGD/SNAG or proximal methods providing results similar with the state-of-art. Convergence guarantees proposed were sound and experimental results on generative adversarial networks and adversarial training demonstrate the efficiency of the proposed algorithms.
generated: the the the the the the. the. the. the the, the, the,., the the of the the and the the to the the a the the paper the the- the the in the the is the the that the
original:This paper proposes a theoretically sound and practically effective method to compress quantized gradients and reduce communication in distributed optimization. The method is interesting and worth publication.
generated: the the the the the the. the. the.. the the, the, the,, the the and the the of the and the and and the of the of of the the paper the
original:The authors set up a simple combination of an energy based model and a flow based model that corrects the flow based model with an energy based term. The merits of this relative only an energy based model is improved sampling to compute the gradient. The advantage over a only flow based model is that the kinds of transforms that can be used are less limited.
generated: the the the the the the. the. the. the the, the, the, the the and the the of the and the and,... the.. and the of the the to the
original:This paper introduces a convolution where the kernel is parametrised continuously over time (in the context of recurrent networks) to address vanishing gradients issues, by using another neural network to generate the kernels. This is a meaningful idea, addressing an important problem. The paper is well written and clear. The idea is novel (parametrised kernel already exist, but the way it's used here is new). The experimental section is solid, although some reviewers suggests it could be extended with more baselines. All reviewers recommend to accept the paper, therefore I also recommend accept.
generated: the the the the the the. the. the. the the, the, the the and the the of the the to the the paper the the a the the- the the in the the is the the that the the The the the for the the this the
original:This paper provides a normal map-inspired implicit surface representation involving a smooth surface whose high frequency detail comes from normal displacements. Reviewers were impressed with the results and theoretical discussion in the paper. The AC agrees. The authors were responsive to reviewer feedback and addressed some questions about parameter choice during the rebuttal phase, including new experiments/discussion in the supplementary document. Note the response to reviewer WHEF notes that the authors will be releasing data/code; the AC strongly hopes the authors are true to their word in that regard. The AC chose to disregard some comments from reviewer G54X regarding tests with noise, as this method appears to be tuned to computer graphics applications; the level
generated: the the the the the the. the. the the, the.. the, the, the the and the the of the and the of the and the of, of the.... the
original:Multi-objective learning is an increasingly important topic. This paper presents a method for better finding parts of the Pareto frontier through a new method to estimate the distance to the frontier and use this proxy to refine the state space partition. The reviewers found this paper interesting and compelling and generally well written. The reviewers also thought the work could be further improved by better clarifying in the text where the proposed approach might fail, and what properties of the domain are needed, and also to better situate this paper within the related work, potentially including additional experimental comparisons. The authors provided detailed responses to the proposed questions and the authors are encouraged to ensure that these suggestions and discussions are well represented in
generated: the the the the the the. the. the. the the, the, the, the the and the the of the and the and,. and the of the the a the the to the the paper the
original:This paper introduces a new type of language model, the GNN-LM, which uses a graph neural network to allow a language model to reference similar contexts in the training corpus in addition to the input context. The empirical results are good, and the model sets a new SOTA on the benchmark Wikitext-103 corpus, as well as improving over strong baselines on two other language modeling datasets (enwiki8 and Billion Word Benchmark). The main drawback, as noted by one reviewer, is the computational expense of the method with significant slowdowns compared to the baseline. Two reviewers voted strong accept, with a third raising several concerns. The largest concern was the lack of comparison to
generated: the the the the the the. the. the.. the the, the the and the, the,, the and the of the the of the and of the of and the the paper the
original:All reviewers agree on acceptance and I agree with them. I recommend a spotlight.
generated: the the the the the. the the. the. the, the, the the,., the and the the of the the and the and the of the of and of the to the
original:This is an intriguing work that introduces a novel sparse training technique. The core insight is a novel reparametrization or sparsity pattern based on the so-called butterfly matrices that enables fast training and good generalization. The theory is solid and useful. Most importantly, the method is novel and is likely to become impactful. Understanding better what contributes to the excellent performance is an interesting question for future work. In agreement with all the reviewers, it is my pleasure to accept the work.
generated: the the the the the. the the. the. the, the the, the,., the and the the of the the and and the and the of the,. the.... the
original:Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors' rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.
generated: the the the the the the. the. the the, the.. the, the, the the and the and the the of the and the of the of the the to the the a the the paper the the- the
original:This paper proposes an extra loss to add on top of the contrastive learning. The contrastive learning seek representations invariant to transformation, while the extra loss the authors proposed encourage representations to be equivariant to the transformation (i. e. retain information about transformation in later representations). While reviewers and I agree this is a sensible motivation, and acknowledge good results that authors have obtained, the fact that most, if not all, improvement is combing from the 4-way rotation transformation is a bit unsatisfactory. Furthermore, this additional loss was proposed before and is actually quite well known, so the actual novelty in the proposed technique is somewhat limited. Nevertheless, this paper provides a comprehensive evaluation
generated: the the the the the the. the. the. the the, the, the the and the the of the the to the the a the the paper the the- the the in the the is the the that the the The the the for the the this the the
original:This paper regards video understanding as an image classification task, and reports promising performance against state of the arts on several standard benchmarks. Though the method is quite simple, it achieves good results. The visualization in this paper also provides good insight. All reviewers give positive recommendations for this paper.
generated: the the the the the the. the. the.. the the, the, the,, the the and the and the the of the and the of the of and of the the paper the
original:The paper presents a new framework of synthesizing differential private data using deep generative models. Reviewers liked the significance of the problem. They raised some concerns which was appropriately addressed in the rebuttal. We hope the authors will take feedback into account and prepare a stronger camera ready version.
generated: the the the the the the. the. the. the the, the, the, the the of the the and the and the and,. and the the to the the paper the the a the the- the
original:This paper provides some empirical investigation of the choice of the prior distribution for the weights in Bayesian neural networks. It shows empirically that, when trained via SGD, weights in feedforward neural networks exhibit heavy-tails, while weights in convolutional neural networks are spatially correlated. From this observation they show that the use of such priors leads to some improved performances compared to the iid Gaussian prior in some experimental settings. Reviewers have conflicting views on this paper, that have not been reconcilied after the author's response and the discussion. On the plus side, the paper is very well written, the experimental part is carefully conducted, and provides some insights on the choice
generated: the the the the the. the the. the. the, the the and the the of the the, the, the and the and... the.., and the of of the
original:The paper presents improvements to AlphaZero and MuZero for settings where one is restricted in the number of rollouts. The initial response from reviewers was generally favorable but the reviewers wanted more details and clarifications of multiple parts of the paper, and further intuition about the Gumbel distribution. The authors’ responses were detailed and convinced or maintained strong positive support of most reviewers. The authors also stated that they plan to provide a release of the code and also provided a policy improvement proof. Overall this is an interesting approach that is likely to be of significant interest to many.
generated: the the the the the. the the. the the, the.. the, the the and the, the and the the of the and the of the of and, of the the to the
original:All reviewers found that the proposed LM with Brownian motion is interesting and novel. Several reviewers raised (minor) concerns about experiments, but have been generally resolved by the authors.
generated: the the the the the. the the. the the, the.. the, the the and the, the and the the of the and the of the of and, of the the to the
original:The paper provides a procedure for certifying L2 robustness in image classification. The paper shows that the technique indeed works in practice by demonstrating it's accuracy on CIFAR-10 and CIFAR-100 datasets. The reviewers are positive about the paper. Please do incorporate feedback, especially around experimental setup to ensure that the work compares various methods fairly and provides a clear picture to the reader.
generated: the the the the the the. the. the. the the, the, the the and the the of the the to the the paper the the a the the- the the in the the is the the that the the The the the for the the this the the
original:All the reviewers agree that this paper made a solid contribution of understanding the algorithmic regularization of SGD noise (in particular the label noise for regression) after reaching zero loss. The framework is novel and has the potential to extend to other settings.
generated: the the the the the the. the. the the, the.. the, the, the the and the and the the of the of the and, and the of of the the to the
original:This paper provides well-written and thorough analysis demonstrating that closed-set recognition performance correlates with open-set recognition performance, and that simply making the close-set model strong via augmentation, label smoothing, etc. along with small scoring changes (using logits rather than softmax probabilities) can get close to (or better than in some cases) performance than much more complicated methods. The authors also propose a large-scale benchmark that varies the semantic similarity across classes, allowing for a more fine-grained analysis of this problem. Overall, all of the reviewers thoughts that the paper provides very thorough validation of an insight that would be very interesting to the community. Reviewer HAF
generated: the the the the the the. the. the. the the, the, the, the the and the and the the of the and the of the of., and of the the to the
original:The paper provides a method for with tuning continuous hyperparameters (HPs). It is closely related to a previous work (Lorraine, 2019) that was limited to certain HPs, and in particular could not be applied to HPs controlling the learning such as learning rate, momentum, and are known to be influential to the convergence and overall performance (for non-convex objectives). The reviews indicate a uniform opinion that the paper tackles an important problem, that its methods provide a non-trivial improvement over previous techniques and in particular those of (Lorrain, 2019), and that the provided experiments are extensive and convincing. The initial reviews had several concerns about technical
generated: the the the the the. the the. the.. the, the the, the, the of the the and the and the the of the of the and, of and the paper the
original:This paper presents a new method for clustering multiple graphs, without vertex correspondence, by combing existing approaches on graphon estimation and spectral clustering. All reviewers agree that this is a neat paper with new theoretical and empirical results. The main concerns were also properly addressed during rebutal. Overall, it is a good paper.
generated: the the the the the. the. the the. the, the, the the,., the and the the of the the and the and the of the of and of the paper the
original:This paper tackles a problem at the intersection of AutoML and trustworthiness that has not been studied much before, and provides a first solution, leaving much space for a lot of interesting future research. All reviewers agree that this is a strong paper and clearly recommend acceptance. I recommend acceptance as an oral since the paper opens the door for a lot of interesting follow-ups.
generated: the the the the the the. the. the the, the. the, the the and the the of the, the and the of the and., and the of the the to the the a the
original:The paper propose a universal technique that enables weak supervision over any label type while still offering desirable properties, including practical flexibility, computational efficiency, and theoretical guarantees. Over the course of the rebuttal, the authors have made a substantial overhaul on writing and experimentation. The universality claims are now better supported by bounds, and experiments cover comparison to snorkel, majority vote and supervised learning, on multiple applications. The authors are encouraged to move the related work section to the main body of the paper. The authors should also clarify to what extent the contributions they make pertain to Snorkel as opposed to weak supervision more generally. This may require revisiting both the introduction as well as perhaps the
generated: the the the the the the. the. the the, the.. the,, the the of the the and the and the, the.. the., the of the and the the a the the to the
original:The manuscript develops a new kind of graph neural network (a Graph Mechanics Network; GMN) that is particularly well suited to representing and making predictions about physical mechanics systems (and data with similar structure). It does so by developing a way to build geometric constraints implicitly and naturally into the forward kinematics of the network, while still allowing for effective learning from data. The manuscript proves some essential properties of the new architecture and runs experiments both with simulated particles, hinges, sticks (and their combination), as well as with motion capture data. Reviewers were generally impressed by the writing and clarity of the work, as well as the main results. In addition, in those cases where reviewers thought that
generated: the the the the the the. the. the the, the.. the, the, the the and the the of the and the of the the to the the a the the paper the the- the the in the the is the the
original:The paper addresses two important aspects of deep learning: model transferability and authorization for use. It presents original solutions for both of these problems. All of the reviewers agree that the paper is a valuable contributions. Minor concerns and critical remarks have been addressed by the authors during the discussion.
generated: the the the the the the. the. the. the the, the, the, the the and the the of the and the of the and the of the the a the the to the the paper the the- the the in the the
original:In the end, all reviewers agreed that this is a solid piece of work. However, there were also some doubts regarding the relevance of the block diagonal design and the underlying assumptions about the p/n ratio. The majority of the reviewers, on the other hand, had the impression that the positive aspects dominate the potential problems, and I also share this viewpoint. However, I'd like to encourage the authors to carefully address the points of criticism raised by the reviewers in their final version.
generated: the the the the the the. the. the. the, the the, the.. the the and the,., the and the the of the and the of the of and of the
original:The authors provide a framework for unsupervised clarification based on minimizing a between-cluster discriminative similarity. It is more flexible than existing methods whose kernel similarity implicitly assumes uniform weights, and the authors connect to ideas such as max-margin and weighted kernel approaches. This yields a clustering algorithm naturally that alternates between updating class labels and similarity weights. Moreover the reviewers (and I) appreciate the analysis of generalization error through Rademacher complexity arguments and detailed author responses. I might add while the paper draws connections to weighted kernel methods and have since added references to sparse subspace clustering etc, there is recent interest in using similar arguments to derive error bounds and uniform concentration results
generated: the the the the the the. the. the. the the, the, the the and the the of the,., the.. the., the and the of the and the of of the
original:This paper proposes a new paradigm --- called in-sample Q learning --- to tackle offline reinforcement learning. Based on the novel idea of using expectile regression, the proposed algorithm enjoys stable performance by focusing on in-sample actions and avoiding querying the values of unseen actions. The empirical performance of the proposed algorithm is appealing, outperforming existing baselines on several tasks. The paper is also well written.
generated: the the the the the the. the. the.. the the, the, the the and the,, the and the and the the of the of the of and of the the paper the
original:This paper presents a hierarchical Bayesian approach to exploration in grid worlds. The paper considers the hypothesis that humans maintain a hierarchical representation when exploring a space, where the distribution over unknown space can be modeled with a structured probabilistic program. The paper compares the behavior of people during exploration tasks to the behavior of a Bayesian model under different distributional approximations. The results indicate that people can behave similarly to a sophisticated Bayesian model on small grid world domains. The reviews highlighted several concerns about the paper. One initial concern was that the experimental domain is too simple and small compared to real world environments encountered by robots or humans. However, this work is similar in scope to other exploration work
generated: the the the the the. the the. the. the the, the the and the, the and the the of the,., the and and the of the the to the the a the the paper the
original:This paper addresses the problem of goal navigation in unseen environments by learning to build a local, then a registered, global occupancy and semantic map of object categories from reprojected RGB+D observations, while extrapolating (hallucinating) unseen observations from contextual semantic priors (e. g., "tables are usually surrounded by chairs"). It then uses a measure of epistemic uncertainty on different estimations (realisations) of that map as a navigation goal selection policy to perform active exploration, and controls the agent using a local goal-driven policy; different information gain metrics are investigated. Essentially, the policy accumulates the predicted semantic maps and uses the uncertainty of the semantic mapping to select
generated: the the the the the the. the. the the, the. the, the the and the the of the,., the.. the., the and the of the and and the of the
original:This paper presents an interesting analysis of mixup, discussing when it works and when it fails. The theory is further illustrated with small but intuitive examples, which facilitates understanding the underlying phenomena and verifies correctness of the predictions made by the theory. The submission has received three reviews with high variance ranging from 3 to 8: mn55 favoring rejection while eGEK recommending accept. I read all the reviews and authors' response. Unfortunately, mn55 did not follow up to express how convinced they are with author's reply, but I do find the responses to mn55 very solid and convincing. In concordance with eGEK, I do find the provided analysis important and helpful
generated: the the the the the. the the. the. the, the the,., the, the and the the of the the and the of the and and the of of the paper the
original:While one reviewer remained concerned about the possibility of convergence to bad equilibria and felt that the proposed method appears to be four minor changes from prior work (PAIRED), the authors demonstrate empirically that the proposed changes make a significant difference in their evaluation. Other reviewers were positive about this work and all others rated this work as an accept. Post rebuttal the most positive reviewer increased their score to an 8 and felt did a good job answering their concerns. They wanted to see an analysis of systems with larger numbers of agents, but felt that the current manuscript was more than sufficient to warrant acceptance, and fell into the category of a good paper with the additional ablations provided during the rebuttal
generated: the the the the the the. the. the the, the. the, the the and the and the the of the,., the and the of the of of the the paper the the to the
original:This paper studies the dependency of SGD convergence on order of examples. The main observation of the paper is: if the averages of consecutive stochastic gradients converge faster to the full gradient, then the SGD with the corresponding sampling strategy will have a faster convergence rate. For different sampling strategies, sampling with replacement has slower convergence in stochastic gradient, where sampling without replacement can converge faster. The paper also proposes two new algorithms that can improve convergence rates in some interesting settings. The reviewers find the analysis clean and the new algorithms are interesting. There is some concern on the dependency on n or d for the faster rate, which should be discussed more clearly in the final version of the
generated: the the the the the the. the. the.. the the, the, the,, the the and the and the the of the and the of the of and of the the paper the
original:This paper introduces a method to determine which precision to use for the weights, as well as a quantisation method using hysteresis to improve performance with low-precision weights, including 4-bits. Reviewers tend to agree that the two points presented are useful and can have a large impact on the field. Generally, reviewers pointed out that motivations, notations and experimental studies could be improved. This has been partly addressed by the authors. I recommend to accept this paper for ICLR 2022.
generated: the the the the the. the the. the. the, the, the the,., the and the the and the of the the of the to the the to the paper the the paper the a the
original:Verifying robustness of neural networks is an important application in machine learning. The submission takes on this challenge via the interval bound propagation (IBP) framework and provides a theoretical analysis on the training procedure. They establish, in the large network with case, that the certification via IBP reflects the robustness of the neural network. Despite the tensions between the changing architecture and the required accuracy, the results are insightful. The AC recommends the authors to revise the paper, correcting the significant amounts of typos and improve the presentation for its final version.
generated: the the the the the. the the. the. the, the, the the and the the,., the and the of the the of the to the the to the paper the the paper the a the
original:The authors provide in this manuscript a theoretical analysis to explain why deep neural networks become linear in the neighbourhood of the initial optimisation point as their width tends to infinity. They approach this question by viewing the network as a multi-level assembly model. All reviewers agree that this is an interesting, novel, and relevant study. The paper is very well-written. Initially, a weak point raised by a reviewer was that an empirical evaluation of the theory was missing. The authors addressed this issue in a satisfactory manner in their response. In conclusion, this is a strong contribution worth publication.
generated: the the the the the the. the. the.. the the, the, the,, the the and the and the the of the and the of the of and of the the paper the
original:All reviewers agree that this paper is a useful and valuable contributions to ML engineering. - insightful analysis.. highly user friendly operator design - "useful and I can see it having large adoption in the community of scientific computing"... " - "Personally I tend to buy these advantages of einops"... "However, there is a lack of solid empirical study to validate the effectiveness and efficiency of the design" - "a useful and appealing new coding tool." The negative reviewers appear fixated on the (true) observation that the paper does not look like a conventional ICLR paper, thati it "reads like a technical blog", and "lacks rigour". I belive
generated: the the the the the the. the. the the, the. the, the the and the, the and the and the the of the of the of., and of the the to the
original:The paper aims at characterizing conditions for optimal representations required for the domain generalization problem under covariate shift. Under the Idealized Domain Generalization (IDG), the paper provides a variational characterization of the optimal representation and shows a number of intriguing results: (i) optimal representation should remain discriminative across domains, (ii) the representation’s marginal support needs to be the same across source and target. (ii) It is also shown that without any target information, no representation can do uniformly well over constant representation, thus supporting the necessity of target knowledge. Finally, the paper provides practical objectives of the proposed variational characterization by self-supervised learning using data-
generated: the the the the the the. the. the. the the, the, the the of the the and the the to the the a the the paper the the- the the in the the is the the that the the The the the for the the this the the
original:The paper aims to improve generalization and sample efficiency in robotic control. In this regard authors note that modular robot systems, which provide building blocks for a task-specific morphology, can be considered just another domain where transformers can be used. The authors propose to learn a universal controller over this modular design space and leverage successful “large-scale pre-training and fine-tuning” scheme for transformer. We thank the reviewers and authors for engaging in an active discussion. Reviewers found that methodological novelty is limited ("The contributions on algorithmic and network designs are marginal. Their approach is a direct application of Transformers and Reinforcement Learning approaches.") and there are very similar approaches in the
generated: the the the the the the. the. the.. the the, the, the, the the and the and the the of the and the of the of and, of the the paper the
original:Initially, some reviewers have raised several points of criticism regarding certain aspects of the model whose novelty/significance was a bit unclear. After the rebuttal and the discussion phase, however, everyone agreed that most of these concerns could be addressed in a convincing way, and finally all reviewers were in favor of this paper. After carefully going over all the reviews, the rebuttal and the discussions, I fully agree with the reviewers and came to the conclusion that this paper indeed contains some interesting, novel and relevant contributions.
generated: the the the the the the. the. the the, the.. the, the the and the and the the of the,, the and the of the of the the paper the the to the the a the
original:The paper provides new insights about how to identify latent variable distributions, making explicit assumptions about invariances. A lot of this is studied in the literature of non-linear ICA, although the emphasis here is on dropping the "I". I think more could be said about how allowing for dependencies among latents truly change the nature of the problem since any distribution can be built out of independent latents, by some more explicit contrast against the recent references given by the reviewers. In any case, the role of allowing for dependencies in the context of the invariances adopted is discussed, and despite no experimentation, the theoretical results are of general interest to the ICLR community and a worthwhile
generated: the the the the the the. the. the. the the, the, the the and the the of the the to the the paper the the a the the- the the in the the is the the that the the The the the for the the this the the
original:The authors present a GAN for learning a continuous representation of disease-related image patterns from regional volume information generated from structural MRI images. The reviewers find the problem relevant and appreciate the proposed solution. They find the paper well-written and find the empirical results on Alzheimer brain MRIs relevant for the neuroscience community. The overall objective function includes several hyper-parameters. As pointed out as the main weak point by multiple reviewers this may hint at overengineering/overfitting to a data set. However, the reviewers also mention that the regularizers are all sufficiently well-motivated in the paper and the author response. Reviewers highlight comparisons on the real data as a strong result demonstrating
generated: the the the the the the. the. the.. the the, the, the, the the and the the of the and the of the and, and the of of the the paper the
original:This paper extends the Neural Collapse (NC) phenomenon discovered by Papyan, Han and Donoho (2020) on deep learning image classifications with Cross Entropy (CE) loss, to the scenario with Mean Squared Error (MSE), that achieves similar performance to CE and favors deeper analysis. In particular, the paper shows that the least square loss can be decomposed orthogonally into a 'central' path as the optimal least square loss, and its perpendicular loss. Moreover, the paper shows by experiments that after the zero training error (Terminal Phase of Training, or TPT) the perpendicular loss is typically much smaller than the optimal least square loss, and the
generated: the the the the the the. the. the. the the, the, the, the the and the the of the and the of the and the of the the a the the paper the the to the the- the the in the the
original:In the paper, it introduces a forget-and-relearn framework to the iterative learning algorithm. It provides serval new insight that forgetting could be favorable to learning and validates the insights via image classification and language tasks. The idea is novel and inspiring. Although there are some debates on the experiment and the generality of the proposed method, I think authors answered those questions decently and many researchers would be interested in this direction.
generated: the the the the the the. the. the. the the, the, the the of the the and the the to the the a the the paper the the- the the in the the is the the that the the The the the for the the this the the
original:This paper goes beyond the NTK setting in analyzing optimization and generalization in ReLU networks. It nicely generalizes NTK by showing that generalization depends on a family of kernels rather than the single NTK. The reviewers appreciated the results. One thing that is missing is a clear separation between NTK results and the ones proposed here. Although it is ok to defer this to future work, a discussion of this point in the paper would be helpful.
generated: the the the the the the. the. the the, the.. the, the, the the and the and the the of the and the of the of the the to the the a the the paper the the- the the
original:This paper proposed MIDI-DDSP, a structured hierarchical generative model which offers both detailed expressive controls (as in traditional synthesizers) as well as the realistic audio quality (as in black-box neural audio synthesis). Overall the reviews are very positive. All the reviewers unanimously agree that the paper is very well-written and presented a very convincing model and a meaningful step-up from the earlier work of DDSP. The authors also presented a well-documented website for the project and promised to release the source code. The reviewers raised some clarifying questions and minor corrections which the authors addressed during the response. Therefore, I vote for accept.
generated: the the the the the the. the. the the, the.. the, the the of the the and the,, the of the and the of the and the the a the the paper the the to the
original:The paper modifies DPMs by replacing the denoising L2 losses with GANs to learn the iterative denoising process. This leads to excellent results using a small number of refinement steps. In some sense, this also takes away one of the key advantages of DPMs over GANs, which is DPMs minimize a well-defined objective function. Nevertheless, the results are convincing, but not spectacular. I am not convinced that we should continue to report training FID on CIFAR-10. I would have like to see class-conditional ImageNet results. Also, it is not clear whether the proposed technique provides additional gains on top
generated: the the the the the. the the. the the, the.. the, the the and the, the and the the of the and the of the of and, of the the to the
original:This paper builds on the success of the FermiNet neural wave function framework by pairing it with a graph neural network which predicts the parameters of neural wave function from the geometry. The resulting PESNet trains significantly faster, with no loss of accuracy. This method constitutes an important advance in ML-powered quantum mechanical calculations. The reviewers unanimously recommend acceptance.
generated: the the the the the the. the. the. the the, the, the, the the and the the of the and the of the and the of the the paper the the a the the to the the- the the in the the
original:The paper presents a novel method of fusion of information from two modalities: text (context and question) and a Knowledge Base, for the task of question answering. The proposed method looks quite simple and clear, while the results show strong gains against baseline methods on 3 different datasets. Ablation studies show that the model achieves good performance on more complex questions. While the reviewers raise some concerns, e. g., on the sensitivity of the proposed method, the technical novelty against prior works, they see values in this paper in general. And the authors did a good job in their rebuttal. After several rounds of interactions, some reviewers were convinced to raise their scores by a little bit. As
generated: the the the the the the. the. the the, the.. the, the, the the and the and the the of the and the of the of, and of the the to the
original:This work presents an approach to learning good representations for few-shot learning when supervision is provided at the super-class level and is otherwise missing at the sub-class level. After some discussion with the authors, all reviewers are supportive of this work being accepted. Two reviewers were even supportive of this work being presented at least as a spotlight. The approach presented is well motivated, experiments demonstrate its value and include a nice application in the medical domain, making the work stand out relatively to most work in few-shot classification. Therefore, I'm happy to recommend this work be accepted and receive a spotlight presentation.
generated: the the the the the the. the. the the, the. the, the, the the and the the of the and the and., and the of the the to the the a the the paper the
original:This is a borderline paper. The scores were initially below the bar. The novelty of the work is limited and there are strong claims in the paper that should be revised. The authors can also do a better job in positioning their work with respect to the existing results. However, the authors managed to address several questions/concerns of the reviewers and convince them to raise their scores. I would strongly recommend the authors to address the rest of the reviewers' comments, especially those related to strong claims and connection to related work, and further improve their work in preparing its final draft.
generated: the the the the the. the the. the. the, the the and the the of the the, the, the and the and... the.., and the of of the
original:This paper presents a conditional variational autoencoder (CVAE) approach to solve an instance of stochastic integer program (SIP) using graph convolutional networks. Experiments show that their method achieves high quality solutions with high performance. It holds merit as an interesting novel application of CVAEs to the ML for combinatorial optimization literature, as well as for the nice empirical results which show a very nice improvement. Two reviewers had a concern that the contribution is a bit narrowly focused toward MILP-focused journal rather than a general-purpose ML conference since the core contribution is the novel application. On the other hand, they believe that combinatorial optimization has
generated: the the the the the the. the. the.. the the, the, the,, the the and the and the the of the and the of the of and of the the paper the
original:The paper investigates the use of flow models for out-of-distribution detection. The paper proposes to use a combination of random projections in the latent space of flow models and one-sample / two-sample statistical tests for detecting OOD inputs. The authors present results on image benchmarks as well as non-image benchmarks. The reviewers found the approach well-motivated and appreciated the ablations. The authors did a good job of addressing reviewer concerns during the rebuttal. During the discussion phase, the consensus decision leaned towards acceptance. I recommend accept and encourage the reviewers to address any remaining concerns in the final version. It might be worth discussing this paper in the related work: Density
generated: the the the the the the. the. the. the the, the, the, the the of the the and the and the and,. and the the to the the paper the the a the the- the
original:This paper proposes a feature selection method to identify features for downstream supervised tasks, focused on addressing challenges with sample scarcity and feature correlations. The proposed approach is highly motivating in biological and medical applications. Reviewers pointed out various strengths including potential high impacts in biomedical applications, technical novelty and significance, and comprehensive and illustrative experiments. The authors adequately addressed major concerns raised by reviewers.
generated: the the the the the the. the. the the, the.. the, the and the the of the, the the and the of the.. the., the and the of the the a the the to the
original:The paper proposes a GAN framework for dynamic point cloud superresolution. It does not need scene flow supervision for training and has an interesting adaptive upsampling mechanism. Results are shown on several datasets and are reasonably convincing. Overall, all the reviewers are slightly positive about the work. After the rebuttal, all the five reviewers converged to a marginally-above-the-threshold recommendation. The meta-reviewer agreed with their assessment and would like to recommend accepting the paper.
generated: the the the the the the. the. the the, the. the, the, the the and the the of the and the of the the to the the paper the the a the the- the the in the the is the the that the
original:The paper was praised for being clearly written, well-motivated, and for addressing an important problem: measuring intrinsic robustness. It improves the previous results on intrinsic robustness based on concentration of data distribution, by incorporating the constraint on the label uncertainty of the models. This requires information on label uncertainty for each data sample rarely available (here CIFAR-10H is considered), but could open new directions for future work on adversarial robustness, confidence calibration or label noises.
generated: the the the the the the. the. the. the the, the, the the and the, the and the the of the and the of the of,. and of the the to the
original:The paper studies multi label classification problem. Particularly, they introduce multi-label box model, which uses probabilistic semantics of box embeddings, representing labels as boxes instead of vectors. Their model is evaluated extensively on 12 datasets, and reviewers agreed the paper was well written and well motivated. While it is pretty straightforward application of box embeddings to multi label problem, it is well motivated and the paper adds to the existing literature on box embeddings. Reviewer Eo7g had a concern with experimental setting, including missing a baseline Abboud et al. (2020). Even after the baseline was added, the reviewer was not convinced about the model’s performance
generated: the the the the the the. the. the the, the.. the, the the and the, the and the the of the of the and, and the of of the the to the
original:The authors propose a memory-based continual learning method that decomposes the models' parameters and that shares a large number of the decomposed parameters across tasks. In other words, only a small number of parameters are task-specific and the memory usage of storing models from previous tasks is hence a fraction of the memory usage of previous approaches. The authors take advantage of their method to propose specific ensembling approaches and demonstrate the strong performance of their methods using several datasets. In the rebuttal, the authors were very reactive and provided many useful additional results during including a comparison of the computational cost of their method vs. others, results using two new datasets (CUBS & Flowers), and additional results
generated: the the the the the the. the. the the, the.. the, the, the the and the the of the and the and, and the of the.... the., the
original:This paper introduces a new technique for discovering closed-form functional forms (ordinary differential equations) that explain noisy observed trajectories x(t) where the "label" x'(t) = f(x(t), t) is not observed, but without trying to approximate it. The method first tries to approximate a smoother trajector x^hat(t), then relies on a variational formulation using a loss function over functionals {C_j}_j, defined in terms of an orthonormal basis {g_1, …, g_S} of sampling functions such that the sum of squares of all the C_j approximates the theoretical distance between f
generated: the the the the the. the the. the the, the.. the, the the and the, the and the the of the and the of the the to the the paper the the a the the- the the in the
original:This work starts from the observation that maximum likelihood estimation, while consistent, has a bias on a finite sample which is likely to hurt for small sample sizes. From this, they apply Firth bias reduction to the few-shot learning setting and demonstrate its empirical benefits, notably relatively to L2 regularization or label smoothing alternatives. After some discussion with the authors, all reviewers are supportive of this work being accepted. Two are also suggesting this work be featured as a spotlight. The proposed method is simple, well motivated, and appears to be effective. Therefore, I'm happy to recommend this work be accepted and receive a spotlight presentation.
generated: the the the the the. the the. the the, the.. the, the the and the, the and the and the the of the of the of and, of the the to the
original:This paper proposes use of a novel generative modelling approach, over both sequences and structure of proteins, to co-design the CDR region of antibodies so achieve good binding/neutralization. The reviewers are in agreement that the problem is one of importance, and that the technical and empirical contributions are strong. There are concerns over the relevance of evaluating the method by using a predictive model as ground truth. Still, the overall contributions remain.
