generated:This paper proposes a novel method to solve the problem of detecting the neural network architecture in graph models. The authors propose a method to learn a deep learning model based on a neural network-based adversarial attack. The paper is well-written, well-motivated, and the empirical results
original:All reviewers agree on acceptance and I agree with them. I recommend a spotlight.
generated:This paper proposes a novel method to solve the problem of generalization in neural network learning. The authors propose a method that uses a large number of non-supervised learning tasks, which can be used to improve the performance of deep learning models. The paper is well-written and well
original:In this paper, a new learning scheme for minimizing the confidence set by conformal prediction is proposed. Most of the reviewers agree that the idea is interesting and novel. This is an important contribution to trustworthy ML, with theoretically sound considerations and thorough experimental validation.
generated:This paper proposes a novel approach to address the problem of generalization in graph neural network models. The authors propose a method for generative neural network (GNN) architecture that is relatively simple, efficient, and outperforms existing methods. The reviewers agree that the paper is well-written
original:This paper proposes an efficient method for message passing that can incorporate structural information that is provably stronger than 1-WL. As compared to three strands of provably powerful (more than 1 WL) GNNs, the method has limited additional computational overhead, and can also show encouraging results on the over smoothing problem. Overall speaking, all the reviewers like this paper quite a lot, although the also raised some minor concerns. The paper also attracted some unofficial reviewers who provided quite a few related works. The authors did a good job in interacting with the reviewers and addressing their minor concerns. So, we believe the paper is worth accepting, and could be a significant work in the field
generated:This paper proposes a method for solving the problem of detecting and solving problems in RL. The paper is well-written, well-motivated, and well-developed. The reviewers were all positive about the paper. The authors did a good job of addressing the concerns raised by the reviewers,
original:The paper gives a method for generating contrastive explanations, in terms of user-specified concepts, for an agent in a sequential decision making setting. The reviewers found the paper to be a strong contribution to explainable AI and RL. There were some concerns about the writing, but the revisions have addressed most of these. Overall, I am delighted to recommend acceptance. I urge the authors to incorporate the feedback in the reviews in the final version.
generated:This paper proposes a method for learning a neural network based on a gradient-based neural network (SLR) based on the neural network. The authors propose a method to learn the representation of a classifier. The proposed method is evaluated on a large number of tasks, which can be
original:The paper proposes a new recurrent architecture based on discretization of ODEs which allow for learning multi-scale representations and help with the vanishing gradient problem. The reviewers all agree this architecture is novel and provide substantial theoretical and empirical evidence. A strong accept.
generated:This paper proposes a method for learning a supervised learning network that learns a classifier-based reinforcement learning network to learn a latent latent reward function. The authors propose to use the learned network to solve the problem of optimizing the optimization of deep learning. The reviewers agree that the paper is
original:The paper presents improvements to AlphaZero and MuZero for settings where one is restricted in the number of rollouts. The initial response from reviewers was generally favorable but the reviewers wanted more details and clarifications of multiple parts of the paper, and further intuition about the Gumbel distribution. The authors’ responses were detailed and convinced or maintained strong positive support of most reviewers. The authors also stated that they plan to provide a release of the code and also provided a policy improvement proof. Overall this is an interesting approach that is likely to be of significant interest to many.
generated:This paper proposes a method to solve the problem of non-constrained RL. The authors propose a method that uses a stochastic approach to solve this problem.
original:All reviewers have converged to an unanimous rating of the paper, highlighting, in the paper or during the discussion, many strengths, including a compelling approach clearly relevant to applications and its solid range of experiments. A clear accept, and I would encourage the authors to push in the final version the experiments and discussions following the threads with reviewers (in particular, Vo8C and ULvk). Thanks also to authors and reviewers for a thorough discussion which helped to strengthen further the paper's content. AC.
generated:This paper proposes a simple method to solve the problem of fine-tuning the performance of multi-task re-modification. The authors propose to solve this problem by using a new method of re-supervised transition. The method is evaluated on a large number of tasks. The
original:This paper is about unsupervised translation between programming languages. The main positive is that it introduces the idea of using a form of unit test generation and execution behavior within a programming language back-translation setup, and it puts together together a number of pieces in an interesting way: text-to-text transformers, unit test generation, execution and code coverage. Results show a substantial improvement. The main weaknesses are that there are some caveats that need to be made, such as the (heuristic, not learned) way that test cases are translated across languages is not fully general, and that limits the applicability. There are also some cases where I find that the authors are stretching claims a
generated:This paper proposes a novel method to solve the problem of detecting the uncertainty gap between the learned model and the state-space (MWR) in RL.
original:This paper empirically studies various design choices in offline model-based RL algorithms, with a focus on MOPO (Model-based Offline Policy Optimization). Among the key design choices is the uncertainty measure used in MOPO that provides an (approximate) lower bound on the performance, the horizon rollout length, and the number of model used in ensemble. The reviewers are positive about the paper, found the experiments thorough, and the results filling a gap in the current literature. They have raised several issues in their reviews, many of which are addressed in the rebuttal and the revised paper. I would like to recommend acceptance of the paper. Also since the results of this work
generated:This paper proposes a novel approach to the problem of unsupervised reinforcement learning. The authors propose a novel method for learning a new task. The paper is well-written, well-motivated, and the empirical results are convincing. The reviewers agree that the paper is interesting, and
original:Exploration can happen at various levels of granularity and at different times during an episode, and this work performs a study of the problem of exploration (when to explore/when to switch between exploring and exploitation, at what time-scale to do so, and what signals would be good triggers to switch). The study is performed on atari games. Strenghts: ------------ The study is well motivated and the manuscript is overall well written Studies a new problem area, and proposes an initial novel method for this problem extensive study on atari problems Weaknesses -------------- some clarity issues as pointed out by the reviewers no illustrative task is given to give a more intuitive exposition of the "
generated:This paper proposes a novel method for learning in deep learning. The reviewers agreed that the paper is well-written, well-motivated, and well motivated. The authors did a good job in the discussion period and the reviewers have addressed some of the concerns raised by the reviewers. The
original:This paper addresses the problem of goal navigation in unseen environments by learning to build a local, then a registered, global occupancy and semantic map of object categories from reprojected RGB+D observations, while extrapolating (hallucinating) unseen observations from contextual semantic priors (e. g., "tables are usually surrounded by chairs"). It then uses a measure of epistemic uncertainty on different estimations (realisations) of that map as a navigation goal selection policy to perform active exploration, and controls the agent using a local goal-driven policy; different information gain metrics are investigated. Essentially, the policy accumulates the predicted semantic maps and uses the uncertainty of the semantic mapping to select
generated:This paper proposes a novel method to solve the problem of learning in deep neural networks. The method is based on a graph-based neural network (GAN) based on deep neural network models, which can be used to learn the latent representations of a classifier. The proposed method is
original:The paper provides an interesting analysis of aligned GAN models. The paper shows that when a model is obtained (fine-tuned) from another, then the corresponding hidden semantic spaces are aligned. The paper uses this property to show that without any additional architecture or training, the models can perform diverse tasks such as image translation and morphing. The paper also demonstrates that zero-shot tasks can be performed by learning in the parent domain and transferring to the child domain. All reviewers agree that the paper presents an interesting analysis and findings and will make a valuable contribution to the field. The reviewers raised some particular concerns, which were addressed by the authors in their response.
generated:This paper proposes a novel method of solving the problem of exploiting weak bounds on the uncertainty of the value of the uncertainty. The paper is well written and well-written, and is well-motivated. The reviewers were all positive about the contributions of the authors, and all the reviewers
original:This paper tackles a problem at the intersection of AutoML and trustworthiness that has not been studied much before, and provides a first solution, leaving much space for a lot of interesting future research. All reviewers agree that this is a strong paper and clearly recommend acceptance. I recommend acceptance as an oral since the paper opens the door for a lot of interesting follow-ups.
generated:This paper proposes a novel method to solve an important problem in deep learning. The reviewers agree that the paper is well-written and well-motivated, and the empirical results show that the proposed method outperforms existing methods. In particular, the reviewers agree with the reviewers that the reviewers
original:This paper presented a domain transportation perspective on optimizing recommender systems. The basic motivation is to view recommendation as applying some form of intervention, implying a distributional shift after the recommendation/intervention. Distribution shift brings tremendous difficulty to traditional causal inference or missing data theory perspective of recommender systems as it violates the distributional overlapping assumption: in simple terms, if the model recommends radically different set of items, there isn't much you can say about its generalization ability; on the other hand, if the model only recommends items that it already observed during training (no distribution shift at all), it would inherent all the biases which already exist in the data. To that end, this paper proposed a
generated:This paper proposes a novel method to solve the problem of reducing the complexity of deep learning in deep learning. The authors propose a method to improve the efficiency of learning in RL. The paper is well-written, well-motivated, and well-developed. The reviewers agree that the
original:The paper shows that most variance of gradients used in FL and distributed learning in general is in very low rank subspaces, an observation also made in Konecny et al 2016 and some other related works in deep learning, though sometimes for a different purpose. The paper then proposes lightweight updates combining a fresh gradient with old updates. Experiments and a theoretical convergence guarantee complement the results, which are mostly convincing. The experiments compare against ATOMO but strangely not against the more common PowerSGD, which would also work with partial client participation. Overall, reviewers all agreed that the paper is interesting, well-motivated and deserves acceptance. We hope the authors will incorporate the open
generated:This paper proposes a method for learning a neural network to learn a large number of representations of a set of data. The authors show that the proposed method achieves good performance in the large-scale benchmark. The paper is well-written, well-motivated, and well motivated. The
original:All the reviewers liked the paper. The proposed method contains novel ideas of learning feature representation to maixmize the mutral informatio nbetween the latent code and its corresponding observation for fine-grained class clustering. The model seems to successfully avoid mode collapse while training generators and able to generate various object (foregrounds) with varying backgrounds. The foreground and background control ability is an outstanding feature of the paper. Please incorporate the comments of the reviewers in the final version. BTW, the real score of this paper should be 7.0 as Reviewer 5wFE commented that he/she would raise the score from 5 to 6 but at the time of this meta review,
generated:This paper proposes a new framework for solving the problem of unsupervised reinforcement learning. The paper is well-written, well-motivated, and well motivated.
original:This is a deep theoretical paper with results that I consider very interesting. I have *not* had time to check them myself, but I have background in these theoretical matters and the results seem reasonable to me - the hardness of even checking the quality of a solution is well known for partition functions (as well as hardness of any reasonable approximation), but the undecidability seems new - I assume it comes naturally and it is a very interesting result - I have seen similar decidability issues for #P: general probabilistic polynomial time Turing machines (it is unclear if a connection was sought here). Reviewers are all positive about the content, and authors have acknowledge some points
generated:This paper proposes a simple method to solve the problem of generating deep neural networks. The method is based on a neural network-based neural network architecture. The authors propose a method that uses a gradient-based variational neural network. The idea is novel, and the empirical results are interesting
original:This paper introduces Autoregressive Diffusion Models (ARDMs), which generalises order-agnostic autoregressive models and absorbing discrete diffusion. All reviewers appreciated the paper with a few also finding it very dense. The experimental section is a bit lacking in detail. This has to some degree been answered in the discussion and should also be included in the final version of the paper. Acceptance is recommended.
generated:This paper proposes a novel method for learning in deep learning. The reviewers agree with the reviewers that the paper is well-written and well-motivated, and the reviewers have raised some concerns about the novelty of the method. The authors have addressed the concerns raised by the reviewers in the
original:The authors propose a memory-based continual learning method that decomposes the models' parameters and that shares a large number of the decomposed parameters across tasks. In other words, only a small number of parameters are task-specific and the memory usage of storing models from previous tasks is hence a fraction of the memory usage of previous approaches. The authors take advantage of their method to propose specific ensembling approaches and demonstrate the strong performance of their methods using several datasets. In the rebuttal, the authors were very reactive and provided many useful additional results during including a comparison of the computational cost of their method vs. others, results using two new datasets (CUBS & Flowers), and additional results
generated:This paper proposes a method for detecting adversarial adversarial robustness in neural network learning. The authors propose a method to solve the adversarial weakness problem. The paper is well-written and well-motivated, and the empirical results show that the proposed method is novel, and
original:The paper formalizes the adversarial attack problem for transductive defenses, where the model is sequentially updated with a batch of (adversarial) test inputs. The paper comes up with a quite generic attack scheme and their instantiation of this scheme shows that RMC and DENT are not robust respectively not more robust than the underlying adversarially robust base model. Positive - formal treatment of attacks on transductive defenses including discussion about different types of attacker knowledge - the attack model is quite generic and could work for future transductive defenses and thus is a useful baseline attack which could be suggested to be used by future transductive defenses for robustness evaluation. In particular,
generated:This paper proposes a method for learning learning models. The method is based on a neural network based on deep neural networks, which can be used to learn a task. The authors show that the proposed method outperforms previous methods. The paper is well-written and well-motivated. The reviewers
original:The authors propose a rank coding scheme for recurrent neural networks (RNNs) - inspired by spiking neural networks - in order to improve inference times at the classification of sequential data. The basic idea is to train the RNN to classify the sequence early - even before the full sequence has been observed. They also introduce a regularisation term that allows for a speed-accuracy trade-off. The method is tested on two toy-tasks as well as on temporal MNIST and Google Speech Commands. The results are very good, typically improving inference time with very little loss in accuracy. Furthermore, the idea seems novel and the paper is well written. An initial criticism was that experiments
generated:This paper proposes a novel method for detecting the causalness of visual representations in visual images. The reviewers agreed that the paper is well-written and well-motivated, and the reviewers were in agreement with the reviewers' comments. The authors did a good job of addressing the concerns raised
original:The authors propose a normalization method for cross-lingual text representations. The goal is to normalize the monolingual embeddings based on spectral normalization. The study shows that produced text representations keep their meaning and improve performance on downstream tasks. There is a disagreement among the reviewers. The main concern is whether the main contribution is an empirical study or a novel idea. I think the authors well-addressed the concerns of most reviewers. The idea and empirical study are enough for publication for ICLR-2022.
generated:This paper proposes a novel method for learning visual representations of images in image-based language. The authors propose a method to learn the representation of images from visual images. The paper is well-written, well-motivated, and the empirical results are convincing. The reviewers agree that the
original:This work proposes a new framework that can learn the object-centric representation for video. The authors did a good job during rebuttal and turned one slightly negative reviewer into positive ones. The final scores are 6,6,8,8. AC agrees that this work is very interesting and deserves to be published on ICLR. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are also encouraged to make other necessary changes.
generated:This paper proposes a novel method of learning. The reviewers agreed that the paper is well-written and well-motivated, and the empirical results are interesting.
original:The work presented in this study gives a theoretical finite-sample generalisation performance of stochastic gradient descent on linear models, for different batch-sizes and feature structures. This approach enable the authors to predict the training and test losses of neural networks on real data. While there were some parts that were initially mis-understood by some reviewers in the initial version of the papers, the extensive discussions between the authors and the reviewers led to several updates, both in the reference to prior work, but also in the presentation clarity. The wide impact and relevance to ICLR of this type of contribution made us recommend this work for acceptance at ICLR.
generated:This paper proposes an approach to solve the problem of stochastic reinforcement learning. The paper is well-written, well-motivated, and the reviewers have addressed the concerns raised by the reviewers, and provides a strong contribution to the community. The reviewers have raised some concerns about the
original:The paper shows interesting and discussion inspiring results on multi-agent trajectory prediction, as needed, for instance, in autonomous driving. Among the key technical ideas is a “conditional scene transformer” approach for flexible predictions for different agents. Results on two public benchmarks are impressive. Some reviewers are a bit torn about the significance of the technical contributions and the analyses of the results. Nevertheless, on average, the reviewers vote the paper to be above the acceptance threshold.
generated:This paper proposes a new framework for learning learning in deep neural network learning. The authors propose a new method for learning to predict the output of deep neural networks. The paper is well-written, well-motivated, and well motivated. The reviewers agree that the paper is interesting and
original:The paper investigates various approaches, and a unifying framework, for sequence design. There were a variety of opinions about the paper. It was felt, after discussion, that the paper would benefit from a sharper focus, and somewhat suffers from being overwhelmed by various approaches, lacking a clear narrative. But overall all reviewers had a positive sentiment, and the paper makes a nice contribution to the growing body of work on protein design.
generated:This paper proposes a method for learning a deep neural network that learns a neural network (GCNN) based on a non-supervised learning method. The paper is well-written, well-motivated, and the empirical results are convincing. The reviewers agree that the paper is a
original:The manuscript develops a new kind of graph neural network (a Graph Mechanics Network; GMN) that is particularly well suited to representing and making predictions about physical mechanics systems (and data with similar structure). It does so by developing a way to build geometric constraints implicitly and naturally into the forward kinematics of the network, while still allowing for effective learning from data. The manuscript proves some essential properties of the new architecture and runs experiments both with simulated particles, hinges, sticks (and their combination), as well as with motion capture data. Reviewers were generally impressed by the writing and clarity of the work, as well as the main results. In addition, in those cases where reviewers thought that
generated:This paper proposes a method to improve the robustness of deep learning models. The authors propose a novel method to solve the problem of non-supervised learning. The method is novel and novel, and the empirical results are interesting. The paper is well-written and well-motivated
original:This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets. The strengths of the paper are as follows: + In terms of significance of the topic, the problem tackled in the paper is significant and highly relevant. + The motivation of using model ensemble is clearly illustrated via a figure and well justified with theoretical analysis. + Algorithmically, the paper proposes Adaptive Sampling and K
generated:This paper proposes a novel method to solve the problem of solving adversarial attacks. The authors propose a method that uses a gradient-based neural network to learn the loss function of the adversarial model. The paper is well-written, well-motivated, and the empirical results are strong
original:The paper shows that the transfer attack is query efficient and the success rate can be kept high with the zeroth-order score-based attack as a backup. Experiments show state-of-the-art results. Pros: - Simple method based on a simple idea. - State of the art performance. Cons: - Proposal is a straightforward combination of two methods, and therefore technical contribution is marginal. - The threat model is easy (surrogate can be trained on the same datasets and use the same loss function) and questionable. Most of the experimental evidence shows that the research for this threat model is almost saturated (and the problem seems almost solved). This paper got a
generated:This paper proposes a novel approach to the problem of generalization in deep learning. The authors propose a new method for detecting the weakness of deep neural network models that can be used to solve the problem. The paper is well written, well-motivated and well-written, and
original:The paper provides a thorough study of the evolution of Hessian depending on a wide variety of aspects such as initialization, architectural choices, and common training heuristics. The paper makes a number of interesting observations. Some of them are not really new but overall, the experimental evaluation of the paper makes it a valuable resource for the community. The reviewers are overall quite positive. One reviewer notes that more investigation of the behavior of batch-normalization is required. I encourage the author to address this concern in the final manuscript. There is a lot of recent work on batch-normalization that might be worth discussing, e. g.: Training BatchNorm and Only BatchNorm: On the
generated:This paper proposes a method to improve the performance of deep neural network models. The authors show that the paper is well-motivated and well-written.
original:The paper considers the setting of bi-level optimization and proposes a quasi-Newton scheme to reduce the cost of Jacobian inversion, which is the main bottleneck of bi-level optimization methods. The paper proves that the proposed scheme correctly estimates the true implicit gradient. The theoretical results are supported by numerical experiments, which are encouraging and show that the proposed method is either competitive with or outperforms the Jacobian Free method recently proposed in the literature. Even though the reviews expressed some initial concerns regarding the empirical performance of the proposed method, the authors adequately addressed those concerns and provided additional experiments. Thus, a consensus was reached that the paper should be accepted.
generated:This paper proposes a novel method to solve the problem of re-supervised reinforcement learning. The authors propose a method that achieves good results by using a new method that uses a variational method.
original:The paper proposes a new pseudometric, DARD, for comparing reward functions that avoid policy optimization. DARD builds on a recent work by Gleave et al. 2020 where the pseudometric EPIC was proposed. In contrast to EPIC, DARD operates on an approximate transition model and evaluates reward functions only on transitions close to their training distribution. Empirical experiments in different domains demonstrate the effectiveness of the proposed pseudometric. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers' questions and increased their overall assessment of the paper. At the end of the discussion phase
generated:This paper proposes an approach to optimize deep learning. The authors propose a method to solve the problem of learning in graph neural network. The paper is well-written and well-motivated, and the empirical results are convincing. The reviewers have raised some concerns about the novelty of the
original:The paper considers the setting of distributed optimization and proposes an adaptive gradient averaging and compression scheme to reduce the communication cost. The proposed scheme is shown to achieve the same convergence rate as full-gradient AMSGrad algorithm, but due to the reduced cost, it exhibits linear speedup as the number of workers grows. The reviews appreciated the clear presentation of the results, technical soundness, and convincing numerical experiments. The paper is a solid contribution to distributed optimization. Thus, I recommend acceptance.
generated:This paper proposes a novel method of learning neural network models that learns the adversarial loss functions. The authors propose a method for learning the adversarial loss function. The reviewers agree that the paper is well-written and well-motivated, and the empirical results are strong.
original:The paper presents an approach to learn the surrogate loss for complex prediction tasks where the task loss is non-differentiable and non-decomposable. The novelty of the approach is to rely on differentiable sorting, optimizing the spearman correlation between the true loss and the surrogate. This leads to a pipeline that is simpler to integrate to existing works than approaches that try to learn a differentiable approximation to the task loss, and to better experimental results. The paper is well written and the approach clearly presented. The reviewers liked the simplicity of the approach and the promising experimental results on a variety of challenging tasks (human pose estimation and machine reading).
generated:This paper proposes a method to solve the problem of detecting adversarial adversarial robustness in graph models.
original:In this work, authors use proxy distributions learned by advanced generative models to improve adversarial robustness. In the discussion period, authors did a good job in addressing reviewers' questions and comments. All reviewers think the paper is above the accept threshold, so do I.
generated:This paper proposes a new method to learn a method to solve the problem of re-supervised reinforcement learning. The authors propose a novel method for learning a set of tasks. The reviewers have addressed the concerns raised by the reviewers in the rebuttal to the paper.
original:The paper proposes a new method for unsupervised text style transfer by assuming there exist some pseudo-parallal sentences pairs in the data. The method thus first mines and constructs a synthetic parallel corpus with certain similarity metrics, and then trains the model via imitation learning. Reviewers have found the method is sound and the empiricial results are decent. The assumption on pseudo-parallal pairs would limited the application of the methods in other settings where the source/target text distributions are very different. The authors have added discussion on this limitation during rebuttal.
generated:This paper proposes a method to solve the problem of unsupervised learning in deep learning. The authors propose a method that uses a neural network-based method of learning.
original:The authors propose a new framework of population learning that optimizes a single conditional model to learn and represent multiple diverse policies in real-world games. All reviewers agree the ideas are interesting and the empirical results are strong. The meta reviewer agrees and recommends acceptance.
generated:This paper proposes a novel method to solve the problem of unsupervised reinforcement learning in deep neural networks. The authors propose a method to build a deep neural network with a large number of convolutional networks, which can be used to learn. The paper is well-motivated and
original:The submission introduces an algorithm for structured pruning of fully connected ReLU layers using ideas from tropical geometry. The paper begins with a very accessible overview of key concepts from tropical geometry, and shows how ReLU networks can be thought of as tropical polynomials. It gives an efficient K-means-based algorithm for pruning units in a way that approximately minimizes the Hausdorff distance between certain polytopes. Experiments show that the method outperforms other methods based on tropical geometry and is competitive with SOTA methods from a few years ago. I think the reviewers, authors and I all agree on the following points: tropical geometry is a mathematical topic not commonly used
generated:This paper proposes a method to solve the problem of generalization in deep learning. The authors propose a method that uses a neural network based on a non-convex representation of a classifier to learn the latent distribution of a graph. The method is evaluated on a large set of
original:All reviewers agreed that this is a strong paper, that the methodological contributions are both relevant and significant, and that the experimental validation is convincing. I fully share this viewpoint!
generated:This paper proposes a novel method of learning learning models. The authors propose a method for learning models that can be trained on a large number of tasks. The method is evaluated on a set of different settings, which can be used to solve the problem of generalized learning. The reviewers were
original:This paper proposes a federated learning (FL) scheme that is suitable for clients/devices with heterogeneous resources. The scheme Split-Mix trains multiple models of different sizes and adversarial-robustness levels, which are tailored to the budgets of the individual device. Empirical results show encouraging results. It is clear that FL will have to work with clients with diverse resources, a point that is appreciated. Indeed, it is anticipated that widely-dispersed inference will have to deal with a highly-heterogeneous mix of clients. The study is quite thorough. One aspect that is not convincing in the experiments is the budgets being exponentially distributed: having a strong concentration around a mean
generated:This paper proposes a novel method for detecting and estimating the accuracy of deep convolutional neural network models. The authors propose a method that uses a classifier to select a class of GANs that can be used to predict the state of the classifier. The main idea is to
original:The paper proposes and studies a method for the responsible disclosure of a fingerprint along with samples generated by a generative model, which has important applications in identifying "deep fakes". The authors establish both the detectability of their fingerprint-without significant loss of fidelity-as well as the robustness to perturbations. The reviewers found the problem and contributions to be important and significant, well substantiated by an extensive experimental study.
generated:This paper proposes a novel method for learning adversarial reinforcement learning. The method is novel, and is well-motivated, and provides a new perspective on the domain-based learning problem. The authors have addressed the concerns raised by the reviewers in the rebuttal. The paper is well
original:This paper proposes asymmetry learning for learning counterfactual classifiers, i. e. classifiers which are invariant to certain symmetry transformations w. r. t. hidden variables that differ between the training and test sets. The reviewers universally agreed that the proposed setting, and theoretical contribution, were interesting and novel. They also praised the writing quality, but had some quibbles about the quality of the experiments, and discussion of prior work. Neither of these concerns were considered significant enough to be a barrier to acceptance, but the authors should try to improve them, if possible.
generated:This paper proposes a novel approach to the problem of generalization in deep learning. The reviewers agree that the paper is well-written and well-motivated, and the empirical results are interesting.
original:This paper proposes to use LOO to characterize the generalization error of neural networks via the connection between NN and kernel learning. The reviewers find the new results interesting. The meta reviewer agrees and thus recommend acceptance.
generated:This paper proposes a novel method to solve the problem of non-linear reinforcement learning. The reviewers agree that the paper is well-written and well-motivated.
original:All reviewers are very positive about this paper. The reviewer with the lowest score did independent experiments that show that the authors' method works well, and has had an extensive discussion with the authors that justifies a higher score. The paper is potentially very valuable to practitioners, since it shows how to compensate for a training set that is not representative of the test data. Suggestion from the area chair to the authors: Briefly discuss the relationship between influence scores and propensity scores, which are standard in the literature on causal modeling and on sample selection bias, as in https://jmlr. csail. mit. edu/papers/volume10/bickel09a/bickel
generated:This paper proposes a novel method for learning neural networks. The reviewers agree that the paper is well-motivated and well-written, and that the reviewers are in agreement with the reviewers' recommendation to accept this paper.
original:The paper analyzes the learning behavior of deep networks inside RL algorithms, and proposes an interesting hypothesis: that many of the observed difficulties in deep RL methods stem from _capacity loss_ of the trained network (that is, the network loses the ability to adapt quickly to fit new functions). As the paper points out, some of these difficulties have popularly been attributed to other causes (such as difficulties in exploration) or to less-specific causes (such as reward sparsity: the paper proposes that capacity loss mediates observed problems due to sparsity). The paper investigates its hypothesis two ways: first by attempting to measure how capacity varies over time during training of existing deep RL methods, and second
generated:This paper proposes a method to solve the problem of generating graph-based graph models. The authors propose a method that uses a neural network-based neural network (SLR) based approach to generate graph functions. The method is evaluated on multiple benchmarks. The paper is well written, well
original:This paper proposes a spanning tree-based graph generation framework for molecular graph generation, which is an interesting problem. The tree-based approach is efficient and relatively effective in molecular graph generation tasks, and the empirical results are convincing. There were some concerns during the initial reviews, but all of them have been addressed during the discussion phase. Thus, I recommend this work be accepted.
generated:This paper proposes a novel method for solving the problem of detecting the weakness of stochastic descent. The reviewers agreed that the paper is well-written and well-motivated. The authors did a good job of addressing the concerns raised by the reviewers in the rebuttal. The
original:This manuscript introduces a theoretical framework to analyze the sim2real transfer gap of policies learned via domain randomization algorithms. This work focusses on understanding the success of existing domain randomization algorithms through providing a theoretical analysis. The theoretical sim2real gap analysis requires two critical components: *uniform sampling* and *use of memory* **Strengths** All reviewers agree that this manuscript provides a strong theoretical analysis for an important problem (understanding sim2real gap) well written manuscript, and well motivated Intuitive understanding for theoretical analysis is provided **Weaknesses** analysis is limited to sim2real transfer without fine-tuning in the real world the manuscript doesn't provide a novel
generated:This paper proposes a new framework for the convexization of GNNs. The authors propose a method to solve the problem of detecting the adversarial uncertainty between the gradient function (GCD) and the latent loss of the neural network. The paper is well-written and well-
original:All the reviewers agree that this paper made a solid contribution of understanding the algorithmic regularization of SGD noise (in particular the label noise for regression) after reaching zero loss. The framework is novel and has the potential to extend to other settings.
generated:This paper proposes a method to solve the problem of detecting and estimating the value of a latent reward in RL. The authors propose a method that uses a gradient-based approach to address the problem. The paper is well written and the empirical results are convincing. The reviewers agree that the paper
original:Description of paper content: The paper addresses the problem of credit assignment for delayed reward problems. Their method, Randomized Return Decomposition, learns a reward function that provides immediate reward. The algorithm works by randomly subsampling trajectories and predicting the empirical return by regression using a sum of rewards on the included states. The method is compared to a variety of existing methods on Mujoco problems in “episodic reward” settings, where the reward is zero except for the final step of the episode, where it is the sum of rewards from the original task. Theoretical argument suggests the method is an interpolation of return decomposition (regress based on all states
generated:This paper proposes a novel method for learning a deep neural network (GNN) based on a neural network. The authors propose a method to learn a large number of tasks. The reviewers agreed that the paper is well-written, well-motivated, and well-developed. The
original:The paper presents a novel method of fusion of information from two modalities: text (context and question) and a Knowledge Base, for the task of question answering. The proposed method looks quite simple and clear, while the results show strong gains against baseline methods on 3 different datasets. Ablation studies show that the model achieves good performance on more complex questions. While the reviewers raise some concerns, e. g., on the sensitivity of the proposed method, the technical novelty against prior works, they see values in this paper in general. And the authors did a good job in their rebuttal. After several rounds of interactions, some reviewers were convinced to raise their scores by a little bit. As
generated:This paper proposes a novel method for learning graph models. The reviewers agreed that the paper is well-written, well-motivated, and the empirical results are convincing. The authors did a good job of addressing an important problem.
original:The authors introduce the Time-Aware Multiperistence Spatio-Supra Graph CN that uses multiparameter persistence to capture the latent time dependencies in spatio-temporal data. This is a novel and experimentally well-supported work. The novelty is achieved by combining research in topological analysis (multipersistence) and neural networks. Technically sound. Clear presentation and extensive experimental section. Reviewers were uniformly positive, agreeing that the approach was interesting and well-motivated, and the experiments convincing. Some concerns that were raised were successfully addressed by the authors and revised in the manuscript. Happy to recommend acceptance. A veru nice paper!
generated:This paper proposes a novel method to learn to predict the distance between a classifier and the output point space. The reviewers have raised some concerns about the novelty of the method. The authors have addressed the concerns raised by the reviewers in the rebuttal and addressed some of the reviewers' concerns
original:The paper proposes a GAN framework for dynamic point cloud superresolution. It does not need scene flow supervision for training and has an interesting adaptive upsampling mechanism. Results are shown on several datasets and are reasonably convincing. Overall, all the reviewers are slightly positive about the work. After the rebuttal, all the five reviewers converged to a marginally-above-the-threshold recommendation. The meta-reviewer agreed with their assessment and would like to recommend accepting the paper.
generated:This paper proposes a novel method for learning RL models. The reviewers agree that the paper is well-written, and the empirical results are interesting, and that the proposed method is novel, which is a bit above the threshold to be accepted for acceptance to ICLR.
original:The authors introduce a method for improving reinforcement learning in sparse reward settings. In particular, they propose to take advantage of a suboptimal behavior policy as a guidance policy that is incorporated in a TRPO-like update. The reviewers agree that this is a novel and interesting idea and given the authors' rebuttal with additional experiments, clarifications and discussions, they agreed to accept the paper. However, they also point out several flaws (e. g. evaluation on a more challenging sparse-reward task such as Adroid) that I encourage the authors to address in the final version of the paper.
generated:This paper proposes a method to solve the problem of learning to learn to predict the knowledge of new tasks. The paper is well-written and well-motivated, and the proposed method is novel.
original:All reviewers believe that this paper is valuable, and the authors have made a significant, careful contribution. Some suggestions from the area chair: - "in causality" is not a standard technical term and also not non-technical idiomatic English, so it should be explained the first time it is used. - The authors should briefly cite and discuss research on so-called positive and unlabeled (PU) learning. This seems like the special case where there is exactly one known class and one novel class. The distinction between sampling in causality and labeling in causality appears in the PU literature, though not under this name. - The authors could also mention the obvious but surprising point that
generated:This paper proposes a novel approach to the problem of generating deep learning models that is novel, and is well-written and well-motivated.
original:This paper investigates the linear mode connectivity of the loss landscape of neural networks, i. e. whether a convex combination of two parameters of local optima on the SGD paths has low loss values (i. e. low barrier) up to some permutations. To probe this question, this paper empirically studies the loss gap, named as “barrier”, between two local minima and their convex combinations or linear interpolation. Before permutations, such barriers are typically non-zero; yet, after taking into account of permutation invariance of models, such barriers could be reduced along to zero with the width increasing, a main conjecture formulated in the paper
generated:This paper proposes a novel method to solve the problem of reducing the complexity of deep learning in deep learning. The authors propose a method to improve the efficiency of learning in RL. The paper is well-written, well-motivated, and well-developed. The reviewers agree that the
original:The paper shows that most variance of gradients used in FL and distributed learning in general is in very low rank subspaces, an observation also made in Konecny et al 2016 and some other related works in deep learning, though sometimes for a different purpose. The paper then proposes lightweight updates combining a fresh gradient with old updates. Experiments and a theoretical convergence guarantee complement the results, which are mostly convincing. The experiments compare against ATOMO but strangely not against the more common PowerSGD, which would also work with partial client participation. Overall, reviewers all agreed that the paper is interesting, well-motivated and deserves acceptance. We hope the authors will incorporate the open
generated:This paper proposes a novel and simple method to solve the problem of generalization in the graph-based RL problem. The paper is well-written and well-motivated, and the empirical results are strong.
original:This paper presents an extension of the Predictive State Representation (PSRs) to multi-agent systems, with a dynamic interaction graph represents each agent’s predictive state based on its “neighborhood” agents. Three types of agent networks are considered: static complete graphs (all agents affect all others experience); static non-complete graphs (only some agents affect one another); and dynamic non-complete graphs (agents affect one another in a time varying way). A number of theoretical results are presented, including PAC bounds for the approximations in the framework. The paper also contains a number of experiments that clearly show the advantages of the proposed technique over some related methods
generated:This paper proposes a novel method to solve the problem of generalization in deep neural networks. The method is based on the neural network-based neural network architecture, and the method is evaluated on a set of graph-based graphs that can be used to predict the performance of the proposed method
original:This is an intriguing work that introduces a novel sparse training technique. The core insight is a novel reparametrization or sparsity pattern based on the so-called butterfly matrices that enables fast training and good generalization. The theory is solid and useful. Most importantly, the method is novel and is likely to become impactful. Understanding better what contributes to the excellent performance is an interesting question for future work. In agreement with all the reviewers, it is my pleasure to accept the work.
generated:This paper proposes a method for using a neural network to solve the problem of unsupervised neural network-based classification of images. The paper is well-written, well-motivated, and the empirical results are convincing.
original:This paper proposed MIDI-DDSP, a structured hierarchical generative model which offers both detailed expressive controls (as in traditional synthesizers) as well as the realistic audio quality (as in black-box neural audio synthesis). Overall the reviews are very positive. All the reviewers unanimously agree that the paper is very well-written and presented a very convincing model and a meaningful step-up from the earlier work of DDSP. The authors also presented a well-documented website for the project and promised to release the source code. The reviewers raised some clarifying questions and minor corrections which the authors addressed during the response. Therefore, I vote for accept.
generated:This paper proposes a novel method of learning in graph-based RL. The reviewers agree that the paper is interesting and novel, and the empirical results show that the proposed method outperforms GNN and outperforms DQN in a number of tasks. The paper is well written, and
original:Description of paper content: The paper provides a framework to develop a family of algorithms that decompose rewards into linear combinations of several reward channels. The value functions per channel are estimated in a new space using an invertible function transformation, f. The framework encompasses several previously published algorithms, including Log Q-Learning. Conditions are provided for acceptable choices of f. Convergence to the optimal Q function in the tabular case is proven for a special learning update. Summary of paper discussion: All review scores were above the acceptance threshold. Overall, the reviewers found the idea interesting, the theoretical results satisfying, and the writing and presentation clear. Initial concern about the directedness of the experiments in showing
generated:This paper proposes a new method for solving stochastic RL problems. The paper is well-written and well-motivated, and the empirical results are interesting. The reviewers were all in agreement with the reviewers, but raised some concerns about the novelty of the paper. The authors did
original:Summary: The paper discusses Markov games with general function approximation, and investigates in particular reinforcement learning algorithms that learn a Nash policy in a trial-and-error fashion. They consider two settings: the decoupled one where the player does not observe the opponent’s policy and the coordinated one where optimistic planning is performed for both. The main contribution is a new complexity measure called Minimax Eluder dimension which is used to control the regret of the proposed algorithms. Discussions: The reviewers raised many minor concerns regarding the writing (typos of missing notation) and the clarity (missing discussions and explanations), which were addressed during the discussion phase. In light of this revision, the
generated:This paper proposes a method to solve the problem of re-supervised learning in RL. The paper is well-written and well-motivated.
original:This work suggests using models of the environment as regularizers for performing explicit transfer in RL. Here are some of the highlights from the reviews and subsequent discussions: * Novel problem * Unclear to some of the reviewers why the problem setting is in fact important. * Well-written * Interesting theoretical results * Somewhat limited experimental results Post-rebuttal, while there is not necessarily a great consensus, the reviewers all feel that it's an improved piece of work. While I am myself not fully convinced that the problem setting motivation truly aligns with the kind of empirical results that the work provides, on the balance I think this work is interesting and has sufficient novel contributions to be accepted at I
generated:This paper proposes a novel method to solve the problem of learning a deep neural network. The paper is well-written and well-motivated, and the empirical results are interesting, which should be a bit of interest to the community. The reviewers agree that this paper is a nice contribution
original:All three reviewers viewed this paper as marginally above the acceptance threshold (6). Most of the initial concerns of reviewers were around (a) the applicability of the theory to actual practical use cases and networks, and (b) the presentation and framing of the work, and scope of its results. There were fairly detailed responses from the authors: two of the three reviewers increased their scores after the author response. There's still some lingering questions as to how "real-world" relevant the theory is, but the consensus at this point is to accept the paper. My primary concern for acceptance would be that the proofs techniques are based on Boolean circuits, and none of the reviewers (nor the AC
generated:This paper proposes a novel method to solve the problem of generating graph convolutional networks. The authors propose a method for generating graph representations that can be used to predict the size of a set of graph representations. The paper is well-written, well-motivated, and well-
original:This paper presents a new method for clustering multiple graphs, without vertex correspondence, by combing existing approaches on graphon estimation and spectral clustering. All reviewers agree that this is a neat paper with new theoretical and empirical results. The main concerns were also properly addressed during rebutal. Overall, it is a good paper.
generated:This paper proposes a method to solve the problem of optimizing graph-based generative models (GANs) with high-dimensional representations. The proposed method is well-written, well-motivated, and well-developed. The reviewers have addressed the concerns raised by the reviewers
original:An interesting paper on combining NerFs with StyleGAN to get high-quality and high-resolution 3d aware generative models. The results are very good visually and also allow interactive speeds. The technique is natural and concurrent papers are proposing variations The reviewers identified a few limitations including that the nerf does not have a viewing direction and also seems limited to aligned objects with a common structure, like faces. Still the results are very interesting and suitable for publication.
generated:This paper proposes a method to solve the problem of generating and re-supervised representations of visual recognition. The authors have addressed the concerns raised by the reviewers in the rebuttal to the paper, which is well-written and well-motivated.
original:This paper proposes an adaptive tree search algorithm for NMT models with non-decomposable metrics and shows its efficacy against strong baselines. This is an interesting contribution towards overcoming the performance caps introduced by the uncontrolled-for biases of beam search, and it speaks to a growing community interested in decoding beyond greedy surprisal minimisation. The initial reviews brought to light a number of concerns that in my view are well addressed in the rebuttal and in the current version of the manuscript. One of the key issues was a confusion caused by the use of the term 'non-autoregressive' to refer to the intractability of the metric / objective function of certain models. This use
generated:This paper proposes a method for learning a representation learning approach for RL. The authors propose a method to improve the empirical performance of the RL algorithm, which is based on the RL approach, which can be used to learn a model. The paper is well-written, well-motivated
original:In this paper, the authors extend the FLAMBE to the infinite-horizon MDP and largely improved the sample complexity of the representation learning in FLAMBE. Meanwhile, the authors also consider the offline representation learning with the same framework. Although there is still some computational issue in MLE for the linear MDP, the paper completes a solid step towards making linear MDP for practice. The paper could be impactful for the RL community. As the reviewers suggested, there are still several minors to be addressed: - The extension of the proposed algorithm for finite-horizon MDP should be added. - The directly comparison between the sample complexity of FLAMBE and the proposed algorithm
generated:This paper proposes a method to solve the problem of learning a neural network. The authors show that the paper is well motivated and well executed. The reviewers agreed that the authors have addressed the concerns raised by the reviewers in the rebuttal and addressed the reviewers' concerns in the final version of
original:This paper improves the training speed and decrease the computation cost of AdvProp, which is a method that leverages the adversarial example to improve the image recognition accuracy. The method achieves the speedup by leveraging a collection of practical heuristics, including reusing some gradient computation during training. The paper is well written, well justified with empirical supports, and can be potentially useful in many vision tasks. On the other hand, some novelty of the method is incremental, and the issues regarding empirical results and claims pointed out by the reviewers need to be addressed in the revision.
generated:This paper proposes a method to solve the problem of learning knowledge. The authors propose a method that uses a non-parameters-based approach to address the problem.
original:This paper tackles the problem of exploration using intrinsic rewards in RL in states that have never been encountered before. The authors derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement, which estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples. The intrinsic reward resulting from the so-called DISDAIN (discriminator disagreement intrinsic reward) exploration bonus is more tailored to the true objective compared to pseudocount-based methods. Reviewers agree that the paper is well-motivated and well-written, that the proposed DISDAIN exploration method is simple and practical, and that experiments are convincing. Exper
generated:This paper proposes a novel method for generating graph-based graph models on graph-wise graph models. The method is based on the neural network neural network (GNN) model. The authors show that the proposed method is well-motivated, and the empirical results are strong. The
original:This paper addresses the scale issue in Graph Neural Networks by proposing a “condensation” approach that produces a small synthetic graph from a large original graph such that GNNs trained on both graphs have comparable performance. Reviewer cTj2 had concerns with novelty: they claimed the proposed method was close to gradient matching. However, they admitted that the graph setting was new. They suggested some clarity and experimental improvements. Reviewer R5cV made a similar comment w. r. t. the similarity to gradient matching. Though overall they were more positive than R5cV and thought the idea was interesting and results were compelling. Reviewer XqrK like the others
generated:This paper proposes a simple method to solve the stochastic optimization problem of deep neural network (NN) models. The paper is well-written, well-motivated, and well-developed.
original:This is a solid paper and considers the problem of training a wide neural network with a single hidden layer. This can be framed as an optimization problem in the space of probability distributions with a suitable entropy regularization, where each atom in the distribution corresponds to a hidden neuron. The dual of this problem (for finite data) is a finite-dimensional optimization problem and the paper proposes a particle based coordinate ascent scheme. The paper provides some convergence rate results. After the rebuttal, the authors have also included more experimental/numerical results. The authors have answered the concerns raised by the reviewers and overall, the paper can be accepted: The presented approach appears to be sufficiently novel and might be
generated:This paper proposes a novel method to solve the problem of detecting spurious attacks in machine learning. The authors propose a method for generating a method to optimize the accuracy of the proposed method. The paper is well-written and well-motivated, and the empirical results are convincing. The reviewers
original:The paper proposed a novel approach that leverages the discrepancies between the (global) series association and the (local) prior association for detecting anomalies in time series. The authors provided detailed empirical support to motivate the above detection criterion, and introduced a two-branch attention architecture for modeling the discrepancies and establishing an anomaly score. All reviewers acknowledge the technical novelty of this work (including the key insight of modeling anomalousness with Transformer’s self-attention and concrete training mechanism via a minimax optimization process) as well as the comprehensiveness of the empirical study. Meanwhile, there were some concerns in the positioning of the work, in particular in the clarity in connection to related work
generated:This paper proposes a method to solve the problem of adversarial descent in GALR. The authors propose a method of embedding a classifier to a GAN-based transformers. The proposed method is evaluated on a large number of benchmarks. The paper is well-written and well-
original:This paper has a deep analysis of the over-smoothing phenomenon in BERT from the perspective of graph. Over-smoothing refers to token uniformity problem in BERT, different input patches mapping to similar latent representation in ViT and the problem of shallower representation better than deeper (overthinking). The authors build a relationship between Transformer blocks and graphs. Namely, self-attention matrix can be regarded as a normalized adjacency matrix of a weighted graph. They prove that if the standard deviation in layer normalization is sufficiently large, the outputs of the transformer stack will converge to a low-rank subspace, resulting in over-smoothing. In
generated:This paper proposes a novel method to solve the problem of non-convex. The authors propose a method that uses graph-based convolutional neural networks that can be used to learn the state of the brain. The paper is well-written, well-motivated, and well
original:This paper studies the problem of motion prediction for multiple agents in a scene using transformer-based VAE like architecture. The paper received mixed reviews initially which generally tended towards borderline acceptance. All reviews appreciated extensive experiments but had some clarifications and requests for ablations. The authors provided a strong rebuttal that addressed many of the reviewers' concerns. The paper was discussed and all the reviewers updated their reviews in the post-rebuttal phase. Reviewers unanimously agree that the paper should be accepted. AC agrees with the reviewers and suggests strong acceptance. The authors are urged to incorporate reviewers' comments in the camera-ready.
generated:This paper proposes a method to solve the problem of using a non-supervised learning method to learn deep neural networks. The reviewers agree that the paper is well-written, well-motivated, and the empirical results are strong. The authors did a good job of addressing the problem
original:The authors set up a simple combination of an energy based model and a flow based model that corrects the flow based model with an energy based term. The merits of this relative only an energy based model is improved sampling to compute the gradient. The advantage over a only flow based model is that the kinds of transforms that can be used are less limited.
generated:This paper proposes a simple method to solve a large-scale optimization problem. The method is based on a gradient-based optimization. The paper is well-written, well-motivated, and well-developed. The reviewers were all positive about the paper, but raised some concerns about the
original:This paper studies online learning using SGD with momentum for nonstationary data. For the specific setting of linear regression with Gaussian noise and oscillatory covariate shift, a linear oscillator ODE is derived that describes the dynamics of the learned parameters. This then allows analysis of convergence/divergence of learning for different settings of the learning rate and momentum. The theoretical results are validated empirically, and are shown to generalize to other settings such as those with other optimizers (Adam) or other models (neural nets). The reviewers praise the clear writing and the rigorous and systematic analysis. 3 out of 4 reviewers recommend accepting the paper. The negative reviewer does not find the
generated:This paper proposes a novel method for using deep neural networks to solve the problem of stochastic reinforcement learning. The paper is well-written, well-motivated, and is well motivated. The reviewers agree that the paper is a nice contribution to the literature. The authors did a
original:This paper studies the important statistical phenomenon of double descent, a very timely topic, using influence functions, and thereby derives lower bounds for the population loss. The reviewers generally appreciated the conceptual as well as the technical contributions in the work, but argued that the set of assumptions taken by authors can potentially diminish the significance of the analysis. This, as well as additional issues regarding the empirical and the analytical support for the modeling assumptions (and the implied scope of applicability, i. e. lazy\kernel vs. rich regime) have generated considerable discussion between the reviewers and the authors. Along the process, major and minor concerns were addressed to the satisfaction of the reviewers, resulting in a substantial improvement of
generated:This paper proposes a novel method to solve the problem of the weakness of deep learning in deep learning. The authors propose a method to evaluate the performance of large-scale graph models. The paper is well-written and well-motivated, and the reviewers agree that the paper is
original:This paper introduces a method to determine which precision to use for the weights, as well as a quantisation method using hysteresis to improve performance with low-precision weights, including 4-bits. Reviewers tend to agree that the two points presented are useful and can have a large impact on the field. Generally, reviewers pointed out that motivations, notations and experimental studies could be improved. This has been partly addressed by the authors. I recommend to accept this paper for ICLR 2022.
generated:This paper proposes a novel method to solve the RL problem. The authors propose a method that uses a gradient-based approach to solve this problem. In the rebuttal to the paper, the authors did a good job addressing the concerns raised by the reviewers. The reviewers have raised some concerns
original:This paper proposes and studies a variant of policy optimization---mirror descent policy optimization (MDPO)---which was inspired by the mirror descent algorithm in the optimization literature. The proposed algorithm attempts to find a policy parameter that maximizes the expected regularized advantage function, where the regularization term is based on the KL divergence between the new policy iterate and the current policy iterate. The main contributions are algorithmic and empirical, with detailed discussions provided to illuminate the connection between MDPO and other existing policy optimization paradigms like TRPO, PPO, etc. The paper provides an interesting and useful contribution to the growing literature of policy optimization.
generated:This paper proposes a novel method to solve the problem of embedding a deep neural network (SAN) based on a neural network based on an adversarial neural network to represent an arbitrary representation of an image. The paper is well-written and well-motivated, and the reviewers
original:This paper provides a normal map-inspired implicit surface representation involving a smooth surface whose high frequency detail comes from normal displacements. Reviewers were impressed with the results and theoretical discussion in the paper. The AC agrees. The authors were responsive to reviewer feedback and addressed some questions about parameter choice during the rebuttal phase, including new experiments/discussion in the supplementary document. Note the response to reviewer WHEF notes that the authors will be releasing data/code; the AC strongly hopes the authors are true to their word in that regard. The AC chose to disregard some comments from reviewer G54X regarding tests with noise, as this method appears to be tuned to computer graphics applications; the level
generated:This paper proposes a novel method to solve the problem of unsupervised learning in neural network models. The reviewers agreed that the paper is well-motivated and well-written, and the empirical results are convincing.
original:This paper suggests the use of networks for supervised learning which are composed of a bijective network (e. g. a flow) followed by a separable function. This allows easy integration over the input space, which can be used to formulate novel regularizers (examples given are for local consistency, and for out-of-distribution detection). The approach is pretty novel, and it's an interesting paper. The reviewers were very divided, however; one reviewer giving it a 1, and another an 8, with the other two reviewers arguing weakly to accept. The "1" took issues with the general formulation, feeling that the necessity to optimize bounds on the true objectives in
generated:This paper proposes a method for learning in RL. The authors propose a method to learn in RL by using a neural network based on a non-parameter-based learning model. The paper is well-written and well-motivated, and the empirical results are interesting, and it
original:This paper proposes an alternative approach to epsilon-greedy exploration by instead generating multi-step plans from an RNN, and then stochastically determining whether to continue with the plan or re-plan. The reviewers agreed that this idea is novel and interesting, that the paper is well-written, and that the evaluations are convincing, showing large improvements over epsilon-greedy exploration and more consistently strong performance than other baselines. While the original reviews contained some questions around discussion of related work and the simplicity of the evaluation environments, the reviewers felt these concerns were adequately addressed by the rebuttal. I agree the paper explores a very interesting idea and convincingly demonstrates its potential
generated:This paper proposes a novel method for solving the problem of generating deep neural networks. The paper is well-written, well-motivated, and well-developed. The reviewers agree that the paper is a good contribution to the community. The authors did a good job of addressing the problem
original:The paper studies the length distortion in a random (deep) ReLU network — namely, it bounds the expectation and higher moments of the length of the curve in feature space produced by applying a random ReLU work to a smooth curve. Because the product of layer norms grows exponentially in the depth, it might be natural to conjecture that the length grows exponentially in depth. Indeed, this has been claimed in previous theoretical work. The submission argues through rigorous mathematical analysis and corroborating experiments that this claim is incorrect. In fact, the length exhibits a slow (1/depth) contraction as the network depth increases. The paper also works out higher order moments and extensions to higher dimensional volumes. These results
generated:This paper proposes a novel method for using graph models to learn to represent graph representations for graph models. The reviewers agreed that the paper is a solid contribution to the field of graph neural network learning. The paper is well written, the method is novel, and the empirical results are strong.
original:The reviewers were split about this paper: on one hand they would have liked to see more experiments on different problem settings on the other they appreciated the elegance of graph encoding methods and current results. After going through the paper and discussion I have voted to accept for the following reason: the additional experiments and discussion posted during the rebuttal phase have addressed many of the main concerns of the reviewers (i. e., training time, message passing figure, discussion on encoding and SAT solvers). The only remaining one I see is the request for additional experiments which I don't think is grounds for rejection: current results are comprehensive and an additional experiment I think would not alter the main conclusions. I urge the
generated:This paper proposes a method to learn the optimal neural network architecture for deep neural networks. The method is based on a neural network neural network.
original:The paper introduces As-ViT, an interesting framework for searching and scaling ViTs without training. Overall, the paper received positive reviews. On the other hand, R1 rated the paper as marginally below the threshold, raising concerns about search on small datasets and issues regarding the comparison in terms of FLOPS/accuracy with other methods. The authors adequately addressed these concerns in the rebuttal, and helped clarify other questions by R2 and R3. R1 did not participate in the discussion after the author response nor updated his/her review. The AC agrees with R2 and R3 that the paper passes the acceptance bar of ICLR, as the unified approach for efficient search
generated:This paper proposes a novel method to solve the problem of unsupervised reinforcement learning.
original:Multi-objective learning is an increasingly important topic. This paper presents a method for better finding parts of the Pareto frontier through a new method to estimate the distance to the frontier and use this proxy to refine the state space partition. The reviewers found this paper interesting and compelling and generally well written. The reviewers also thought the work could be further improved by better clarifying in the text where the proposed approach might fail, and what properties of the domain are needed, and also to better situate this paper within the related work, potentially including additional experimental comparisons. The authors provided detailed responses to the proposed questions and the authors are encouraged to ensure that these suggestions and discussions are well represented in
generated:This paper proposes a novel approach to the generalization of deep learning models. The reviewers agree that the paper is well-written, well-motivated, and the empirical results are strong. The authors did a good job of addressing some of the concerns raised in the reviewers' reviews.
original:This article introduces an interesting variant of the work of Nakkiran & Bansal (2020). It shows empirically that the test error of deep models can be approximated from the disagreement on the unlabelled test data between two different trainings on the same data. The authors then show theoretically that a calibration property can explain such behaviour, and they report experiments showing that the relationship does exist in practical situations. All reviewers agree on the practical and theoretical value of the article, which is very well organised and written. The ideas developed here are likely to lead to further work in the future, and they clearly deserve to be published at ICLR. I agree with one of the reviewers
generated:This paper proposes a new framework for using visual recognition in deep neural network learning. The paper proposes the use of the visual representation of visual representations of visual information in visual language models. The reviewers agreed that the paper is well-written and well-motivated, and the empirical results are
original:This paper develops a new large language model trained on 25TB of (simplified) HTML text data. The HTML tags provide valuable information about the document structure. The training adapted the BART denoising objectives (to inject noisy size hint to control generation length during training). The paper also studies various prompting methods for the model. The model achieves state-of-the-art performance on zero-shot summarization and several text classification tasks. Reviewers have found the motivation of pretraining with structured text convincing, and the results are good.
generated:This paper proposes a new framework for the optimization of deep neural network models. The authors propose a method to solve the problem of reducing the complexity of the neural network. The paper is well-written, well-motivated, and is a good contribution to the literature. The reviewers were
original:The paper addresses hierarchical kernels and provides an analysis of their RKHS along with generalization bounds and cases where improved generalization can be obtained. The reviewers appreciated the analysis and its implications. There were multiple concerns regarding presentation clarity, which the authors should address in the camera ready version.
generated:This paper proposes a new method for learning a task-specific version of a classifier. The paper proposes to learn a set of tasks with a large number of tasks. The proposed method is well-motivated, well-developed, and well-written. The reviewers agree that the
original:This work defines the new problem of lifelong few-shot language learning where the goal is to continually learn new few-shot tasks and use those to benefit future tasks while not forgetting previous tasks. With larger models, this is an important goal due to the cost of updating and retraining these models. The work also shows superiority to existing approaches like EWC and MAS. After the author's rebuttal, the experimental section is also thorough with evaluation on a good range of tasks and approaches such as adapters showing good results. While this setting appears simpler than the full lifelong-learning setting and the approach combines existing ideas, this work's contribution to the definition and thinking about this problem is valuable. However
generated:This paper proposes a novel method of learning learning models. The authors propose a method to learn in deep reinforcement learning. The proposed method is based on a non-convex representation of a classifier and the proposed method can be used to predict the performance of the model. The reviewers
original:The paper focused on deep regression problems and proposed a label encoding technique which can be thought as a sibling of the famous error-correcting output codes but designed for regression problems. The main idea is well illustrated in Figure 1 at the top of page 3, where the encoder and decoder are the main objects of the proposal (and a quantizer is also needed for using the encoder/decoder which is a uniform quantizer in the paper). The idea/proposal is supported by solid theoretical arguments and convincing empirical evidences (not only the paper but also the rebuttal). While there were some concerns in the beginning, the authors have successfully clarified all the concerns and then the
generated:This paper proposes a novel method for using neural network models to solve an important problem in deep neural network optimization. The authors propose a novel approach to solve the problem of gradient descent. The paper is well-written, well-motivated, and is a nice contribution to the community of
original:This paper proposes an algorithmic approach to estimating upper and lower bounds of the rate-distortion (R-D) function of a data source on the basis of samples drawn from it. The proposed upper bound is based on the variational objective employed in the Blahut-Arimoto algorithm, whereas the proposed lower bound is based on the dual characterization of the R-D function. In both bounds neural networks trained with samples are utilized. Experimental results on four sources (Gaussian, banana-shaped, GAN-generated images, and natural images) are provided. The four review scores were initially two positives and two negatives. Some reviewers evaluated positively the argument on the lower bound
generated:This paper proposes a method for optimizing the loss of a large number of images. The authors propose a method to improve the generalization performance of the neural network. The paper is well-written, well-motivated, and is well motivated. The reviewers agreed that the paper is
original:I recommend this paper to be accepted. All reviewers are in agreement that this paper is above the bar.
generated:This paper proposes a novel method to solve the problem of generating graph generative models.
original:The paper argues that existing evaluation metrics for GGMs are insufficient and perform an extensive empirical study questioning their ability to measure the diversity and fidelity of the generated graphs. To solve these limitations, they propose a new evaluation metric that computes the Maximum Mean Discrepancy (MMD) between graph representations of the sampled and real graphs, as extracted from an untrained GGM model. All the reviewers agreed that the research problem is interesting and the overall idea behind the proposed metric is sound and novel. While there were some concerns regarding some details/comparisons/conclusions of the experimental evaluation, the rebuttal managed to cleared up these concerns and all the reviewers eventually supported acceptance.
generated:This paper proposes a method to solve the problem of generalization in deep learning. The authors propose a method for learning learning in the domain of RL. The paper is well-written and well-motivated, and the empirical results are convincing. The reviewers were all positive about the paper,
original:One might assume that the k-means problem has already been beaten to death, but this paper shows there are still remaining questions. And rather interesting ones at that, with a novel angle of having additional help from a prediction algorithm of cluster memberships. This connects to learning-augmented algorithms research. The reviewers agreed that the problem is interesting and gives a novel angle, and the interestingness stems from novelty, and the ability to "escape" from NP-hardness. The reviewers and authors had nice discussions about details and conclusions, on how limiting is it that the authors focus on reasonably accurate predictors, for instance, and where could the predictors come from. This is
generated:This paper proposes a method to solve the problem of learning representation learning in RL. The authors propose a method that uses a gradient-based learning method that achieves good performance in deep learning. The proposed method is based on a neural network, which can be used to learn representation models. The
original:This paper proposes augmenting standard forward prediction techniques used for representation learning with backward prediction as well, termed "learning via retracing". The paper implements this idea in a Cycle-Consistency World Model (CCWM) and demonstrates that CCWM improves performance of a Dreamer agent across a number of Control Suite tasks. The paper also proposes a way to detect "irreversible" transitions and exclude them from the backwards prediction step. This paper generated mixed opinions, and the reviewers did not come to a consensus on whether it should be accepted or rejected. In particular, Reviewer VSAG maintained it should be accepted, while Reviewer NEVM maintained it should be rejected. The
generated:This paper proposes a novel method to solve the problem of generating a large set of adversarial explorations. The authors propose a method that uses a combination of visual recognition systems, which can be used to generate a variety of examples. The proposed method is evaluated on a number of benchmarks.
original:This paper proposes a counterfactual explanation method, termed DISSECT, for image classification. While previous work is concerned with generating one single counterfactual, DISSECT aims to produce multiple counterfactuals, with each illustrating one possible way the class label could be altered. Intermediate images between the benign example and the counterfactuals are also generated to show how the decision boundary is crossed. The reviewers find the idea novel, the presentation clear, and the empirical evaluation thorough. However, there are concerns regarding whether the method will generalize to other domains because it relies on a strong generative model. In addition, there is no human-subject study to show whether and how
generated:This paper proposes a novel method for learning graph representation learning. The authors show that the proposed method outperforms previous methods on graph representations. The proposed method is well-written, well-motivated, and the empirical results are convincing.
original:Improving the expressiveness of GNN is an important problem in the current graph learning community. Its key idea is to generate subgraphs from the original graph, then encode the subgraphs into the message passing process of GNN. The proposed method is proven to be strictly more powerful than 1-WL. The authors also quantize how design choices such as the subgraph selection policy and equivariant neural architecture can affect the architecture’s expressive power. After the rebuttal, all reviewers are glad to accept this submission. During the discussion, while reviewer B3oK has shown some concerns on the concurrent works in NeurIPS 2021, it should not affect the
generated:This paper proposes a novel method to solve the problem of unsupervised reinforcement learning. The authors propose a method to learn a deep neural network with a large number of tasks. The method is evaluated on a large set of datasets. The reviewers agree that the paper is well-motivated
original:This paper received 4 quality reviews. The rebuttal and discussions were effective. All reviewers raised their ratings after the rebuttal. It finally received 3 ratings of 8, and 1 rating of 5. The AC concurs with the contributions made by this work and recommend acceptance.
generated:This paper proposes a novel method to solve the problem of generating a non-convex data model with a large number of variables. The empirical results of the paper are interesting, and the empirical results are interesting. The paper is well-written, well-motivated, and is
original:This paper extends recent and very active literature on analyzing learning algorithms in the simplified setting of Gaussian data and model weights, with the main generalization being to allow for non-isotropic covariance matrices. The main technical results seem to be correct and slightly novel, though reviewers feel they are not innovative or unexpected enough to stand on their own. However, the main contributions of the paper are then to interpret these results to give phenomenological results (regarding double descent, etc.), and reviewers were unanimously happy with these. In the end, all reviewers were positive about the paper. The largest reviewer criticisms of the paper were technical issues (ot31) and lack of context of recent literature
generated:This paper proposes a simple method to solve the problem of the generalization problem. The reviewers have raised some concerns about the novelty of the paper, which is well-written and well-motivated, and the empirical results of the authors' work. The main concerns raised by reviewers are
original:This paper offers a refinement of the information-theoretic characterization of the generalization of models obtained via SGD. This is assessed on some basic neural architectures and inspires the use of new regularizers. Overall, even though the perspective of this paper is not novel, the presented results appear to be clearer and tighter than prior instances of the same ideas. This was appreciated by most reviewers. The few clarity and organization concerns that were raised by the reviewers were adequately addressed by the authors. Overall, the paper deserves to be shared with the community.
generated:This paper proposes a novel approach to the problem of learning knowledge in RL. The paper is well-written, well-motivated, and is well motivated.
original:Strong submission that analyses the unsupervised skill discovery setting from the perspective of information geometry, which leads to some interesting conclusions. In particular, it is shown that this does not lead to skills that are optimal for all reward functions, but does provide a good initialization for methods that aim to find optimal policies. Across the board, the reviewers believe the analysis provided by this work is both important and novel. And while there were some initial concerns raised, such as lack of empirical confirmation of some of the claims and some questions about the analysis, the authors have addressed all of these concerns convincingly. Hence, I strongly recommend acceptance of this submission.
