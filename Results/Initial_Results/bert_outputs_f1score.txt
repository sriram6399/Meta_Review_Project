generated:This paper proposes a novel method for learning representation models. The authors propose a method to solve the problem of learning representation. The method is based on a neural network (NLP) based on stochastic attention, and the method is evaluated on a large number of benchmarks. The reviewers
original:This work receives mostly positive rates. Most reviewers agree that the use of Bayesian attention to neural processes is novel, and its interpretation is interesting. Since the reviewer TBTA requests a substantial revision of the submission and fortunately authors’ feedback is thoroughly satisfactory, we highly recommend the authors to prepare for a significantly improved camera-ready version that clarifies most of reviewers’ concerns.
generated:This paper proposes a novel method for learning graph models. The authors propose to learn graph models with a GNN based on a graph-based neural network (GNN) based on the GNN-based GNN (GCN) model. The paper is well-written and well
original:This paper proposes a very simple procedure to accelerate the inference time of graph-structured Neural Networks, by distilling knowledge of a GNN into a node-wise MLP. Despite some concerns about the novelty of the methodology (which borrows heavily from previous KD works), reviewers generally found this empirical work well executed and providing a potentially useful baseline for large-scale applications. Therefore, the AC recommends acceptance.
generated:This paper proposes a novel method for solving an important problem, and is well-written, well-motivated, and well-developed. The authors did a good job of addressing the problem of covariance in the paper, and the reviewers agree that the paper should be accepted for publication
original:This paper is a solid contribution to researchers in this field, as it provides a new idea for the basic problem of determining the direction of causality between two variables, using the functional causal model as a dynamical system and optimal transport.
generated:This paper proposes a novel method of learning learning in RL. The authors propose a method to solve the problem of generalization in RL, which is an important problem. The paper is well-written, well-motivated, and well-developed. The reviewers agree that the paper is
original:The topic of learning reward functions from preferences and how to do this efficiently is of high interest to the ML/RL community. All reviewers appreciate the suggested technical approach and the thorough evaluations that demonstrate clear improvements. While the technical novelty of the paper is not entirely compelling, all reviewers recommend acceptance of the paper.
generated:This paper proposes a new method for detecting the generalization performance of deep neural networks. The reviewers agree that the paper is well-written and well-motivated, and the reviewers were in agreement with the reviewers' comments. The authors did a good job in addressing the concerns raised by
original:This paper presents a through study of generalization in visual representation learning. It compares in distribution generalization to out of distribution generalization using a comprehensive benchmark. The paper received very positive reviews from all reviewers. Reviewers agreed that the paper has several strengths: It is very well written, the presented benchmark is very useful and the analysis is thorough. One concern that was brought up by the reviewers was that a majority of the presented findings are expected and in a sense, known to the community. The authors have addressed this concern by pointing out that their findings are more fine grained than past works and that their proposed benchmark is a stepping stone towards measuring general robustness. I must note that in
generated:This paper proposes a method for learning to represent visual images. The reviewers have raised some concerns about the paper, which is well-written and well-motivated. The authors have addressed the reviewers' concerns in the rebuttal.
original:The authors explore the hypothesis of whether grounded representations can be leaved from text only. They show that a language model trained with relatively little data can make conceptual domains such as color to a grounded world representation such as RGB coordinates. The paper was positively received by the reviewers, specifically after a fruitful discussion to further clarify the points that the authors were making and their conclusions. The authors have already edited some parts of the paper, I ask them to go back and include other points that the reviewers have made. I recommend this paper for acceptance, it will generate good discussion and ideas at ICLR.
generated:This paper proposes a novel method of learning learning models with a deep neural network. The reviewers agreed that the paper is well-written and well-motivated, and that the reviewers are in agreement with the reviewers' recommendations.
original:This paper has been independently reviewed by four expert reviewers. Two of them recommended straight acceptance, one of them assesses this work as marginally acceptable after increasing their score as a result of the author's rebuttal, and the last reviewer considers this paper marginally below the acceptance threshold. While the reviewers agree on the importance of the targeted problem and relative novelty of the presented work, the main points of criticism involve empirical evaluations - its methodology, experimental design, missing relevant and important comparisons. Since the authors have addressed most of those concerns in their rebuttal, I am leaning towards recommending acceptance of this work for ICLR.
generated:This paper proposes a method to improve the performance of RL-based learning. The authors propose a method that achieves good performance in the domain of visual recognition. The paper is well-written and well-motivated, and the empirical results are strong. The reviewers have addressed the concerns raised
original:This paper is proposed to investigate the robustness of self-supervised learning (SSL) and supervised learning (SL) in both balanced (in domain) and imbalanced (out of domain) settings. It can be concluded that SL can regularly learn better representations than SSL, and representations are better from balanced than from imbalanced datasets. The SSL is more robust than SL in the imbalanced settings, which is the crucial of this paper. Expect the experimental results, the authors also provided theoretical analysis to support their claims. The authors also extend a well-established method SAM into the Reweighted SAM as the technical contribution to better address the imbalanced setting. The paper is well written with
generated:This paper proposes a method for solving the problem of detecting and re-supervised RL. The authors show that the paper is well-written, well-motivated, and well motivated. The reviewers have raised some concerns about the novelty and novelty of the paper and the empirical results,
original:This paper considers helping to decide whether behavior cloning or offline RL is likely to be more effective given a particular offline dataset. The reviewers initially appreciated the importance of insights into this question around how to best leverage an existing dataset. They also had some initial concerns, due in part because the theory is restricted to tabular settings, whereas many challenges typically arise when function approximators are used, the realisticness of the assumptions over the data collection process, and a number of places where further details or clarifications would better situate and strengthen the work. The authors gave very extensive responses to the feedback which made reviewers feel much more confident about the revised paper and resulted in significantly higher scores. Though there
generated:This paper proposes a method to solve the problem of solving a simple domain-based domain-specific problem. The authors propose a simple method for solving an important problem, which can be used to solve a large number of important problems. The paper is well-written and well-motivated
original:The paper introduces the maximum n-times coverage, a new NP-hard (and non-submodular) optimization problem. It is shown that the problem can naturally arise in ML-based vaccine design, and two heuristics are given to solve the problem. The results are used to produce a pan-strain COVID vaccine. The reviewers and I think that this is an interesting paper with a compelling application. There were some concerns about theoretical novelty and biological accuracy but these were addressed during the author response period. Given this, I am delighted to recommend acceptance. Please incorporate the feedback in the reviews in the final version of the paper.
generated:This paper proposes a method to exploit the adversarial complexity of vision transformers.
original:This paper provides an interesting study on the adversarial robustness comparisons between ViTs and CNNs, and successfully challenges the previous belief that ViTs are always more robust than CNNs on defending against adversarial attacks. Specifically, as revealed in this paper, when the attacker considers the attention mechanisms, the resulting patch attack can hurt ViTs more. Overall, all the reviewers enjoy reading this paper and appreciate the comprehensive robustness comparisons between ViTs and CNNs. The reviewers were concerned about the missing experiments about adversarial training, vague statements about the inspiration for future defenses, visualization of adversarial examples, etc. All these concerns are well addressed during the discussion period, and all reviewers reach a
generated:This paper proposes a new framework for using visual recognition in deep neural network learning. The paper proposes the use of the visual representation of visual representations of visual information in visual language models. The reviewers agreed that the paper is well-written and well-motivated, and the empirical results are
original:This paper develops a new large language model trained on 25TB of (simplified) HTML text data. The HTML tags provide valuable information about the document structure. The training adapted the BART denoising objectives (to inject noisy size hint to control generation length during training). The paper also studies various prompting methods for the model. The model achieves state-of-the-art performance on zero-shot summarization and several text classification tasks. Reviewers have found the motivation of pretraining with structured text convincing, and the results are good.
generated:This paper proposes a method for learning a neural network to learn a non-convex representation of a classifier. The method is based on neural networks, which can be learned by using RL. The paper is well-written and well-motivated, and the empirical results are strong.
original:A novel method is described that uses RL to search for a rule set which predicts multiple relations at once for KBC-like problems. The rules can include latent predicates, which reduces the complexity of individual rules, similar to Cropper & Muggleton's (2015) meta-interpretive learning framework, which is usual for rule-learning systems. Another novel aspect is use of a cache memory for rules. Pros - the idea of using RL instead of carefully-designed discrete search for symbolic learning systems is a very nice novel idea - the experimental results are strong Cons - the benchmarks are synthetic (although GraphLog does at least include noise) - although the Cropper and Muggleton
generated:This paper proposes a novel method for learning and re-supervised reinforcement learning for RL-based reinforcement learning. The authors propose a method that uses visual recognition to solve the problem of unsupervised supervision of visual recognition. The paper is well-written, well-motivated, and
original:The paper addresses vision-based and proprioception-based policies for learning quadrupedal locomotion, using simulation and real-robot experiments with the A1 robot dog. The reviewers agree on the significance of the algorithmic, simulation, and real-world results. Given that there are also real-robot evaluations, and an interesting sim-to-real transfer, the paper appears to be an important acceptance to ICLR.
generated:This paper proposes a method for detecting adversarial adversarial robustness in neural network learning. The authors propose a method to solve the adversarial weakness problem. The paper is well-written and well-motivated, and the empirical results show that the proposed method is novel, and
original:The paper formalizes the adversarial attack problem for transductive defenses, where the model is sequentially updated with a batch of (adversarial) test inputs. The paper comes up with a quite generic attack scheme and their instantiation of this scheme shows that RMC and DENT are not robust respectively not more robust than the underlying adversarially robust base model. Positive - formal treatment of attacks on transductive defenses including discussion about different types of attacker knowledge - the attack model is quite generic and could work for future transductive defenses and thus is a useful baseline attack which could be suggested to be used by future transductive defenses for robustness evaluation. In particular,
generated:This paper proposes a novel method for using graph-based neural network models. The reviewers agreed that the paper is well-written and well-motivated, and that the authors have addressed the concerns raised by the reviewers. In the rebuttal to the reviewers, the authors did a good
original:The paper addresses a problem encountered in many real-world applications, i. e. the treatment of tabular data, composed of heterogeneous feature types, where samples are not i. i. d. In this case, learning is more effective if the typically successful approach for i. i. d. data (boosted decision trees + committee techniques) is combined with GNN to take into account the dependencies between samples. The main contribution of the paper with respect to previous work in the field is the introduction of a principled approach to pursue such integration. One important component of the proposed approach is played by the definition of a specific bi-level loss (efficient bilevel boosted smoothing
generated:This paper proposes a novel method for learning in graph models. The authors propose a neural network based on a gradient-based neural network (ST-GNN) based on neural networks that can represent graph-based graph representations. The reviewers agree that the paper is well-motivated and
original:This paper proposes a new time-varying convolutional architecture (ST-GNN) for dynamic graphs. The reviewers were positive about the presentation and detailed theory, especially on the stability analysis. The shared criticism was on experimental validation synthetic datasets that the reviewers did not find appealing. The AC believes that while the lacking validation concerns are legit, there is a lack of sophisticated dynamic graph benchmarks in the community yet, so the authors did their best effort to test their method. We thus recommend to accept the paper.
generated:This paper proposes a novel method for the generalization of deep neural network models. The paper is well-written and well-motivated, and is well motivated. The reviewers were all positive about the paper. The authors did a good job of addressing the concerns raised by the reviewers and
original:*Summary:* Low-rank bias in nonlinear architectures. *Strengths:* - Significant theoretical contribution. - Well written; detailed sketch of proofs. *Weaknesses:* - More intuitions desired. - Restrictive assumptions. *Discussion:* Authors made efforts to improve the discussion in response to 6P7z. Authors agree with eeoo about Assumption 2 being relatively restrictive but point out that main results do not need it. They discuss Assumption 1 and revised it formulation. Reviewer eeoo was satisfied with this. Following the discussion udhX raised their score (after authors acknowledged an early problems and improved them) and found the paper well written
generated:This paper proposes a method to solve the non-convex optimization problem of deep neural network models. The authors show that the proposed method outperforms previous methods. The reviewers have raised several concerns about the paper.
original:Canonical correlation analysis is a method for studying associations between two sets of variables. However these methods lose their effectiveness when the number of variables is larger than the number of samples. This paper proposes a method, based on stochastic gating, for solving a ℓ0 -CCA problem where the goal is to learn correlated representations based on sparse subsets of variables. Essentially, this paper combines ideas from Yamada et al. and Suo et al. who introduced Gaussian-based relaxations of Bernoulli random variables, and sparse CCA respectively. They also extend their methods to work with nonlinear functions by integrating deep neural networks into the ℓ0
generated:This paper proposes a novel method for learning in neural network models. The authors propose a method to solve the problem of learning in deep learning. The method is based on a neural network-based neural network.
original:This paper presents ODConv, a convolution pattern which uses attention in the convolutions across all dimensions of the weight tensor. The paper is well motivated and well explained, easy to follow. This work is built on top of previous work, but reviewers all agree that the contributions of this paper are significant. The experimental section is comprehensive, with several benchmarks, and show clear improvements. The reviewers suggested a few additional remarks, and discussions to add to the paper, which the authors have addressed in the rebuttal. Reviewers seem in general happy with the authors answers to their concerns. This seems like a sound and meaningful paper. I am fully in favour of acceptance, and I recommend
generated:This paper proposes a method for using a neural network to learn a deep neural network (SNN) based on a gradient-based neural network. The reviewers agreed that the paper is well written, and the empirical results are interesting, and it is well-written and well-motivated
original:The authors provide a theory for training feed-forward spiking neural networks (SNNs) on input-to-output spike train mappings. They utilise for this heterogenous neurone and skip connections. The resulting method is tested on DVS Gesture, N-Caltech 101 and sequential MNIST. It achieved very good performance. The reviewers agreed that the results are interesting and significant. In the initial reviews, the reviewers pointed out some doubts about the theory and clarity of writing. These doubts and objections were addressed in the revision and the reviewers were quite satisfied with that. In conclusion, the manuscript presents interesting results for SNNs with a solid theory and very good experimental
generated:This paper proposes a novel method for solving the problem of generating deep neural networks. The paper is well-written, well-motivated, and well-developed. The reviewers agree that the paper is a good contribution to the community. The authors did a good job of addressing the problem
original:The paper studies the length distortion in a random (deep) ReLU network — namely, it bounds the expectation and higher moments of the length of the curve in feature space produced by applying a random ReLU work to a smooth curve. Because the product of layer norms grows exponentially in the depth, it might be natural to conjecture that the length grows exponentially in depth. Indeed, this has been claimed in previous theoretical work. The submission argues through rigorous mathematical analysis and corroborating experiments that this claim is incorrect. In fact, the length exhibits a slow (1/depth) contraction as the network depth increases. The paper also works out higher order moments and extensions to higher dimensional volumes. These results
generated:This paper proposes a method to solve the adversarial optimization problem. The authors propose a method that is novel and novel, and the reviewers agree that the paper is well-written and well-motivated.
original:The paper proposes monotonic splines as an improvement on current approaches to parametrising quantiles in distributional RL. The idea is an obvious, natural improvement on what exists, and yields improved experimental results.
generated:This paper proposes a method to solve the problem of deep learning. The reviewers agree that the paper is well-written and well-motivated, and the empirical results are convincing. The authors show that the proposed method is novel, well-developed, and outperforms previous methods. The
original:This paper presents a novel methodology for performing meta learning for gradient-based hyperparameter optimization. The approach overcomes limitations (scaling, e. g.) of previous methods through distilling the gradients of the hyperparameters. The paper received 4 reviews, of which all were positive (6, 6, 8, 8). The reviewers appreciated the technical clarity of the paper and found the proposed approach sensible, novel, technically sophisticated and effective. The main concerns were regarding the comprehensiveness of the experiments and technical presentation of the dataset distillation. It seems that the reviewers found the author response (lots of results were added) satisfactory regarding these points. Thus the recommendation is to accept
generated:This paper proposes a novel method to learn the sampling density of a neural network to encode a classifier. The proposed method is based on a deep neural network (NN) based on deep neural networks, which can be used to predict the sampling distance from the input data. The method is
original:Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors' rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.
generated:This paper proposes a novel method to solve the problem of using continuous-time reinforcement learning in graph models. The authors propose a new method for using a simple gradient-based method that uses a large number of continuous-space convolutional models, which can be used to learn to learn
original:The paper tackles a very interesting problem in the context of diffusion-based generative models and provides empirical improvements. Pre-rebuttal, reviewers' main concerns lie in the motivation and clarification of the method, while after rebuttal, all reviewers satisfied the response and gave positive scores. The authors should include the additional results to well address the reviewers' concerns in the final version.
generated:This paper is a solid contribution to the conference phase of the conference. The paper is well-written, well-motivated, and is well motivated. The reviewers have addressed the concerns raised by the reviewers in the rebuttal. The authors did a good job of addressing the concerns of
original:This paper presents a comparison and analysis of continual learning methods for pretrained language models. The authors categorise continual learning methods into three categories, those that use cross task regularisation, those that employ some form of experience replay of previous training examples, and those that dynamically alter the network architecture for each task. Evaluation results from representative examples of these three paradigms are then presented and analysed. In general methods that incorporate experience reply appear to perform the best, while analysis of the predictive power of individual layers of the pretrained models suggests that some network layers are more robust to catastrophic forgetting than others, and that this also varies across architectures (BERT, ALBERT, etc.). In general
generated:This paper proposes a novel method to solve the problem of detecting and quantifying the impact of the shift in the performance of deep learning models in language models.
original:The paper studies real world ML APIs' performance shifts due to API updates/retraining and proposes a framework to efficiently estimate those shifts. The problem is very important and the presented approach definitely novel. My concern is about limited novelty of the theoretical analysis and weak experimental evaluation (just two dates, limited number of systems tested, small number of ablations). As of now the paper looks like an interesting but unfinished proposal. Looking forward to the discussion between the authors and the reviewers to address the concerns. In the rebuttal, the authors have addressed reviewers' comments, in particular by adding additional experiments that strengthen the paper. All the reviewers recommend the paper to be accepted. It is suggested that in
generated:This paper proposes a novel method to solve the problem of unsupervised reinforcement learning. The reviewers agreed that the paper is well-written and well-motivated, and the empirical results are interesting.
original:In the end, all reviewers agreed that this is a solid piece of work. However, there were also some doubts regarding the relevance of the block diagonal design and the underlying assumptions about the p/n ratio. The majority of the reviewers, on the other hand, had the impression that the positive aspects dominate the potential problems, and I also share this viewpoint. However, I'd like to encourage the authors to carefully address the points of criticism raised by the reviewers in their final version.
generated:This paper proposes a novel approach to the problem of learning neural networks. The reviewers were positive about the paper, and the authors did a good job of addressing some of the concerns raised by the reviewers. The paper is well-written, well-motivated, and well motivated. The
original:The paper points out an interesting and, to me unexpected, problem when learning Q-functions to do with spectral bias. Figures 1 and 2 are quite striking. The diagnosis and proposed solution elegantly combines ideas from NTKs and NeRFs. The proposed random Fourier actor-critic performs well in practice. The main problem reviewers had in the end is that the authors added substantial new empirical results too late to review thoroughly.
generated:This paper proposes a method to solve an important problem in machine learning. The authors propose a novel method to improve the performance of deep neural network models.
original:This paper introduces Back2Future, a deep learning approach for refining predictions when backfill dynamics are present. All reviewers agree on that the authors successfully motivate their work and introduce a topic of great interest, i. e. that of dealing with the effect of revising previously recorded data and its effect timeseries predictions. The reviewers also underline the strong and thorough experimental section. Among the reviews is also underlined the potential impact of the work for the research domain. Many thanks to the authors for replying to the minor concerns raised. I concur with the reviews and find this submission very interesting, convincing and thus recommend for accept. Thank you for submitting the paper to ICLR.
generated:This paper proposes a novel method for learning a method to solve the problem of solving the general problem of optimizing the optimization of deep learning. The authors propose a method that uses the task of learning the task from a set of tasks. The paper is well-written and well-
original:This paper addresses an important issue of AutoML systems, specifically their ability to "cold start" on a new problem. Some of the reviewers initially had concerns about the experimental validation and the theoretical foundations of the method, but during the discussion phase the authors addressed concerns extremely well. The authors already included most of the feedback of reviewers, further strengthening the paper.
generated:This paper proposes a novel method for the learning of graph neural networks. The method is based on a neural network (GNN) based on gradient descent. The authors propose a method that is relatively simple, and the empirical results are strong. The paper is well-written and well-motivated
original:Overall the paper present the idea of caching and using stale information to update instead of sub sampling for speeding up graph convolution neural network. Reviewers liked the idea but also there were concerns about experimental comparisons. In the rebuttal the authors did provide more evidence of comparison with other caching based and other relevant baselines. Overall the importance of scaling up GCNN and empirical results helped the paper cross the high bar.
generated:This paper proposes a novel method for learning a neural network to learn a classifier. The paper is well written, well written and well motivated, and is well motivated. The reviewers were positive about the paper and all reviewers agreed that the paper is interesting and the reviewers did a good job
original:All reviewers consistently agree on the high quality of the research presented in this paper, such that it the paper clearly is significantly above the acceptance threshold of ICLR.
generated:This paper proposes a novel method of learning neural network models. The reviewers agree that the paper is well-written, well-motivated, and well motivated, and is well written.
original:This paper presents a novel framing of what's at stake when selecting/segmenting text for use in language model pretraining. Four reviewers with experience working with these models agreed that the conceptual and theoretical work here is insightful and worth sharing. The empirical work is fairly small-scale and does not yet support broad conclusions, but reviewers did not see such conclusions as necessary for the paper to be valuable.
generated:This paper proposes a method for learning learning-based neural networks to learn deep neural networks. The authors propose a method to learn a deep neural network based on a neural network-based method. The paper is well-written and well-motivated, and the empirical evaluation of the method
original:Dear Authors, The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted: - This paper does contribute solidly to a timely topic of theoretical understanding of sparisty recovery with deep unroling. - The original version had very limited experiments and only synthetic ones, which raised concerns about whether the setting is motivated and whether the algorithm works on actual real data. The revision fixed that to an extent with some experiments on real data. Yet, there are still some concerns that we suggest to be tackled for the final version: - The capacity analysis is carried out inside a strongly convex regime while the algorithm is advocated for noncon
generated:This paper proposes to solve the problem of task-specific reinforcement learning by using a method that uses a neural network to learn a task. The authors show that the paper is well-written and well-motivated.
original:The main detractor of this paper feels that the paper makes a relatively small technical and empirical contribution given existing results on HER (Andrychowicz et al., NeurIPS 2017). However, several other reviewers, who had more engagement in the discussion, were strong supporters. Having looked at the paper myself I thought the selection of experimental problems undermined the results. Experiments are most compelling when many unaffiliated groups compete on the same benchmarks. But the basic idea of integrating HER with AlphaZero, and a reasonable attempt at this, seems to be interesting enough to warrant a poster.
generated:This paper proposes a novel method to solve the problem of adversarial loss. The reviewers agree that this paper should be accepted for publication. The paper is well-written, well-motivated, and well written, and the reviewers have addressed the concerns raised by the reviewers. The authors
original:This contribution investigates and takes a step back on an important problem in recent ML, namely the impact of the noise distribution in density estimation using Noise Contrastive Estimation. The work offers both theoretical insights and convincing experiments. For these reasons, this work should be endorsed for publication at ICLR 2022.
generated:This paper proposes a novel method for using graph models to predict the output of deep neural networks. The paper is well-written and well-motivated, and the empirical results are convincing. The empirical evaluation of the proposed method is somewhat weak, but it is a nice contribution to the
original:The authors introduce a novel probabilistic hierarchical clustering method for graphs. In particular they design an end-to-end gradient-based learning to optimize the Dasgupta cost and Tree Sampling Divergence cost at the same time. Overall the paper presents solid results both from a theoretical and experimental perspective so I think it is a good fit for the conference and I suggest accepting it.
generated:This paper proposes a novel method to solve the problem of re-supervised reinforcement learning in deep neural network architecture. The authors propose a method to learn the state of the classifier. The paper is well-written and well-motivated, and the empirical results are convincing. The
original:This paper proposes a novel method for the single-shot domain adaptation with the help of Generative Adversarial Nets. The proposed method is interesting, novel, and versatile. Moreover, the performance is impressive and better than the existing methods. However, the writing needs some improvement for better readability. More quantitive results should be provided in the revision for completeness.
generated:This paper proposes a novel method of estimating uncertainty. The authors propose a method that can be used to predict the probability of a set of variables (i. e. the value of a class, and the distance between the features of the class and the state of the network). The method
original:This paper presents a method for producing higher quality uncertainty estimates by mapping the predictions from an arbitrary (e. g. deep learning) model to an exponential family distribution. This is achieved by using the model to map from the inputs to a low-dimensional latent space and then using a normalizing flow to map to the parameters of the distribution. The authors show empirically that this improves over a variety of baselines on a number of OOD and uncertainty quantification tasks. This paper received 5 reviews who all agreed that the paper should be accepted (6, 6, 8, 8, 8). The reviewers in general found the method novel compared to existing literature, compelling and the results strong.
generated:This paper proposes a novel method for detecting out-of-distribution (OD) in graph models. The authors propose a simple method to solve this problem. The reviewers agreed that the paper is well-written and well-motivated.
original:The paper investigates the use of flow models for out-of-distribution detection. The paper proposes to use a combination of random projections in the latent space of flow models and one-sample / two-sample statistical tests for detecting OOD inputs. The authors present results on image benchmarks as well as non-image benchmarks. The reviewers found the approach well-motivated and appreciated the ablations. The authors did a good job of addressing reviewer concerns during the rebuttal. During the discussion phase, the consensus decision leaned towards acceptance. I recommend accept and encourage the reviewers to address any remaining concerns in the final version. It might be worth discussing this paper in the related work: Density
generated:This paper proposes a novel approach to the problem of learning models. The reviewers agreed that the paper is well-written and well-motivated, and the reviewers were in agreement with the reviewers' recommendation that this paper is a strong contribution to the community. The authors did a good job
original:This paper provides a very large-scale study on the pretraining of image recognition models. Specifically, three scaling factors (model sizes, dataset sizes, and training time) are extensively investigated. One important phenomenon observed by this paper is that stronger upstream accuracy may not necessarily contribute to stronger performance on downstream tasks---actually sometimes these two types of performance could even be at odds with each other Overall, all the reviewers enjoy reading this paper and highly appreciate the empirical results presented in this paper. There were only a few concerns raised by the reviewers but most were well addressed during the discussion period. All reviewers reach a consensus on accepting this paper and believe this study is worthy to be heard by the community.
generated:This paper proposes a novel approach to attack deep neural network models. The authors propose a method to encode a classifier (i. e. a latent representation of a state-specific representation of the sender and the sender’s point of mind). The authors show that the proposed approach
original:The authors provide an interesting improvement on privacy attacks in federated learning, demonstrating the ability to extract individual points even over large batches. While there were some concerns about the technical difficulty of the approach, reviewers were broadly in support of the work. As I tend to agree, this is an interesting strengthening beyond what it appears we were able to do before. This is yet another piece of evidence against the canard in FL that only sharing gradient updates provides privacy guarantees.
generated:This paper proposes a novel approach to learning graph representations in graph neural networks. The reviewers agree that the paper is well-written, well-motivated, and the empirical results are strong. The main concerns raised by reviewers are about the novelty of the paper and the novelty and novelty of
original:Heterophily is known to degrade the performance of graph neural networks. This paper explores whether, for graph convolutional networks (GCNs), this is a general phenomenon, or if there are some circumstances under which a GCN can still perform well in a heterophilous setting. This paper characterizes one such setting under a contextual stochastic block model (CSBM) distribution with two classes (generalized in the appendix to multiple classes). The main takeaway is that there are indeed scenarios where a GCN can be expected to perform well, even under heterophilic neighborhoods. There are limitations, and the reviewers have been fairly thorough in pointing these out: the analysis is
generated:This paper proposes a method to solve the problem of learning representation learning in RL. The authors propose a method that uses a gradient-based learning method that achieves good performance in deep learning. The proposed method is based on a neural network, which can be used to learn representation models. The
original:This paper proposes augmenting standard forward prediction techniques used for representation learning with backward prediction as well, termed "learning via retracing". The paper implements this idea in a Cycle-Consistency World Model (CCWM) and demonstrates that CCWM improves performance of a Dreamer agent across a number of Control Suite tasks. The paper also proposes a way to detect "irreversible" transitions and exclude them from the backwards prediction step. This paper generated mixed opinions, and the reviewers did not come to a consensus on whether it should be accepted or rejected. In particular, Reviewer VSAG maintained it should be accepted, while Reviewer NEVM maintained it should be rejected. The
generated:This paper proposes a method to solve the problem of detecting and re-supervised vision and vision. The authors propose a method that uses a neural network to learn the representation of a classifier. The paper is well-written, well-motivated, and the proposed method is novel
original:This paper proposes a method to solve the inverse problem of identifying parameters of a dynamic physical system from image observations. The main idea is to train a rendering-invariant state-prediction (RISP), which estimates the inverse mapping from the pixel to the state domain. The authors introduce a new loss to this end, and an efficient gradient computation of the loss. The paper received three clear accept recommendations. The reviewers discussed the potential improvement of RISP when combined to disentanglement methods, and also raise several concerns regarding experiments, e. g. rendering conditions during training and testing, or evaluation on real data. The rebuttal did a good job in answering reviewers' concerns
generated:This paper proposes a method for using a simple method to learn to learn by using a variational method. The method is novel, and the empirical results are well-written, well-motivated, and well-developed. The reviewers have raised some concerns about the novelty of the method
original:The paper provides a neat idea about explaining (linear) predictors based on designing ways of perturbing parameters. It is focused on linear models (which can still lead to non-linear classifiers), but it is a relevant case, particularly for explainability.
generated:This paper proposes a method for using a simple method to solve the problem of fine-tuning deep learning. The method is novel and novel, and is well-motivated, and well-written.
original:This paper examines the extent to which a large language model (LM) can generalize to unseen tasks via "instruction tuning", a process that fine-tunes the LM on a large number of tasks with natural language instructions. At test time, the model is evaluated zero-shot on held out tasks. The empirical results are good, and the 137B FLAN model generally out performs the 175B untuned GPT-3 model. All reviewers voted to accept with uniformly high scores, despite two commenting on the relative lack of novelty. The discussion period focused on questions raised by two reviewers regarding the usefulness of fine-tuning with instructions vs. multi-task fine-tuning
generated:This paper proposes a novel method for using a non-supervised neural network model to learn the classifier. The authors propose a method that achieves good performance in the real-world. The paper is well written and well-motivated, and the reviewers agree that the paper is interesting and
original:PAPER: This paper introduces a new method to learn joint representations from multimodal data, with potentially missing data. The primary novelty builds from the idea of semi-supervised VAE, introducing the concept of bi-directional information flow, which is termed “mutual supervision”. This approach brings the same advantages of semi-supervised VAE to the multimodal setting, allowing the cross-modal interactions to be modeled in the latent space. DISCUSSION: The discussion brought many important issues, addressed by both reviewers and authors. In general, it seems that most reviewers appreciate the technical novelty of the paper, related to the mutual supervision.
generated:This paper proposes a new method for sampling graph classification. The authors propose a method that is relatively simple and novel, and the empirical results show that the proposed method is well-motivated and well-developed.
original:The authors proposed an algorithm for sampling DAGs that is suited for continuous optimization. The sampling algorithm has two main steps: In the first step, a causal order over the variables is selected. In the second step, edges are sampled based on the selected order. Moreover, based on this algorithm, they proposed a method in order to learn the causal structure from the observational data. The causal structure learning algorithm is guaranteed to output a DAG at any time and it is not required any pre- or post-processing unlike previous work. There were concerns by two reviewers on the slight lack of novelty ("the proposed method of this paper is only a combination of well-developed techniques") but I
generated:This paper proposes a method to improve the robustness of deep neural network models. The authors propose a novel approach to optimize the adversarial attack function of neural networks. The reviewers agree that the paper is well-written, well-motivated, and the empirical results show that the proposed
original:In this paper, the authors enhance the adversarial transferability of vision transformers by introducing two novel strategies specific to the architecture of ViT models: Self-Ensemble and Token Refinement method. Comprehensive experiments on various models (including CNN's and ViT's variants) and tasks (classification, detection, and segmentation) successfully verify the effectiveness of the proposed method. In general, the problem studied is relevant and important. The paper is well-written and well-motivated with empirical findings. The proposed two strategies are novel, simple to implement, and effective in practice. Following the author's response and discussion, the average score increases from 6 to 7.5, with
generated:This paper proposes a method to solve the problem of non-convex, non-supervised learning in deep learning. The method is novel, and the reviewers agree that the paper is well-written and well-motivated. The reviewers have raised some concerns about the novelty of the
original:This paper studies model-based RL in the setting where the model can be misspecified. In this case, MLE of model parameters is a not necessarily a good idea because the error in the model estimate compounds when the model is used for planning. The authors solve this problem by optimizing a novel objective, which takes the quality of the next state prediction into account. This paper studies an important problem and this was recognized by all reviewers. Its initial reviews were positive and improved to 8, 8, 6, and 6 after the rebuttal. The rebuttal was comprehensive and exemplary. For instance, one concern of this paper was limited empirical evaluation. The authors added 5 new benchmarks and also
generated:This paper proposes a new method for using a neural network-based method to solve the problem of optimization in deep learning. The paper is well-written and well-motivated, and is a good contribution to the field of neural networks, which should be of interest to the community of
original:The paper investigates the neural tangent kernel NTK of infinitely wide ensembles of soft trees having a particular soft decision functions in their internal nodes. A closed form of the NTK is presented as well as a result bounding the changes of the NTK during training. Implications for practical training procedures are briefly discussed and some experiments are also reported. The review and discussion phase were difficult with two rather uninformative but positive reviews and a negative but detailed review. The latter had, however, some serious flaws. For these reasons I carefully read the paper on my own, too. In turned out that the criticized flaws in the presentation mentioned by the negative reviewer are baseless as long as
generated:This paper proposes a novel method for learning a neural network to learn the latent structure of a classifier. The authors propose a method that uses neural network (GNN) based on neural network models that can be trained on a large number of datasets. The method is evaluated on a set
original:This paper builds on the success of the FermiNet neural wave function framework by pairing it with a graph neural network which predicts the parameters of neural wave function from the geometry. The resulting PESNet trains significantly faster, with no loss of accuracy. This method constitutes an important advance in ML-powered quantum mechanical calculations. The reviewers unanimously recommend acceptance.
generated:This paper proposes a novel method to solve the problem of optimizing deep learning. The reviewers agree that the paper is well-motivated, and the empirical results are interesting, and that the proposed method is novel, and it is a nice contribution to the field of neural network learning.
original:This paper studies the dependency of SGD convergence on order of examples. The main observation of the paper is: if the averages of consecutive stochastic gradients converge faster to the full gradient, then the SGD with the corresponding sampling strategy will have a faster convergence rate. For different sampling strategies, sampling with replacement has slower convergence in stochastic gradient, where sampling without replacement can converge faster. The paper also proposes two new algorithms that can improve convergence rates in some interesting settings. The reviewers find the analysis clean and the new algorithms are interesting. There is some concern on the dependency on n or d for the faster rate, which should be discussed more clearly in the final version of the
generated:This paper proposes a method to solve the problem of learning to learn to predict the knowledge of new tasks. The paper is well-written and well-motivated, and the proposed method is novel.
original:All reviewers believe that this paper is valuable, and the authors have made a significant, careful contribution. Some suggestions from the area chair: - "in causality" is not a standard technical term and also not non-technical idiomatic English, so it should be explained the first time it is used. - The authors should briefly cite and discuss research on so-called positive and unlabeled (PU) learning. This seems like the special case where there is exactly one known class and one novel class. The distinction between sampling in causality and labeling in causality appears in the PU literature, though not under this name. - The authors could also mention the obvious but surprising point that
generated:This paper proposes a method to learn the optimal distance between the value of the latent distance between a set of data and the same set of samples. The proposed method is based on theoretically efficient variational variational descent. The paper is well-written, well-motivated,
original:This paper presents an analytic approach for estimating the optimal reverse variance schedule given a pre-trained score-based model. The experimental results demonstrated the efficacy of the proposed method on several datasets across different sampling budgets. Given the recent interest in score-based generative models, I believe that the paper will find applications in various domains. I am pleased to recommend it for acceptance.
generated:This paper proposes a new neural network model that achieves good performance on multiple benchmark benchmarks. The paper is well-written, well-motivated, and well motivated.
original:All reviewers agreed this was a very strong submission: it was clearly written, was theoretically and experimentally interesting, and had excellent motivation. A clear accept. Authors: you've already indicated that you've updated the submission to respond to reviewer changes, if you could double check their comments for any recommendation you may have missed on accident that would be great! The paper will make a great contribution to the conference!!
generated:This paper proposes a novel method for detecting images. The authors propose a method that uses a neural network to encode a classifier-based neural network model for image classification. The proposed method is based on a deep neural network (i. e. a class-based model). The method is
original:This paper proposes an elegant approach to object detection where an encoder network reads in an image and a decoder network outputs coordinate and category information via a sequence of textual tokens. This method does away with several object detection specific details and tricks such as region proposals and ROI pooling. The paper received positive reviews from all reviewers who agreed that this formulation of object detection was novel and provided a new perspective that may transfer to other computer vision tasks. One common concern amongst reviewers was the slow inference time due to the sequential nature of the decoder -- and this concern was a central point of discussion between the authors and reviewers. My takeaway from this discussion is that this model is certainly slower than traditional
generated:This paper proposes a method to learn a neural network to learn the sender and address the sender. The paper is well-written, well-motivated, and well-developed. The reviewers agree with the reviewers, and the authors have addressed the concerns raised by the reviewers. The authors
original:This paper proposes a novel representation for pose authoring, and was uniformly lauded by all reviewers. The AC concurs this paper is far above the threshold for acceptance at ICLR.
generated:This paper proposes a novel method for learning representations of convolution. The paper is well-written, the method is novel, and the empirical results are strong. The reviewers agree that the paper is a strong contribution to the literature. The authors did a good job of addressing an important problem
original:This papers studies the classical problem of relational learning from a probabilistic perspective. The authors propose four reasonable constraints to encode relational properties, and develop a PGM-based variational method for learning relational properties from data. After extensive discussion with the authors, a majority of the reviewer reviewers agree the approach is interesting, if not without some flaws. The problem studied is interesting, novel, and could lead to new developments in the area of relational learning. It is expected that the experiments have some limitations given the authors have approached the problem from a fresh new angle, which the reviewers have appreciated. Please pay attention to the suggestions from the reviewers, and in particular, please add a more detailed discussion
generated:This paper proposes a method to improve the robustness of deep learning models. The authors propose a novel method to solve the problem of non-supervised learning. The method is novel and novel, and the empirical results are interesting. The paper is well-written and well-motivated
original:This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets. The strengths of the paper are as follows: + In terms of significance of the topic, the problem tackled in the paper is significant and highly relevant. + The motivation of using model ensemble is clearly illustrated via a figure and well justified with theoretical analysis. + Algorithmically, the paper proposes Adaptive Sampling and K
generated:This paper proposes a novel method for learning an adversarial model. The authors propose a method to learn a model that learns an agent by learning an agent from an agent that is not the agent. The method is novel, and is well-written, and well-motivated, and
original:The submitted paper considers the very interesting problem of imitation learning from observations under transition model disparity. The reviewers recommended 2x weak accept and 1x weak reject for the paper. Main concerns about the paper regarded clarity of the presentation, complicatedness of the proposed method, and experimental validation. During the discussion phase, the authors addressed some of the comments and provided an update of the paper providing additional details. While some of the reviewers' concerns still stand, I think the addressed problem is very relevant and the proposed method can be (with clarifications and improvements of the presentation) be interesting to parts of the community. Hence I am recommending acceptance of the paper. Nevertheless, I strongly urge the authors to
generated:This paper proposes a method to learn representation learning by using a neural network based on a variational method. The authors propose to learn adversarial representation learning with a method that learns the representation from a set of variables. The method is evaluated on a number of benchmark datasets, and the reviewers
original:This paper proposes an extra loss to add on top of the contrastive learning. The contrastive learning seek representations invariant to transformation, while the extra loss the authors proposed encourage representations to be equivariant to the transformation (i. e. retain information about transformation in later representations). While reviewers and I agree this is a sensible motivation, and acknowledge good results that authors have obtained, the fact that most, if not all, improvement is combing from the 4-way rotation transformation is a bit unsatisfactory. Furthermore, this additional loss was proposed before and is actually quite well known, so the actual novelty in the proposed technique is somewhat limited. Nevertheless, this paper provides a comprehensive evaluation
generated:This paper is a strong paper that is well-written, well-motivated, and well motivated. The authors have addressed the concerns raised by the reviewers, and the reviewers were in agreement with the reviewers' comments. The reviewers were all positive about the paper, but raised some concerns
original:It can be prohibitively expensive to train a reinforcement learner from scratch &mdash; particularly in cases where experience is expensive to obtain, such as with a physical robot. So, we might hope to speed up RL in a couple of ways: first, by pre-training a representation that makes subsequent RL need less data; and second by running our RL on a cheaper proxy environment such as a simulator. For pre-training, we hope to be able to take advantage of available pre-collected data, and we hope to be able to use supervised learning or reconstruction tasks since they can be cheaper than RL. For either pre-training or a proxy environment, we have to
generated:This paper proposes a method to solve the problem of unsupervised reinforcement learning. The authors propose a method that uses a non-convex variational variational loss-based approach to solve this problem. In the rebuttal and discussion period, the authors have addressed the concerns raised
original:This paper makes significant advances in offline reinforcement learning by proposing a new approach of being pessimistic to deal with uncertainties in the offline data. The proposed approach uses bootstrapped Q-functions to quantify the uncertainty, which by itself is not new, and introduces additional data based on the pseudo-target that is penalized by the uncertainty quantification. The use of such additional data is the first of a kind, and the paper provides theoretical support for the case of linear MDP and empirical support with the D4RL benchmark. The reviewers had originally raised concerns or confusions regarding theoretical analysis and experiments. The authors have well responded to them, and no major concerns remain.
generated:This paper proposes a method for learning representation models for visual recognition. The reviewers agree that the proposed method is interesting, and the empirical results are strong, and it is well-written and well-motivated. In the final version of the paper, the authors show that the method is
original:The paper addresses the problem of generating images by combining visual components. These components are learned during pretraining, forming a dictionary of visual concpets which plays the role of text in DALLE. The technique is based on DALLE and slot attention approach to generate VQ codes in a way that is consistent. Reviewers had various concerns, including (1) that using synthetic images makes it easier to combine visual components' (2) that the novelty and relation to literature was not clear enough (3) missing ablations. The authors provided a detailed rebuttal which addressed reviewer concerns in a convincing way. One remaining issue of the paper is the writing. The paper fails to clearly
generated:This paper proposes a novel method for learning in deep neural networks. The authors propose a method to solve the problem of learning in graph-based learning. The paper is well-written, well-motivated, and the empirical results are convincing. The reviewers agree that the paper is interesting,
original:This paper tackles the neural contextual bandit problem, for which existing approaches consists rely on bandit algorithms based on deep neural networks to learn reward functions. In these existing strategies, exploration takes place over the entire network parameter space, which can be inefficient for the large-size networks typically used in NTK-based approaches. In this work, the authors address this by building on an existing technique of shallow exploration, which consists in exploring over the final layer of the network only, allowing to decouple the deep neural network feature representation learning from most of the exploration of the network parameters. More specifically, they propose a simple and effective UCB-based strategy using this shallow exploration scheme, for which
generated:This paper proposes a novel method to solve the problem of non-convex. The authors propose a method that uses graph-based convolutional neural networks that can be used to learn the state of the brain. The paper is well-written, well-motivated, and well
original:This paper studies the problem of motion prediction for multiple agents in a scene using transformer-based VAE like architecture. The paper received mixed reviews initially which generally tended towards borderline acceptance. All reviews appreciated extensive experiments but had some clarifications and requests for ablations. The authors provided a strong rebuttal that addressed many of the reviewers' concerns. The paper was discussed and all the reviewers updated their reviews in the post-rebuttal phase. Reviewers unanimously agree that the paper should be accepted. AC agrees with the reviewers and suggests strong acceptance. The authors are urged to incorporate reviewers' comments in the camera-ready.
generated:This paper proposes a method for learning a deep neural network to learn a task. The authors propose a method to learn an adversarial problem. The paper is well-written, well-motivated, and the empirical results are convincing. The reviewers have raised some concerns about the novelty of
original:This paper modifies the AlphaZero algorithm to generate proof tree size heuristics and shows empirical improvements over standard search algorithms. This is an interesting distinction that might lead to algorithms with distinct play styles and a deeper understanding of the games that we apply our agents to. The two positive reviewers felt that it was a solid contribution, worthy of publication. There were some questions regarding the clarity of the writing that were addressed in the discussion phase. The two reviewers that gave lower scores felt that the paper did not do a sufficient job motivating the work and distinguishing itself from the literature. Ultimately, I agree with the positive reviewers, and it is my opinion that the revised version is acceptable for publication.
generated:This paper proposes a novel method to solve the problem of generating a non-convex data model with a large number of variables. The empirical results of the paper are interesting, and the empirical results are interesting. The paper is well-written, well-motivated, and is
original:This paper extends recent and very active literature on analyzing learning algorithms in the simplified setting of Gaussian data and model weights, with the main generalization being to allow for non-isotropic covariance matrices. The main technical results seem to be correct and slightly novel, though reviewers feel they are not innovative or unexpected enough to stand on their own. However, the main contributions of the paper are then to interpret these results to give phenomenological results (regarding double descent, etc.), and reviewers were unanimously happy with these. In the end, all reviewers were positive about the paper. The largest reviewer criticisms of the paper were technical issues (ot31) and lack of context of recent literature
generated:This paper proposes a method to solve the problem of generalization in deep learning. The reviewers agree that the paper is well-written, well-motivated, and well-developed.
original:This paper introduces a convolution where the kernel is parametrised continuously over time (in the context of recurrent networks) to address vanishing gradients issues, by using another neural network to generate the kernels. This is a meaningful idea, addressing an important problem. The paper is well written and clear. The idea is novel (parametrised kernel already exist, but the way it's used here is new). The experimental section is solid, although some reviewers suggests it could be extended with more baselines. All reviewers recommend to accept the paper, therefore I also recommend accept.
generated:This paper proposes a novel method of learning. The reviewers agreed that the paper is well-written and well-motivated, and the empirical results are interesting.
original:The work presented in this study gives a theoretical finite-sample generalisation performance of stochastic gradient descent on linear models, for different batch-sizes and feature structures. This approach enable the authors to predict the training and test losses of neural networks on real data. While there were some parts that were initially mis-understood by some reviewers in the initial version of the papers, the extensive discussions between the authors and the reviewers led to several updates, both in the reference to prior work, but also in the presentation clarity. The wide impact and relevance to ICLR of this type of contribution made us recommend this work for acceptance at ICLR.
generated:This paper proposes a method for learning representation learning in RL. The paper is well-written and well-motivated, and the empirical results are strong.
original:This paper introduces a new transformer architecture for representation learning in RL. The key ingredients of the proposed architectures are a novel combination of existing methods: (1) the use of LSTMs to reduce the need for large transformers and (2) a contrastive learning procedure that doesn't require human data augmentation. The resulting approach requires less prior knowledge and provides higher sample efficiency. The paper is convincing, with comprehensive experiments on multiple challenging and well-known benchmarks and an ablation study. The reviewers did expressed concerns that parts of the paper are a very difficult read and could use improvement, especially those relying on substantial external background. The intuition behind several components could be improved, and there are
generated:This paper proposes a novel method for learning graph-based reinforcement learning in graph models. The authors propose a method to learn graph models with a large number of tasks. The paper is well-written, well-motivated, and the empirical results are convincing. The proposed method is novel
original:All reviewers found that the paper offers interesting contributions for multi-agent RL and favour acceptance of the paper. The strengths of the paper are summarized below: - Good algorithmic contribution - Offers a new set of benchmark tasks for coordination in MARL settings - Exhaustive experiments on complex tasks with a reasonable number of agents - All the issues raised by the reviewers (missing references, missing discussion of limitations...) have been satisfactorliy addressed. I therefore join the reviewers in the recommendation to accept the paper.
generated:This paper proposes a method for learning models that outperforms existing baselines. The method is based on a variational variational reinforcement learning method, which uses a non-convex variational method, and the method is shown to outperform other baselines, which can be used
original:The paper explores self-supervised learning on tabular data and proposes a novel augmentation method via corrupting a random subset of features. The idea is simple but effective. Experiments include 69 datasets and compare with a number of methods. The result shows its superiority. It would be inspiring more work for SSL on the tabular domain.
generated:This paper proposes a novel method for learning and learning in the domain of supervised learning. The authors propose a method to learn and learn in a new setting. The proposed method is well-written and well-motivated, and the empirical results are convincing. The reviewers were positive about the
original:This paper is proposed to address a novel but practical setting that the test set consists of both seen and unseen classes of the training set. To tackle the crucial challenge of distribution mismatch between the inlier and outlier features, the authors proposed a new method named ORCA by grouping similar instances to enlarge the class-wise margin for de-biasing. The experimental results on ImageNet have shown the proposed ORCA has significantly outperformed baselines in both inlier classification and outlier detection. The whole paper is written with clear logic and is easy to follow. Moreover, such a new setting may bring more inspiration to the community.
generated:This paper proposes a method for learning a neural network based on a gradient-based neural network (SLR) based on the neural network. The authors propose a method to learn the representation of a classifier. The proposed method is evaluated on a large number of tasks, which can be
original:The paper proposes a new recurrent architecture based on discretization of ODEs which allow for learning multi-scale representations and help with the vanishing gradient problem. The reviewers all agree this architecture is novel and provide substantial theoretical and empirical evidence. A strong accept.
generated:This paper proposes a novel approach to the problem of re-supervised reinforcement learning. The authors propose a new domain-based method for learning in the domain of image classification. The paper is well-written and well-motivated, and the empirical evaluation is thorough. The reviewers were
original:This paper presents U-WILDS, an extension of the multi-task, large-scale domain-shift dataset WILDS. The authors propose an extensive array of experiments evaluating the ability of a wide variety of algorithms to leverage the unlabelled data to address domain-shift problems. The vision behind sounds quite ambitious and convincing to me, namely, the proposed U-WILDS benchmark would be a useful and well-motivated resource for the ML community, and their experiments were very comprehensive. Although they did not introduce any new methods in this paper, U-WILDS significantly expands the range of modalities, applications, and shifts available for studying and benchmarking real
generated:This paper proposes a novel method to solve the problem of generating deep neural network models. The authors propose a method that uses a neural network to learn to predict the output of a set of tasks. The method is evaluated on a large number of benchmarks. The paper is well-written and
original:This paper addresses the problem of program synthesis given input/output examples and a domain-specific language using a bottom-up approach. The paper proposes the use of a neural architecture that exploits the search context (all the programs considered so far and their execution results) to decide which program to evaluate next. The model is trained on-policy using beam-aware training and the method is evaluated on string manipulation and inductive logic programming benchmarks. The results show that the proposed method outperforms previous work in terms of the number of programs evaluated and accuracy. Overall, the reviewers found the paper to be well-written and the idea proposed to be significantly novel and interesting to be presented at the conference and
generated:This paper proposes a novel method for learning a deep learning model. The authors propose to learn the feature embedding networks with a large number of features using a deep neural network with a small number of nodes, which can be used to learn a class-based learning method. The method is
original:The paper makes some novel and interesting observation pertaining the relationship between data heterogeneity and personalization. Reviewers like the paper and ideas in general but raised several concerns. The rebuttal rectified several confusions and provided more clarification which convinced the reviewers that the paper is above bar for publication.
generated:This paper proposes a novel method for using a deep neural network (GNN) to predict the output of a graph. The method is novel, and is well-written, and the empirical results are well-motivated. The authors show that the proposed method is well motivated, and
original:The paper proposed a novel one-pass efficient streaming algorithm for estimating the number of triangles and four cycles. The concerns raised by reviewers were nicely addressed in the rebuttal and all the reviewers agree that the paper is above bar for publication.
generated:This paper proposes a novel method of learning neural network models that can be used to solve the problem of the weakness of deep learning. The reviewers agree that the paper is well-written and well-motivated.
original:This paper experiments with what is required for a deep neural network to be similar to the visual activity in the ventral stream (as judged by the brainscore benchmark). The authors have several interesting contributions, such as showing that a small number of supervised updates are required to predict most of the variance in the brain activity, or that models with randomized synaptic weights can also predict a significant portion of the variance. These different points serve to better connect deep learning to important questions in neuroscience and the presence of the paper at ICLR would create good questions. The discussion between authors and reviewers resulted in a unanimous vote for acceptance, and the authors already made clarifications to the paper. I recommend acceptance.
generated:This paper proposes a novel method of learning learning models that is well-written, well-motivated, and well-developed. The authors have addressed some of the concerns raised by the reviewers in the rebuttal. The paper is a solid contribution to the field of deep learning, and
original:The authors propose a novel method for conditioning deep neural network. They replace the activation function with a linear combination of activation functions (e. g., ReLu). The weights for the activation functions are dynamically computed from the input during inference and training. The approach is evaluated on standard public tasks and shows improvement over well-established alternatives. Pros + A simple novel method for condition that is widely applicable + Adequate empirical evaluations to demonstrate it's effectiveness Cons - No major weakness The reviewers provided several feedback. The authors incorporated the suggestions and clarified residual concerns. The revised version of the paper has improved the readability and utility substantially.
generated:This paper proposes a method to solve the problem of detecting adversarial adversarial robustness in graph models.
original:In this work, authors use proxy distributions learned by advanced generative models to improve adversarial robustness. In the discussion period, authors did a good job in addressing reviewers' questions and comments. All reviewers think the paper is above the accept threshold, so do I.
generated:This paper proposes a new RL environment that uses a new representation of the physical space domain of the domain of GALR, which the authors propose to use to tackle the problem of learning the task of re-supervised reinforcement learning. The paper is well-written, well-mot
original:This paper introduces a new RL benchmark that is a simplified 2D version of Minecraft -- it is designed to support complex behaviors but reduce the training complexity. It is very well written and clear, positioned well with respect to other benchmarks, and is likely to improve the speed of development/testing of some RL algorithms. It is likely to appeal to a subset of the community and drive research in some cases, while others may prefer to stick with full 3D Minecraft. As such, there are some mixed reviews on the paper, with open questions as to whether it would be welcomed by people who work on Minecraft-style domains, whether behaviors learned in the simplified 2D environment would generalize to other
generated:This paper proposes a novel method to learn the task-specific weights in deep neural network. The reviewers agreed that the paper is well-written, well-motivated, and the empirical results are strong. The authors also show that the proposed method outperforms existing methods in the domain of
original:The paper proposes an approach to learn the task-specific weights in pretraining or mutli-task learning. It provides theoretical guarantees to the algorithm, as well as strong empirical results on several NLP problems. All the reviewers agreed that the work is interesting and the paper is well written. During the discussion period, the authors committed to address in the revised version (relatively minor) concerns raised by reviewers, including providing additional clarifications and additional comparisons to related methods. Overall, this is a strong paper that merits an acceptance.
generated:This paper proposes a novel method for solving the adversarial loss, which can be used to learn the optimal representation of the embedding loss. The authors show that it can be based on a non-convex representation. The paper is well-written, well-motivated,
original:The paper under review provides a theoretical analysis for contrastive representation learning. The paper proposes a guarantee on the performance (specifically upper and lower bounds) without resorting to previously used conditional independence assumptions. Throughout, the theoretical results and assumptions are supported by experiments. After a lively discussion, and after changes made to the paper in the revision stage, all four reviewers recommend this paper for acceptance. - Reviewer tWSB appreciates that the paper makes weaker assumptions than prior work (i. e., not assuming conditional independence), but raises a number of serious concerns on the theoretical results: The review questions whether assumption 4.6 used in the theory can be true, and whether the bound is
generated:This paper proposes a simple method to solve the problem of the generalization problem. The reviewers have raised some concerns about the novelty of the paper, which is well-written and well-motivated, and the empirical results of the authors' work. The main concerns raised by reviewers are
original:This paper offers a refinement of the information-theoretic characterization of the generalization of models obtained via SGD. This is assessed on some basic neural architectures and inspires the use of new regularizers. Overall, even though the perspective of this paper is not novel, the presented results appear to be clearer and tighter than prior instances of the same ideas. This was appreciated by most reviewers. The few clarity and organization concerns that were raised by the reviewers were adequately addressed by the authors. Overall, the paper deserves to be shared with the community.
generated:This paper proposes a simple method to solve the problem of theoretically non-convex dynamics with a large number of components. The authors show that the paper is well-motivated, well-written, and well-developed. The reviewers have a number of concerns about the
original:This paper introduces a differentiable yarn-level model of fabrics. The model is more detailed and physically realistic than proposed in earlier work, which may allow for applications to manufacturing guidance and textile design. The paper is generally well-written and contains detailed problem formulation and derivations. Experiments show it is possible to successfully learn a control policy and material parameters using the differentiable model.
generated:This paper proposes a novel method to solve the problem of learning representations of visual recognition. The paper is well-written, well-motivated, and well-developed. The reviewers agree that the paper is a nice contribution to the literature. The authors did a good job of addressing the
original:The paper introduces a novel control-based variational inference approach that learns latent dynamics in an *input-driven* state-space model. An optimal control solution (iLQR) is implicitly used as the recognition model which is fast and compact. Reviewers unanimously agree on the high quality writing and high significance of the work. This paper advances the horizon of nonlinear dynamical system models with unobserved input, an impactful contribution to the neuroscience and time series communities.
generated:This paper proposes a method to solve the problem of detecting the latent variables in image classification. The reviewers agreed that the paper is well-motivated, well-written, and the empirical results are strong. The authors did a good job of addressing the concerns raised by the reviewers.
original:Reviewers all found the work well-motivated in addressing uncertainty, a topic that has not seen much focus in meta-learning and few-shot learning. They describe the challenges well: small sample sizes and OOD shift. They then propose a solution they find works well empirically to overcome these challenges based on a set encoder and an energy function respectively. The proposal is largely one of engineering components that have been found to work well in the literature. I'm sympathetic to this style of research (particularly in today's neural network research), although the reviewers raise a primary concern about whether the choices leading to the proposal are justified. In particular, two Reviewers argue that there are no
generated:This paper proposes a novel method to learn the sampling density of a neural network to encode a classifier. The proposed method is based on a deep neural network (NN) based on deep neural networks, which can be used to predict the sampling distance from the input data. The method is
original:Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors' rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.
generated:This paper proposes a novel method to solve the problem of using graph models to predict the permutation of proteins. The authors propose a new method to learn the morphality of a set of graph-based structures. The method is evaluated on a large set of datasets, and the reviewers are
original:This paper introduces a novel SE(3) equivariant graph matching network, along with a keypoint discovery and alignment approach, for the problem of protein-protein docking, with a novel loss based on optimal transport. The overall consensus is that this is an impactful solution to an important problem, whereby competitive results are achieved without the need for templates, refinement, and are achieved with substantially faster run times.
generated:This paper proposes a method to solve the problem of generating graph-based graph models. The authors propose a method that uses a neural network-based neural network (SLR) based approach to generate graph functions. The method is evaluated on multiple benchmarks. The paper is well written, well
original:This paper proposes a spanning tree-based graph generation framework for molecular graph generation, which is an interesting problem. The tree-based approach is efficient and relatively effective in molecular graph generation tasks, and the empirical results are convincing. There were some concerns during the initial reviews, but all of them have been addressed during the discussion phase. Thus, I recommend this work be accepted.
generated:This paper proposes a novel method to solve the problem of detecting adversarial variables. The method is novel and novel, and the empirical results are interesting. The paper is well-written, well-motivated, and is well motivated.
original:The paper tackles the problem of detecting anomalies in multiple time-series. All the reviewers agreed that the methodology is novel, sound and very interesting. Initially, there were some concerns regarding the experimental evaluation, however, the rebuttal and subsequent discussion cleared up these concerns to some extent and all reviewers are eventually supporting or strongly supporting acceptance.
generated:This paper proposes a method for using a neural network (GNN) architecture that uses a simple gradient-based neural network. The reviewers agree that the paper is well-written, well-motivated, and is a good contribution to the community. The authors did a good job of
original:This paper presents a theoretical analysis of the approximation properties of linear recurrent encoder-decoder architectures, obtaining universal approximation results and subsequently showing approximation rates of targets for RNN encoder-decoders. It introduces a notion of "temporal products," which helps to characterize the types of temporal relationships that can be efficiently learned in this setting. Overall, the reviewers and I all agree that this paper makes important theoretical contributions to the important problem of the approximation capabilities of encoder-decoder architectures. The main weaknesses involve the rather simplified linear problem setup, but this limitation is easily forgiven in this first-of-its-kind rigorous analysis. I recommend acceptance.
generated:This paper proposes a novel method for detecting and detecting adversarial attacks on deep learning models. The authors propose a method to solve the problem of detecting and correcting weakness in deep learning. The reviewers agreed that this paper is well-written, well-motivated, and novel, and
original:The paper addresses two important aspects of deep learning: model transferability and authorization for use. It presents original solutions for both of these problems. All of the reviewers agree that the paper is a valuable contributions. Minor concerns and critical remarks have been addressed by the authors during the discussion.
generated:This paper proposes a novel method for detecting spurious features in neural network models.
original:The paper tackles the important problem of spurious feature detection in deep neural networks. Specifically, it proposes a framework to identify core and spurious features by investigating the activation maps with human supervision. Then, it produces an annotated version of the ImageNet dataset with core and spurious features, called Salient ImageNet, which is then used to empirically assess the robustness of the method against spurious training signals in comparison with current SOTA models. As pointed out by the reviewers, this work is not about causality and the definitions of causal and spurious features were originally vague and inaccurate. During the revision and discussion, the authors changed the terms "causal" features/accuracy to "core"
generated:This paper proposes a method to solve the problem of using adversarial RL-based RL models, which can be used as an empirical evaluation for the robustness of large language models. The method is novel and novel, and the proposed method is well-motivated. The authors propose a
original:This paper is a resource and numerical investigation into the variability of BERT checkpoints. It also provides a bootstrap method for making investigations on the checkpoints. All reviewers appreciate this contribution that can be expected to be used by the NLP community.
generated:This paper proposes a new representation for the latent representation of RL. The paper proposes an approach that uses the causal loss function (SLR) based on a combination of the latent loss function and the latent distance function to encode a causal loss representation that can be used to encode an ablation
original:This paper introduces a first-occupancy representation for reinforcement learning problems, with potential benefits on problems with non-stationary rewards. The representation is defined analogously to the successor representations, but captures the expected discounted time to first arrive at a state instead of measuring discounted visitations. The paper develops the idea and illustrates some uses for exploration, unsupervised RL, and non-stationary reward functions (for example when food rewards are consumed). The reviews brought forward a number of related older ideas in the literature, where several aspects of the method have been previously developed. These include dynamic goal learning, option conditional predictions, general value functions, dynamical distance learning, and temporal difference models.
