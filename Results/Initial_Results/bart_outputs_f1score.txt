generated:This paper proposes a novel method to solve the problem of spurious covariance in deep learning. The reviewers agree that this paper is well-motivated, well-written, and well motivated.
original:The paper studies how to build predictive models that are robust to nuisance-induced spurious correlations present in the data. It introduces nuisance-randomized distillation (NuRD), constructed by reweighting the observed data, to break the nuisance-label dependence and find the most informative representation to predict the label. Experiments on several datasets show that by using a classifier learned on this representation, NuRD is able to improve the classification performance by limiting the impact of nuisance variables. The main concerns were about the presentation and organization of the paper, which was heavily focused on the theoretical justifications but fell short in explaining the intuitions and implementation details. The revision and rebuttal have addressed some
generated:This paper proposes a method to solve the problem of optimization. The reviewers have raised some concerns about the proposed method. The authors have addressed the concerns in the final version of the paper. In the rebuttal, the reviewers have addressed some of the reviewers' concerns.
original:This paper argues that the widely adopted graph attention networks (GAT) have a shortcoming that with the static nature of the attention mechanism, they may fail to represent certain graphs. This paper presents an alternative, GATv2, a simple variant with the same time complexity as GAT but with more expressivity, able to represent the graphs that GAT fails to. This is shown both empirically and theoretically, with various tasks on synthetic as well as standard benchmark graphs. GATs are of high interest to the ICLR community, and this paper makes fundamental progress in how attention works in GNNs. This is one of the few papers that present both empirical and theoretical
generated:This paper proposes a novel method to solve the problem of learning in deep neural network models. The authors propose a method for learning on a neural network based on a gradient-based neural network. The proposed method is evaluated on a large number of tasks. The reviewers were all positive about the
original:This paper proposes two techniques for improving self-supervised learning with a vision transformer. The first improvement is using a multi-stage ViT, which is very similar to Swin transformer and authors recognized this is not a major contribution. The authors further found that using a multi-stage ViT does not produce discriminative patch representation, thus proposing the second improvement with a region level loss. While both improvements are not particularly novel by themselves, combining both leads to a strong empirical result. However, It does looks like the multi-scale vision transformer is the major improvement as removing the regional loss only leads to less than 1% decrease in performance in most cases. In general this is a
generated:This paper proposes a method for learning a neural network to learn a non-convex representation of a classifier. The method is based on neural networks, which can be learned by using RL. The paper is well-written and well-motivated, and the empirical results are strong.
original:A novel method is described that uses RL to search for a rule set which predicts multiple relations at once for KBC-like problems. The rules can include latent predicates, which reduces the complexity of individual rules, similar to Cropper & Muggleton's (2015) meta-interpretive learning framework, which is usual for rule-learning systems. Another novel aspect is use of a cache memory for rules. Pros - the idea of using RL instead of carefully-designed discrete search for symbolic learning systems is a very nice novel idea - the experimental results are strong Cons - the benchmarks are synthetic (although GraphLog does at least include noise) - although the Cropper and Muggleton
generated:This paper proposes a novel method to solve the problem of embedding convolutional neural network models on a graph-based neural network.
original:The submission develops a rotationally equivariant scattering transform on the sphere. Many developments in deep learning make use of spherical representations, and the development of a rotationally equivariant scattering transform is an important if not unexpected development. The reviews are split with half of the reviewers believing it to be slightly above the threshold for acceptance, and half believe it to be slightly below the threshold for acceptance. In the papers favor, it solves an important case of the scattering transform framework, which has been demonstrated to be important in diverse machine learning applications such as learning with small data sets, differentially private learning, and network initialization. As such, continued fundamental development in this area is valuable, especially in
generated:This paper proposes a novel method for learning an adversarial model. The authors propose a method to learn a model that learns an agent by learning an agent from an agent that is not the agent. The method is novel, and is well-written, and well-motivated, and
original:The submitted paper considers the very interesting problem of imitation learning from observations under transition model disparity. The reviewers recommended 2x weak accept and 1x weak reject for the paper. Main concerns about the paper regarded clarity of the presentation, complicatedness of the proposed method, and experimental validation. During the discussion phase, the authors addressed some of the comments and provided an update of the paper providing additional details. While some of the reviewers' concerns still stand, I think the addressed problem is very relevant and the proposed method can be (with clarifications and improvements of the presentation) be interesting to parts of the community. Hence I am recommending acceptance of the paper. Nevertheless, I strongly urge the authors to
generated:This paper proposes a novel approach to the problem of generalization in deep learning. The authors propose a new method for detecting the weakness of deep neural network models that can be used to solve the problem. The paper is well written, well-motivated and well-written, and
original:The paper provides a thorough study of the evolution of Hessian depending on a wide variety of aspects such as initialization, architectural choices, and common training heuristics. The paper makes a number of interesting observations. Some of them are not really new but overall, the experimental evaluation of the paper makes it a valuable resource for the community. The reviewers are overall quite positive. One reviewer notes that more investigation of the behavior of batch-normalization is required. I encourage the author to address this concern in the final manuscript. There is a lot of recent work on batch-normalization that might be worth discussing, e. g.: Training BatchNorm and Only BatchNorm: On the
generated:This paper proposes a method to solve the problem of deep learning in neural networks. The paper is well-written and well-motivated, and is a good contribution to the community. The reviewers agree that the paper should be accepted for publication.
original:This paper proposes Adam and Momentum optimizers, where the optimizer state variables are quantized to 8bit using block dynamics quantization. These modifications significantly improve the memory requirements of training models with many parameters (mainly, NLP models). These are useful contributions which will enable training even larger models than possible today. All reviewers were positive.
generated:This paper proposes a method for using a simple method to learn to learn by using a variational method. The method is novel, and the empirical results are well-written, well-motivated, and well-developed. The reviewers have raised some concerns about the novelty of the method
original:The paper provides a neat idea about explaining (linear) predictors based on designing ways of perturbing parameters. It is focused on linear models (which can still lead to non-linear classifiers), but it is a relevant case, particularly for explainability.
generated:This paper is a strong paper that is well-written, well-motivated, and well motivated. The authors have addressed the concerns raised by the reviewers, and the reviewers were in agreement with the reviewers' comments. The reviewers were all positive about the paper, but raised some concerns
original:It can be prohibitively expensive to train a reinforcement learner from scratch &mdash; particularly in cases where experience is expensive to obtain, such as with a physical robot. So, we might hope to speed up RL in a couple of ways: first, by pre-training a representation that makes subsequent RL need less data; and second by running our RL on a cheaper proxy environment such as a simulator. For pre-training, we hope to be able to take advantage of available pre-collected data, and we hope to be able to use supervised learning or reconstruction tasks since they can be cheaper than RL. For either pre-training or a proxy environment, we have to
generated:This paper proposes a method to solve an important problem. The reviewers agreed that the paper is well-written, well-motivated, and the empirical results are interesting, which should be a bit above the threshold for acceptance. In this paper, the authors propose a novel approach to the
original:The authors presents an alternative view of Neural ODEs, offering a novel understanding of depth in neural networks. The reviewers were overall impressed by the novelty and potential for insights this work brings. There was some disappointment that the empirical results were not stronger (both in terms of pure performance and computational cost) and that it wasn't clear how the theoretical insights into depth actually translated into a practical insight. Nevertheless, I agree with the reviewers that this is a good submission and would I think make for an interesting addition to the conference programme.
generated:This paper is a solid contribution to the conference phase of the conference. The paper is well-written, well-motivated, and is well motivated. The reviewers have addressed the concerns raised by the reviewers in the rebuttal. The authors did a good job of addressing the concerns of
original:This paper presents a comparison and analysis of continual learning methods for pretrained language models. The authors categorise continual learning methods into three categories, those that use cross task regularisation, those that employ some form of experience replay of previous training examples, and those that dynamically alter the network architecture for each task. Evaluation results from representative examples of these three paradigms are then presented and analysed. In general methods that incorporate experience reply appear to perform the best, while analysis of the predictive power of individual layers of the pretrained models suggests that some network layers are more robust to catastrophic forgetting than others, and that this also varies across architectures (BERT, ALBERT, etc.). In general
generated:This paper proposes a method for learning a representation learning approach for RL. The authors propose a method to improve the empirical performance of the RL algorithm, which is based on the RL approach, which can be used to learn a model. The paper is well-written, well-motivated
original:In this paper, the authors extend the FLAMBE to the infinite-horizon MDP and largely improved the sample complexity of the representation learning in FLAMBE. Meanwhile, the authors also consider the offline representation learning with the same framework. Although there is still some computational issue in MLE for the linear MDP, the paper completes a solid step towards making linear MDP for practice. The paper could be impactful for the RL community. As the reviewers suggested, there are still several minors to be addressed: - The extension of the proposed algorithm for finite-horizon MDP should be added. - The directly comparison between the sample complexity of FLAMBE and the proposed algorithm
generated:This paper proposes a method to solve the problem of using deep neural networks (GNNs) to learn the latent space. The reviewers agreed that the paper is well-written and well-motivated, and that the proposed method is novel, and the reviewers were in agreement with the
original:The paper proposes a simple method for uniform sampling from generative manifold using change of variables formula. The method works by first sampling a much larger number of samples (N) from uniform distribution in the latent space and then does sampling by replacement (using probability proportional to change in volume) to generate a smaller number of final samples (k << N) that are seen as approximately sampled from a uniform distribution from the generative manifold. Reviewers had some questions/concerns about the confusing language in the abstract and introduction around the use of the term "uniform" which the authors have addressed satisfactorily. Authors have also provided results on quality (FID metric) of the generated samples
generated:This paper proposes a novel method for learning a non-convex representation of a classifier. The proposed method is evaluated by the reviewers. The authors did a good job of addressing the concerns raised in the reviewers' reviews.
original:This paper introduces the concept of classifier orthogonalization. This is a generalization of orthogonality of linear classifiers (linear classifiers with orthogonal weights) to the non-linear setting. It introduces the notion of a full and principal classifier, where the full classifier is one that minimizes the empirical risk, and the principal classifier is one that uses only partial information. The orthogonalization procedure assumes that the input domain, X can be divided into two sets of latent random variables Z1 and Z2 via a bijective mapping. The random variables Z1 are the principal random variables, and Z2 contains all other information. Z
generated:This paper proposes a new framework for solving the problem of optimizing the efficiency of deep learning. The authors propose a method for solving this problem. The paper is well-written, well-motivated, and well-developed, and is a good contribution to the community. The reviewers
original:Overall the reviewers like the ideas in this paper. It calls out some of the issues with the current line of thinking in the ML/AI community. There were some concerns, but overall this paper offers a new way to think about, present, and question efficiency results. This could be quite infulential. I think this is interesting enough to warrent publicaiton.
generated:This paper proposes a method to learn to be in agreement with the state of the agent. The reviewers agree that the paper is well-written and well-motivated, and the reviewers were in agreement about the novelty of the method. The authors did a good job of addressing the problem of
original:This work proposed a method for encouraging an agent showing altruistic behaviour towards another agent (leader) without having access to the leader's reward function. The basic idea is based on the hypothesis that having the ability to reach many future states (i. e., called choice) is useful for the leader agent, no matter what it reward function is. The altruistic agent learns a policy that maximizes the choice of the leader agent. The paper defines three notions of choice, and evaluates them on four environments. The reviewers believe that this work attempts to solve an important problem, proposes a novel approach, and performs reasonably good experiments. The reviewers are all on the positive side at the end of the
generated:This paper proposes a novel method for sampling and sampling data. The method is novel, the method is interesting, and the empirical results are convincing. The paper is well-written, well-motivated, and is well motivated. The reviewers were all in agreement that the proposed method is
original:The paper studies the problem of multi-class calibration, proposing new notion of "top-label calibration", and presenting and comparing new algorithms for multi-class calibration. Reviewers generally found the paper to be well-written, and tackling a foundational problem. There were some questions regarding the experiments: (1) _Lack of explanation of why unnormalised beats normalised. _ One of the paper's main empirical findings is that using an unnormalised predictor with histogram binning (CW-HB) can significantly outperform a normalised one (N-HB). There is however limited discussion prior to this of why such behaviour is expected. (2) _Lack of
generated:This paper proposes an approach to solve the problem of weak reinforcement learning by using a neural network (NNS) based on a convolutional neural network. The method is based on the neural network architecture (NNR) and the neural networks (NNP) which can be used to
original:This paper tackles a very timely problem. Scores of 5,6,6,8 put it in the borderline region, but in the private discussion the more negative reviewer noted that they would also be OK with the paper being accepted. I therefore recommend acceptance. Going through the paper I missed any mention of available source code. I strongly recommend that the authors make code available; this would greatly increase the paper's impact.
generated:This paper proposes a novel method for detecting image images. The authors propose a method to learn a class-based classification model. The proposed method is based on a neural network based on an attention-based representation of the input data. The paper is well-written and well-motivated
original:The paper presents an approach to select visual tokens in images and reorganize them for the object classification, within Transformers. All four reviewers find the paper interesting and novel, and they are also very positive about the experimental results. The authors also addressed minor concerns of the reviewers successfully through the discussion phase, clarifying details and adding experiments. We recommend accepting the paper.
generated:This paper proposes a novel method for solving the problem of generalization. The paper is well written, well-written, and well-motivated.
original:Reviewers were in agreement but borderline. The paper has a nice hypothesis and develops the work using two realistic datasets, Wikipedia and Code. One reviewer was initially more negative but changed their views based on the authors improvements to the paper. The idea is fairly simple, but does require modellers come up with the structural features. There was discussion that more down-stream tasks are needed to highlight the approach. Moreover, more datasets should be experimented with. In all, experiments are good but improvement is easily done.
generated:This paper is well written, well-written, and well-motivated, and is well motivated. The authors have addressed the concerns raised by the reviewers in the reviewers' reviews.
original:This paper presents a method for inference in state-space models with non-linear dynamics and linear-Gaussian observations. Instead of parameterizing a generative model, the paper proposes to parameterize the conditional distribution of current latent states given previous latent states and observations using locally linear transitions, where the parameters of the linear mappings are given by neural networks. Under fairly standard conditionally-independence assumptions, the paper uses known Bayesian filtering/smoothing tricks to derive a recursive estimation algorithm and a parameter-estimation method based on a simple maximum likelihood objective. Overall, the reviewers found the idea to be novel and interesting and I agree. They also found the relation to the noise2
generated:This paper proposes a novel method to solve an important problem in deep learning. The reviewers agree that the paper is well-written and well-motivated, and the empirical results show that the proposed method outperforms existing methods. In particular, the reviewers agree with the reviewers that the reviewers
original:This paper presented a domain transportation perspective on optimizing recommender systems. The basic motivation is to view recommendation as applying some form of intervention, implying a distributional shift after the recommendation/intervention. Distribution shift brings tremendous difficulty to traditional causal inference or missing data theory perspective of recommender systems as it violates the distributional overlapping assumption: in simple terms, if the model recommends radically different set of items, there isn't much you can say about its generalization ability; on the other hand, if the model only recommends items that it already observed during training (no distribution shift at all), it would inherent all the biases which already exist in the data. To that end, this paper proposed a
generated:This paper is well-written, novel, and well-motivated, which is a bit above the threshold for acceptance. The authors did a good job of addressing the concerns raised by the reviewers, and the authors have addressed the reviewers' concerns in the rebuttal. The paper is
original:Initially, some reviewers have raised several points of criticism regarding certain aspects of the model whose novelty/significance was a bit unclear. After the rebuttal and the discussion phase, however, everyone agreed that most of these concerns could be addressed in a convincing way, and finally all reviewers were in favor of this paper. After carefully going over all the reviews, the rebuttal and the discussions, I fully agree with the reviewers and came to the conclusion that this paper indeed contains some interesting, novel and relevant contributions.
generated:This paper proposes a method for using a neural network (GNN) architecture that uses a simple gradient-based neural network. The reviewers agree that the paper is well-written, well-motivated, and is a good contribution to the community. The authors did a good job of
original:This paper presents a theoretical analysis of the approximation properties of linear recurrent encoder-decoder architectures, obtaining universal approximation results and subsequently showing approximation rates of targets for RNN encoder-decoders. It introduces a notion of "temporal products," which helps to characterize the types of temporal relationships that can be efficiently learned in this setting. Overall, the reviewers and I all agree that this paper makes important theoretical contributions to the important problem of the approximation capabilities of encoder-decoder architectures. The main weaknesses involve the rather simplified linear problem setup, but this limitation is easily forgiven in this first-of-its-kind rigorous analysis. I recommend acceptance.
generated:This paper proposes a novel method to solve the problem of learning in deep learning. The authors propose a method that uses continuous reinforcement learning to solve this problem. The paper is well-written, well-motivated, and the empirical results are strong. The reviewers agree that the paper is interesting
original:The paper sheds light on issues with BN in continual learning and proposes a quite simple, which is a strength, solution to fix it. The Authors first draw attention to the fact that using recalculated moments boosts performance and reduces forgetting, which serves as an argument that at least partially BN contributes to catastrophic forgetting in continual learning. Given that BN remains quite important in certain application areas such as vision, it is a strong motivation for the paper. The experiments are thorough and clearly show that CN is a practically relevant alternative to BN in continual learning. One weakness of the paper is that the method is poorly motivated, and relatedly, it has quite limited novelty. CN combines the strengths
generated:This paper proposes a method to solve the problem of deep learning in deep neural network models.
original:The authors propose a multi-resolution pyramidal attention mechanism to capture long-range dependencies in time series forecasting, achieving linear time and space complexity. The authors conduced an extensive set of experiments and ablation studies demonstrating that the proposed method consistently outperforms the state-of-the-art and provided evidence for the various components of the architecture. They also provided a proof guarantee the linear complexity of long sequence encoding and adequately addressed the concerns raised by the reviewers. The additional additional benchmarks conducted by the author further demonstrated the strong performance of the method. All reviewers agreed that this work makes a solid contribution to the field.
generated:This paper proposes a novel method for using neural network models to solve an important problem in deep neural network optimization. The authors propose a novel approach to solve the problem of gradient descent. The paper is well-written, well-motivated, and is a nice contribution to the community of
original:This paper proposes an algorithmic approach to estimating upper and lower bounds of the rate-distortion (R-D) function of a data source on the basis of samples drawn from it. The proposed upper bound is based on the variational objective employed in the Blahut-Arimoto algorithm, whereas the proposed lower bound is based on the dual characterization of the R-D function. In both bounds neural networks trained with samples are utilized. Experimental results on four sources (Gaussian, banana-shaped, GAN-generated images, and natural images) are provided. The four review scores were initially two positives and two negatives. Some reviewers evaluated positively the argument on the lower bound
generated:This paper proposes a novel method for using deep learning to solve the problem of generating graph-based representations of images. The method is based on a large set of graph representations. The paper is well-written, well-motivated, and is novel, which is a bit of a
original:The paper describes an approach for automatically generating CAD sketches, including both the primitives that describe the drawing, as well as the constraints that describe relationships between the primitives that need to be maintained even if the primitives are changed. This is an important problem that is starting to receive a lot of attention from the literature. Overall, the paper is very well executed and the results are quite compelling. There were some concerns about the relationship with the work by Willis et al. and other papers that were published around the time when this paper was submitted. There is still some novelty in this paper relative to those works as argued in appendix H, but it would have been really good to have a more
generated:This paper proposes a method to solve the problem of unsupervised reinforcement learning. The authors show that the method is well-motivated, well-written, and well-developed. The method is novel, and the empirical results are interesting. The paper provides an interesting analysis of the
original:This work concerns Automatic Music Transcription (AMT) -- transcribing notes given the audio of the music. The paper demonstrates that a single general-purpose transformer model can perform AMT for many instruments across several different transcription datasets. The method represents the first unified AMT model that can transcribe music audio with an arbitrary number of instruments. All reviewers rated this paper highly and are excited about seeing it at the conference. One reviewer noted that "This paper seems to be a great milestone in the AMT research. It is probably the first unified AMT model that can take music audio with an arbitrary number of instruments." The reviewers had some suggestions and comments, which appear to be addressed by
generated:This paper proposes a new framework for using emergent languages. The reviewers agreed that the paper is well-written, and well-motivated, and the reviewers did a good job of addressing the concerns raised by the reviewers.
original:This paper explores ways in which *emergent communication* (EC) methods from representation learning can be evaluated extrinsically, by hooking them into downstream NLP tasks. Reviewers agree that the paper is thorough, and finds encouraging results. This paper is borderline, and difficult to evaluate, even after very substantial discussion (some of it private). From my reading of the reviews and pieces of the paper, I'm very sympathetic to wvqW's concern that none of the present-day applications under study seem likely to benefit from this kind of emergent communication pretraining: *Natural* language pretraining, even transferring across natural languages, is for too strong a baseline,
generated:This paper proposes a simple method to solve the stochastic optimization problem of deep neural network (NN) models. The paper is well-written, well-motivated, and well-developed.
original:This is a solid paper and considers the problem of training a wide neural network with a single hidden layer. This can be framed as an optimization problem in the space of probability distributions with a suitable entropy regularization, where each atom in the distribution corresponds to a hidden neuron. The dual of this problem (for finite data) is a finite-dimensional optimization problem and the paper proposes a particle based coordinate ascent scheme. The paper provides some convergence rate results. After the rebuttal, the authors have also included more experimental/numerical results. The authors have answered the concerns raised by the reviewers and overall, the paper can be accepted: The presented approach appears to be sufficiently novel and might be
generated:This paper proposes a novel method for solving an important problem, and is well-written, well-motivated, and well-developed. The authors did a good job of addressing the problem of covariance in the paper, and the reviewers agree that the paper should be accepted for publication
original:This paper is a solid contribution to researchers in this field, as it provides a new idea for the basic problem of determining the direction of causality between two variables, using the functional causal model as a dynamical system and optimal transport.
generated:This paper proposes a method to solve the problem of solving the domain domain domain problem. The method is based on a neural network (NN) and is evaluated on several benchmarks. The paper is well written and the empirical results are strong.
original:The paper presents a variant of sliced wasserstein distance, where the slicing operation is performed with a neural network. The resulting distance is studied and experiments on synthetic data and as cost in generative modeling are performed. While the idea of the paper is not that novel, the work is overall well executed. Reviewers agreed that the paper is borderline weak accept. Accept as a poster.
generated:This paper proposes a method to solve the problem of adversarial descent in GALR. The authors propose a method of embedding a classifier to a GAN-based transformers. The proposed method is evaluated on a large number of benchmarks. The paper is well-written and well-
original:This paper has a deep analysis of the over-smoothing phenomenon in BERT from the perspective of graph. Over-smoothing refers to token uniformity problem in BERT, different input patches mapping to similar latent representation in ViT and the problem of shallower representation better than deeper (overthinking). The authors build a relationship between Transformer blocks and graphs. Namely, self-attention matrix can be regarded as a normalized adjacency matrix of a weighted graph. They prove that if the standard deviation in layer normalization is sufficiently large, the outputs of the transformer stack will converge to a low-rank subspace, resulting in over-smoothing. In
generated:This paper proposes a method to solve the problem of unsupervised reinforcement learning. The paper is well written, well-motivated, and well-written, and the empirical results are convincing. The reviewers agreed that the paper is a solid contribution to the literature. The authors did a
original:The paper presents a comprehensive analysis of lottery tickets hypothesis (LTH) on automatic speech recognition. The authors verified the existence of highly sparse “winning tickets” in ASR task, and analyzed its robustness to noise, transferable to other datasets, and supported with structured sparsity. As agreed with the reviewers, the paper is well-written, the justification is thorough, and the finding that LTH does perform well on ASR is interesting. Though the novelty is marginal as it's a direct application of the LTH, this is the first investigation of LTH and brings new insights to the community. The decision is mainly based on the thorough justification of the LTH to
generated:This paper proposes a novel method to solve the problem of re-supervised reinforcement learning. The authors propose a method that achieves good results by using a new method that uses a variational method.
original:The paper proposes a new pseudometric, DARD, for comparing reward functions that avoid policy optimization. DARD builds on a recent work by Gleave et al. 2020 where the pseudometric EPIC was proposed. In contrast to EPIC, DARD operates on an approximate transition model and evaluates reward functions only on transitions close to their training distribution. Empirical experiments in different domains demonstrate the effectiveness of the proposed pseudometric. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers' questions and increased their overall assessment of the paper. At the end of the discussion phase
generated:This paper proposes a method to tackle adversarial attacks on deep learning models. The authors show that the proposed method outperforms previous methods. The paper is well-written, well-motivated, and well-developed. The reviewers have raised the score of the method to a high score
original:The paper proposes an approach and specific training algorithm to defend against membership inference attacks (MIA) in machine learning models. Existing MIA attacks are relatively simple and rely on the test loss distribution at the query point and therefore the proposed algorithm sets a positive target mean training loss value and applies gradient ascent if the average loss of current training batch is smaller than it (in addition to the standard gradient descent step). The submission gives extensive experimental results demonstrating advantage over existing defense methods on several benchmarks. The primary limitation of the work is that it defends only against rather naive existing attacks which do not examine the model (but rely only on the loss functions).
generated:This paper proposes a method for learning representations that outperforms the state-of-the-art representation learning architecture in the domain generalization problem. The paper is well-written and well-motivated, and the empirical results are interesting, which should be of interest to the community.
original:The paper aims at characterizing conditions for optimal representations required for the domain generalization problem under covariate shift. Under the Idealized Domain Generalization (IDG), the paper provides a variational characterization of the optimal representation and shows a number of intriguing results: (i) optimal representation should remain discriminative across domains, (ii) the representation’s marginal support needs to be the same across source and target. (ii) It is also shown that without any target information, no representation can do uniformly well over constant representation, thus supporting the necessity of target knowledge. Finally, the paper provides practical objectives of the proposed variational characterization by self-supervised learning using data-
generated:This paper proposes a novel method for solving the problem of stochastic reinforcement learning. The paper is well written and well written, and the empirical evaluation of the results is thorough and well motivated. The reviewers agree that the paper is a solid contribution to the field of RL.
original:All the reviewers think that the work is significant and new. Therefore, they support the paper to be published at ICLR 2022. Given the strong results and the “accept” consensus from the reviewers, I accept the paper as “spotlight”. The authors should implement all the reviewers’ suggestions into the final version.
generated:This paper proposes a novel method for learning unsupervised reinforcement learning in machine learning.
original:The paper investigates the use of equivariant neural network architectures for model-free reinforcement learning in the context of visuomotor robot manipulation tasks, exploiting rotational symmetries in an effort to improve sample efficiency. The paper first provides a formal definition and theoretical evaluation of a class of MDPs for which the reward and transition are invariant to group elements ("group-invariant MDPs"). It goes on to describe equivariant versions DQN, SAC, and learning from demonstration (LfD). Experiments on a set of different manipulation tasks reveal that the proposed architectures outperform contemporary baselines in terms of sample complexity and generalizability, while
generated:This paper proposes a method to learn the optimal representation of the neural network function in deep learning. The paper is well-written and well-motivated, and is a good contribution to the conference.
original:A conceptually and technically highly innovative paper which reinforces an existing powerful connection between the critical set of two-layer ReLU networks and suitable convex programs with cone constraints. The reviewers are in strong consensus that the paper is sound and has merits for publication.
generated:This paper proposes a method to learn representation learning by using a neural network based on a variational method. The authors propose to learn adversarial representation learning with a method that learns the representation from a set of variables. The method is evaluated on a number of benchmark datasets, and the reviewers
original:This paper proposes an extra loss to add on top of the contrastive learning. The contrastive learning seek representations invariant to transformation, while the extra loss the authors proposed encourage representations to be equivariant to the transformation (i. e. retain information about transformation in later representations). While reviewers and I agree this is a sensible motivation, and acknowledge good results that authors have obtained, the fact that most, if not all, improvement is combing from the 4-way rotation transformation is a bit unsatisfactory. Furthermore, this additional loss was proposed before and is actually quite well known, so the actual novelty in the proposed technique is somewhat limited. Nevertheless, this paper provides a comprehensive evaluation
generated:This paper proposes a method to solve the adversarial adversarial robustness problem in deep neural network models. The reviewers have raised some concerns about the paper. The authors have addressed the concerns raised by the reviewers in the rebuttal.
original:In this manuscript, the authors study the relatively unexplored problem of how to characterize and assess the adversarial vulnerability of classification models with categorical input. Even certifying the robustness of such classification models is intrinsically an NP-hard combinatorial problem, the authors show that the robustness certification can be solved via an efficient greedy exploration of the discrete attack space for any measurable classifiers with a mild smoothness constraint. Overall, the theoretical analyses in this paper are rigorous, and reviewers seem to be satisfied with the responses from the authors. Based on the three positive reviewers, this manuscript is recommended to be accepted.
generated:This paper proposes a novel method for learning graph models. The method is novel and novel, and the empirical results show that the proposed method is well-motivated and well-developed.
original:The authors propose a method called Hybrid Memoised Wake-Sleep (HMWS) for training models with both discrete and continuous latent variables efficiently using amortized inference. They extend Memoised Wake-Sleep (MWS), which can only handle discrete latent variables, to discrete-continuous systems by using importance sampling to approximately marginalize out the continuous variables and then applying MWS to the discrete variables. This is well motivated and well written paper. The method is novel, clearly described, and evaluated in two fairly different interesting settings. However, while the empirical evaluation was considerably strengthened by the ablation studies and other experiments included in response to the reviewers, it is still on the
generated:This paper proposes a simple method to solve the problem of the generalization problem. The reviewers have raised some concerns about the novelty of the paper, which is well-written and well-motivated, and the empirical results of the authors' work. The main concerns raised by reviewers are
original:This paper offers a refinement of the information-theoretic characterization of the generalization of models obtained via SGD. This is assessed on some basic neural architectures and inspires the use of new regularizers. Overall, even though the perspective of this paper is not novel, the presented results appear to be clearer and tighter than prior instances of the same ideas. This was appreciated by most reviewers. The few clarity and organization concerns that were raised by the reviewers were adequately addressed by the authors. Overall, the paper deserves to be shared with the community.
generated:This paper proposes a novel method to solve the problem of reducing the complexity of deep learning in deep learning. The authors propose a method to improve the efficiency of learning in RL. The paper is well-written, well-motivated, and well-developed. The reviewers agree that the
original:The paper shows that most variance of gradients used in FL and distributed learning in general is in very low rank subspaces, an observation also made in Konecny et al 2016 and some other related works in deep learning, though sometimes for a different purpose. The paper then proposes lightweight updates combining a fresh gradient with old updates. Experiments and a theoretical convergence guarantee complement the results, which are mostly convincing. The experiments compare against ATOMO but strangely not against the more common PowerSGD, which would also work with partial client participation. Overall, reviewers all agreed that the paper is interesting, well-motivated and deserves acceptance. We hope the authors will incorporate the open
generated:This paper proposes a method for using a graph neural network (GAN) to solve graph-based reinforcement learning problems. The authors propose a method to solve the problem of generating visual images. The paper is well-written and well-motivated, and the empirical results are interesting.
original:The authors provide a convexification for the GAN training via integral probability metrics induced by two-layer neural networks. The exposition relies on the convexification tools recently proposed by the Pilanci et al., and provides interesting insights to follow up in the future.
generated:This paper proposes a novel method for detecting the causalness of visual representations in visual images. The reviewers agreed that the paper is well-written and well-motivated, and the reviewers were in agreement with the reviewers' comments. The authors did a good job of addressing the concerns raised
original:The authors propose a normalization method for cross-lingual text representations. The goal is to normalize the monolingual embeddings based on spectral normalization. The study shows that produced text representations keep their meaning and improve performance on downstream tasks. There is a disagreement among the reviewers. The main concern is whether the main contribution is an empirical study or a novel idea. I think the authors well-addressed the concerns of most reviewers. The idea and empirical study are enough for publication for ICLR-2022.
generated:This paper proposes a novel method to solve the problem of generalization in deep neural networks. The method is based on the neural network-based neural network architecture, and the method is evaluated on a set of graph-based graphs that can be used to predict the performance of the proposed method
original:This is an intriguing work that introduces a novel sparse training technique. The core insight is a novel reparametrization or sparsity pattern based on the so-called butterfly matrices that enables fast training and good generalization. The theory is solid and useful. Most importantly, the method is novel and is likely to become impactful. Understanding better what contributes to the excellent performance is an interesting question for future work. In agreement with all the reviewers, it is my pleasure to accept the work.
generated:This paper proposes a novel method of learning learning models that is well-written, well-motivated, and well-developed. The authors have addressed some of the concerns raised by the reviewers in the rebuttal. The paper is a solid contribution to the field of deep learning, and
original:The authors propose a novel method for conditioning deep neural network. They replace the activation function with a linear combination of activation functions (e. g., ReLu). The weights for the activation functions are dynamically computed from the input during inference and training. The approach is evaluated on standard public tasks and shows improvement over well-established alternatives. Pros + A simple novel method for condition that is widely applicable + Adequate empirical evaluations to demonstrate it's effectiveness Cons - No major weakness The reviewers provided several feedback. The authors incorporated the suggestions and clarified residual concerns. The revised version of the paper has improved the readability and utility substantially.
generated:This paper proposes a novel method for solving the problem of deep learning in deep learning. The reviewers agreed that the paper is well written, well-motivated, and well-written. The authors did a good job of addressing the concerns raised by the reviewers, and the authors have addressed
original:The authors provide in this manuscript a theoretical analysis to explain why deep neural networks become linear in the neighbourhood of the initial optimisation point as their width tends to infinity. They approach this question by viewing the network as a multi-level assembly model. All reviewers agree that this is an interesting, novel, and relevant study. The paper is very well-written. Initially, a weak point raised by a reviewer was that an empirical evaluation of the theory was missing. The authors addressed this issue in a satisfactory manner in their response. In conclusion, this is a strong contribution worth publication.
generated:This paper proposes a novel method to solve the problem of generating graph generative models.
original:The paper argues that existing evaluation metrics for GGMs are insufficient and perform an extensive empirical study questioning their ability to measure the diversity and fidelity of the generated graphs. To solve these limitations, they propose a new evaluation metric that computes the Maximum Mean Discrepancy (MMD) between graph representations of the sampled and real graphs, as extracted from an untrained GGM model. All the reviewers agreed that the research problem is interesting and the overall idea behind the proposed metric is sound and novel. While there were some concerns regarding some details/comparisons/conclusions of the experimental evaluation, the rebuttal managed to cleared up these concerns and all the reviewers eventually supported acceptance.
generated:This paper proposes a method to learn a neural network to learn an important problem. The authors propose a method for learning a non-convex representation of a variable. The paper is well written, well-motivated, and the empirical results show the effectiveness of the method. The
original:Summary: This paper studies the neural contextual bandit problem, and proposes a neural-based bandit approach with a novel exploration strategy, called EE-Net. Besides utilizing a neural network (Exploitation network) to learn the reward function, EE-Net also uses another neural network (Exploration network) to adaptively learn potential gains compared to currently estimated reward. Discussions: The reviewers appreciated the novelty and the quality of the ideas and results in this paper. Most questions were about details in algorithm design choices and in the analysis. The authors have addressed these questions and updated their draft. The reviewers have now reached a consensus and recommend accepting this paper. Recommendation: Accept.
generated:This paper proposes a new neural network (GNN) for the problem of unsupervised reinforcement learning. The authors show that the proposed method outperforms the existing method. The reviewers agree that the paper is well-written and well-motivated.
original:This paper proposes a message passing neural network to solve PDEs. The paper has sound motivation, clear methodology, and extensive empirical study. However, on the other hand, some reviewers also raised their concerns, especially regarding the lack of clear notations and sufficient discussions on the difference between the proposed method and previous works. Furthermore, there is no ablation study and the generalization to multiple spatial resolution is not clearly explained. The authors did a very good job during the rebuttal period: many concerns/doubts/questions from the reviewers were successfully addressed and additional experiments have been performed to support the authors' answers. As a result, several reviewers decided to raise their scores,
generated:This paper proposes a novel method for re-supervised RL. The authors propose a method that uses a new representation of a task-specific goal-specific representation. The reviewers agree that the paper is well written and well written, and the empirical results are strong, and that the proposed
original:This paper proposes a method to improve the sample efficiency of the HER algorithm by sampling goals from a distribution that is learned from human demonstrations. Empirical results on a simulated robotic insertion task show that the proposed method enjoys a better sample efficiency compared to HER. The reviewers find the paper well-written overall and the proposed idea reasonable. However, there are concerns regarding the limited novelty of the proposed method, which seems incremental. Also, the empirical evaluation suffers from a lack of diversity. The considered tasks are virtually all equivalent to an insertion task. The paper would benefit from further empirical evaluations that include tasks such as those considered in the original HER paper.
generated:This paper proposes a novel method to solve the problem of solving the emergent problem of the embedding of graph convolutional networks (GNN) in graph networks.
original:This paper proposes a decomposition-based explanation method for graph neural networks. The motivation of this paper is that existing works based on approximation and perturbation suffer from various drawbacks. To address the challenges of existing works, the authors directly decompose the influence of node groups in the forward pass. The decomposition rules are designed for GCN and GAT. Further, to efficiently select subgraph groups from all possible combinations, the authors propose a greedy approach to search for maximally influential node sets. Experiments on synthetic and real-world datasets verify the improvements over existing works. During their initial responses, reviewers suggested that the authors experiment with more baselines and also clarify some of the technical
generated:This paper proposes a novel method for learning and re-supervised learning of deep neural network models. The method is based on a variational variational neural network, which uses a stochastic variational gradient function to learn and learn in the latent space. The authors propose a method
original:The paper was seen positively by all reviewers. The strength of the paper are: - Intuitive and interesting combination of Koopman Operators and Optimal Control for Reinforcement Learning - Convincing experiments on challenging benchmark tasks - All of the issues of the reviewers (advantages to SAC, gaps in the theory and missing references) have been properly addressed in the rebuttal. I therefore recommend acceptance of the paper.
generated:This paper proposes a novel approach to solve the problem of detecting the weakness of deep neural networks (GNNs) by using a gradient-based neural network (NNN) based on deep neural network models. The paper is well-written, well-motivated, and well
original:*Summary:* Investigate the NTK of PNNs and enhanced bias towards higher frequencies. *Strengths:* - Spectral bias is a contemporary topic. - Some reviewers found the paper well written. *Weaknesses:* - Restricted setting (two-layers / no bias / infinite width), particularly in view of the objective to provide architecture design guidance. Restricted experiments (Introduction indicates learning spherical harmonics). - Sparse discussion of related works, particularly on spectral bias. *Discussion:* During the discussion period authors made efforts to address some of the concerns of the reviewers. A late new experiment prompted KnZp to raise score. TQnp found
generated:This paper proposes a method to improve the robustness of deep neural networks. The authors propose a new method for learning a deep neural network. The paper is well-written and well-motivated, and the empirical results are impressive.
original:This paper is a follow-up paper of Zhang et al. (2021), that proposed a new network architecture for adversarial robustness, l_\infty distance net. Although the l_\infty network is provably 1-Lipschitz w. r. t. the l_\infty distance, its training procedure exploits the l_p relaxation to overcome the non-smoothness of the model but suffers from an unexpected large Lipschitz constant at the early training stage, an issue to be solved. This paper resolves this issue by a new loss design of scaled cross entropy loss and clipped hinge loss. Without using MLP on top of
generated:This paper proposes a method to solve the problem of missingness. The authors propose a method for re-supervised representation of image recognition.
original:This work identifies an interesting bias that can occur when applying occlusion based interpretability methods to debug image classifiers. For context, the motivation behind many of these methods is that by occluding various parts of the image, one can ask counterfactuals such as "what would the model have predicted if this object were not present in the image"? However, the authors note that when occluding pixels, classifiers are still functions of the occlusions themselves, so this process may introduce a bias as a result. This is most clearly demonstrated in Figure 2 where a convolutional architecture classifies various occluded images as "jigsaw" or "crossword puzzle
generated:This paper proposes a novel method for learning learning models. The method is novel, and the empirical results are a bit weak, and it is not clear that the proposed method is well-motivated and well-written. The reviewers agree that the paper should be accepted for publication. The
original:This work propose to learn hierarchical skill representations that, as opposed to prior work, consist of both discrete and continuous latent variables. Specifically, this work proposes to learn 3 level hierarchy via a hierarchical mixture latent variable model from offline data. For test time usage and adaptation on down-stream tasks, the manuscript proposes two ways of utilizing the learned hierarchy in RL settings. **Strengths** A novel method to learn hierarchical representations with mixed (discrete/continuous) latent variables is proposed Detailed experimental evaluation, and baseline comparisons, show promising results **Weaknesses** There were various clarity issues as pointed out by the reviewers (fixed in rebuttal phase) The related work was missing relevant work
generated:This paper proposes a novel approach to the problem of optimizing neural network models. The paper is well-written, well-motivated, and well-developed. The reviewers have raised some concerns about the paper, and the authors did a good job in the rebuttal. The authors
original:Verifying robustness of neural networks is an important application in machine learning. The submission takes on this challenge via the interval bound propagation (IBP) framework and provides a theoretical analysis on the training procedure. They establish, in the large network with case, that the certification via IBP reflects the robustness of the neural network. Despite the tensions between the changing architecture and the required accuracy, the results are insightful. The AC recommends the authors to revise the paper, correcting the significant amounts of typos and improve the presentation for its final version.
generated:This paper proposes a novel approach to the problem of optimizing graph-based reinforcement learning. The paper is well-written, well-motivated, and well-developed. The reviewers agreed that the paper should be accepted for publication. The authors did a good job in the rebuttal
original:The paper considers the effect of permutations in SGD - exploring the question of can we go beyond random permutations (which themselves have shown to be better than with replacement sampling)? The paper studies these questions from multiple viewpoints - showing that there is a one dimensional function for which the optimal permutation can be exponentially better in terms of rate than random. Further they show that for the general high dimensional the gap between random and optimal is non-existent. Further they study a Flip-Flop algorithm which flips the permutation every alternate epoch and for convex quadratics they show that this technique can lead to improved convergence rates for multiple base permutation schemes. Overall the results of the paper
generated:This paper proposes a method for learning learning models with a single-shot learning method. The authors propose a method that uses a large number of task-specific tasks. The method is evaluated on a number of benchmarks. The reviewers agreed that the paper is well-written, well-mot
original:This paper proposes a new method for domain generalization by adopting a single test example. Authors formulate the problem using a variational bayesian framework which ends up in an adaptation technique requiring a single feed-forward computation. The provided empirical results indicate that the proposed method has comparable performance to techniques which require more data. Reviewers all acknowledge the novelty and significance of this work. The paper is well-written and the related work is adequately discussed. Moreover, the proposed method is computationally efficient and empirical results provide strong evidence in its favor. While I am recommending acceptance, I tend to agree with reviewer xA1m about the main weaknesses of this work and I recommend authors to improve them for
generated:This paper proposes a novel method for supervised learning in deep neural network models. The method is based on a neural network model that uses graph-based embedding models, which can be used to solve the problem of learning a class-based representation of a class space. The proposed method is
original:The paper studies multi label classification problem. Particularly, they introduce multi-label box model, which uses probabilistic semantics of box embeddings, representing labels as boxes instead of vectors. Their model is evaluated extensively on 12 datasets, and reviewers agreed the paper was well written and well motivated. While it is pretty straightforward application of box embeddings to multi label problem, it is well motivated and the paper adds to the existing literature on box embeddings. Reviewer Eo7g had a concern with experimental setting, including missing a baseline Abboud et al. (2020). Even after the baseline was added, the reviewer was not convinced about the model’s performance
generated:This paper proposes a method for using a neural network to solve the problem of unsupervised neural network-based classification of images. The paper is well-written, well-motivated, and the empirical results are convincing.
original:This paper proposed MIDI-DDSP, a structured hierarchical generative model which offers both detailed expressive controls (as in traditional synthesizers) as well as the realistic audio quality (as in black-box neural audio synthesis). Overall the reviews are very positive. All the reviewers unanimously agree that the paper is very well-written and presented a very convincing model and a meaningful step-up from the earlier work of DDSP. The authors also presented a well-documented website for the project and promised to release the source code. The reviewers raised some clarifying questions and minor corrections which the authors addressed during the response. Therefore, I vote for accept.
generated:This paper proposes a novel approach to the problem of non-convex optimization. The reviewers agree that this paper is well-written, well-motivated, and novel, and is well motivated. The authors did a good job of addressing the concerns raised by the reviewers, and
original:The paper looks at subspace recovery in the presence of outliers, of which there have been many formulations. They study a recent formulation, DPCP, but relax the requirement that the dimension of the subspace is known -- obviously very important in practice. The approach is quite clever: they exploit the fact that for this non-convex problem, starting a simple algorithm at a randomly chosen starting point will converge to a local minimizer, and they can run an ensemble of these algorithms (each with different starting points) and be guaranteed the solutions will span an appropriate subspace. This idea alone is a nice contribution. The paper has theory and experiments. Most reviewers were positive about
generated:This paper proposes a novel method of learning. The method is based on a neural network-based neural network (SLR) learning architecture, which uses a general-task-based optimizer, which can be used to solve the problem of re-supervised learning. In the paper
original:### Description The paper demonstrates that efficient architectures such as transformers and MLP-mixers, which do not utilize translational equivariance in the design, when regularized with SAM (sharpness aware minimization) can achieve same or better performance as convolutional networks, in the vision problems where the convolutional networks were traditionally superior (with data augmentation or not, regularized or not). The paper demonstrates it very thoroughly through many experiments and analysis of the loss surfaces. ### Decision + Discussion I find the paper to be very timely in its context. It has a remarkable coverage of experimental studies and different use cases: SAM + augmentation, +contrastive, +
generated:This paper proposes a method to solve the problem of unsupervised reinforcement learning in deep learning.
original:Four reviewers have evaluated this submission with one score 6 and three scores 8. Overall, reviewers like the work and note that *a rigorous and principled approach is taken by this work*. AC agrees and advocates an accept.
generated:This paper proposes a new gradient-based neural network based on neural networks that can be used to predict the output of deep convolution. The paper is well-written, well-motivated, and the empirical results are convincing. The reviewers were positive about the novelty of the paper and
original:All three reviewers recommend acceptance. The paper introduces an interesting study and insights on the connection between local attention and dynamic depth-wise convolution, in terms of sparse connectivity, weight sharing, and dynamic weight. The reviews included questions such as the novelty over [Cordonnier et al 2020] and the connection to Multi-scale vision longformer, which were adequately addressed by the authors. The findings in this paper should be interesting to the ICLR community.
generated:This paper proposes a novel method for learning visual representations of images in image-based language. The authors propose a method to learn the representation of images from visual images. The paper is well-written, well-motivated, and the empirical results are convincing. The reviewers agree that the
original:This work proposes a new framework that can learn the object-centric representation for video. The authors did a good job during rebuttal and turned one slightly negative reviewer into positive ones. The final scores are 6,6,8,8. AC agrees that this work is very interesting and deserves to be published on ICLR. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are also encouraged to make other necessary changes.
generated:This paper proposes a novel method for learning models to learn to represent visual representations in visual language. The authors propose a method that learns to encode visual representations from visual representations. The proposed method is evaluated on a large number of benchmarks. The reviewers were all in agreement that the paper is a
original:Strengths: * Strong results across two benchmarks * Ablation study demonstrates importance of components * Provides improvements especially in low resource settings * Well-written paper Weaknesses: * Novelty of the method may be limited as previous works have explored structured outputs as intermediate plans * Not clear method will extend to other domains as decent AMR parses are required to train the imagination module, which might work well on the datasets used (e. g., RocStories), but wouldn't work in settings with more complex language
generated:This paper proposes a novel method for learning to predict the state of a classifier. The authors propose a new benchmark task that uses a deep neural network to learn to estimate the value of a set of inputs. The paper is well-written, well-motivated, and well-
original:This paper introduces the Filtered-CoPhy method, an approach for learning counterfactual reasoning of physical processes in pixel space. The approach enables forecasting raw videos over long horizons, without requiring strong supervision, e. g. object positions or scene properties. The paper initially received one strong accept, one weak accept, and one weak reject recommendations. The main reviewers' concerns relate to clarifications and consolidations in experiments, including stronger baselines, experiments on real data, or more diversity on the datasets. The rebuttal did a good job in answering reviewers' concerns, especially by providing new experimental results and analysis. Eventually, all reviewers recommended a clear acceptance after authors' feedback.
generated:This paper proposes a novel method for learning a neural network to learn the representation of a classifier. The paper is well written, well-motivated and well-written, and well motivated. The reviewers have raised some concerns about the paper. The authors did a good job of addressing
original:This paper proposes a feature selection method to identify features for downstream supervised tasks, focused on addressing challenges with sample scarcity and feature correlations. The proposed approach is highly motivating in biological and medical applications. Reviewers pointed out various strengths including potential high impacts in biomedical applications, technical novelty and significance, and comprehensive and illustrative experiments. The authors adequately addressed major concerns raised by reviewers.
generated:This paper proposes a novel method for learning adversarial reinforcement learning. The method is novel, and is well-motivated, and provides a new perspective on the domain-based learning problem. The authors have addressed the concerns raised by the reviewers in the rebuttal. The paper is well
original:This paper proposes asymmetry learning for learning counterfactual classifiers, i. e. classifiers which are invariant to certain symmetry transformations w. r. t. hidden variables that differ between the training and test sets. The reviewers universally agreed that the proposed setting, and theoretical contribution, were interesting and novel. They also praised the writing quality, but had some quibbles about the quality of the experiments, and discussion of prior work. Neither of these concerns were considered significant enough to be a barrier to acceptance, but the authors should try to improve them, if possible.
generated:This paper proposes an interesting and interesting analysis of neural networks that is well-written, well-motivated, and well-developed. The reviewers agree that this paper is a nice contribution to the field of ML.
original:This paper examined physics-inspired inductive biases in neural networks, in particular Hamiltonian and Lagrangian dynamics. The work separated the benefits arising from incorporating energy conservation, the symplectic bias, the coordinate systems, and second-order dynamics. Through a set of experiments, the paper showed the most important factor for improved performance in the test domains was the second-order dynamics, and not the more common explanation of energy conservation or the other factors. The increased generality of this approach was demonstrated with better predictions on Mujoco tasks that did not conserve energy. All reviewers liked the insights provided by the paper. They agreed that the paper clearly laid out several hypotheses and systematically tested them.
generated:This paper proposes a novel method for using deep learning to solve the problem of convexization in language languages. The authors propose a method to solve this problem by using graph-based learning. The paper is well-written, well-motivated and the empirical results are convincing. The
original:This paper studies the expressivity, complexity and unpredictability of emergent languages in referential games. The authors defined measures of complexity and unpredictability and empirically showed that the expressivity of emergent languages is a trade-off between the complexity and unpredictability of the context that the languages are used in. They introduced a contrastive loss based training method that alleviates the collapse of message types seen using standard referential loss functions. The paper is controversial among the reviewers. On the positive side, most liked how the paper has a clearly stated hypothesis and extensive evaluations which makes a clear contribution to the field of emergent languages. On the negative side, the paper only shows the
generated:This paper proposes a novel method for generalization to the latent variables of visual recognition. The paper is well-written, well-motivated, and well motivated, and is well motivated. The authors have addressed the concerns raised by the reviewers.
original:The paper sets up a complex algorithm for out-of-distribution generalization. The algorithm requires first, a generalization of identification results for variational autoencoders, the followed by second, a causal discovery subroutine, and third, learning an invariant predictor using the discovered causes. The procedure reads sound, and the results on common benchmarks look good, though I do not know how practical the approach would be in general.
generated:This paper proposes a novel method for detecting and detecting adversarial attacks on deep learning models. The authors propose a method to solve the problem of detecting and correcting weakness in deep learning. The reviewers agreed that this paper is well-written, well-motivated, and novel, and
original:The paper addresses two important aspects of deep learning: model transferability and authorization for use. It presents original solutions for both of these problems. All of the reviewers agree that the paper is a valuable contributions. Minor concerns and critical remarks have been addressed by the authors during the discussion.
generated:This paper proposes a novel method to learn graph representation learning in graph models. The authors propose a method that uses a stochastic reinforcement learning (SLR) architecture to learn the representation of graph representations. The proposed method is based on a neural network (GNN) based on the
original:This paper proposes a labeling trick for subgraph representation learning with GNNs. The proposed method, GLASS, improves on subgraph-level tasks. The topic of subgraph representation learning is relatively new, and this paper makes progress in that community which would be appreciated by other researchers interested in the same problem. The paper in the original submission state raised some concerns from the reviewers about unclear writing of the motivation and potential applications, technical novelty, and comparisons with existing approaches (even one that are not specifically designed for subgraph representation learning). It is good that the authors conducted additional experiments to show the effect of SSL (that the approach makes improvements without SSL). This and other clarifications from
generated:This paper proposes a novel approach to the problem of solving a non-convex RL problem. The reviewers agree that the paper is well-motivated, well-written, and well motivated.
original:This paper addresses the reward-free exploration problem with function approximation under linear mixture MDP assumption. The analysis shows that the proposed algorithm is (nearly) minimax optimal. The proposed approach can work with any planning solver to provide an ( ϵ+ϵopt )-optimal policy for any reward function. After reading the authors' feedback and discussing their concerns, the reviewers agree that the contributions in this paper are valuable and that this paper deserves publication. I encourage the authors to follow the reviewers' suggestions as they will prepare the camera-ready version.
generated:This paper proposes a simple method to solve the problem of fine-tuning the performance of multi-task re-modification. The authors propose to solve this problem by using a new method of re-supervised transition. The method is evaluated on a large number of tasks. The
original:This paper is about unsupervised translation between programming languages. The main positive is that it introduces the idea of using a form of unit test generation and execution behavior within a programming language back-translation setup, and it puts together together a number of pieces in an interesting way: text-to-text transformers, unit test generation, execution and code coverage. Results show a substantial improvement. The main weaknesses are that there are some caveats that need to be made, such as the (heuristic, not learned) way that test cases are translated across languages is not fully general, and that limits the applicability. There are also some cases where I find that the authors are stretching claims a
generated:This paper proposes a new neural network architecture that achieves good performance on multiple tasks. The authors propose a novel neural network based on a neural network model that learns the task-specific representations of inputs and outputs. The method is evaluated on multiple benchmarks. The paper proposes an approach that uses a
original:This paper proposes Perceiver IO, a general neural architecture that handles general purpose inputs and outputs. It operates directly in the raw input domains, and thus does away with modality specific architecture components. The paper contains extensive experiments showing the capabilities of this architecture in different domains. The paper received very positive reviews from all reviewers. Some concerns included a need for additional details such as a missing task from GLUE, FLOPs comparisons to past works, nomenclature for the versions of Perceiver IO, etc. These concerns were well addressed by the authors. Others concerns by reviewers were the lack of experiments in a multi task setting. However, it was acknowledged by the authors and reviewers that
generated:This paper proposes a method for using deep neural networks to learn. The method is based on deep neural network neural networks that can be used to learn learning models.
original:The paper contributes to the literature on federated learning by introducing a hybrid local SGD (HL-SGD) method. HL-SGD is motivated by the setups where edge devices are grouped into clusters with fast connections within the cluster, but slower connection between the devices and the server. HL-SGD uses hybrid updates: decentralized updates within the clusters and federated averaging steps between the clusters and the server. Initially, the reviews expressed concerns regarding comparison to prior work, empirical results, and privacy of the proposed scheme. However, the authors adequately addressed all of the concerns, added relevant discussions and results to the paper, and a consensus was reached that the paper should be accepted.
generated:This paper proposes a new neural network architecture that achieves good performance and robustness in graph models. The authors propose a novel approach to the problem of deep neural networks. The paper is well-written, well-motivated, and well motivated. The reviewers agree that the paper is a
original:The paper presents an empirical analysis of Vision Transformers - and in particular multi-headed self-attention - and ConvNets, with a focus on optimization-related properties (loss landscape, Hessian eigenvalues). The paper shows that both classes of models have their strengths and weaknesses and proposes a hybrid model that takes the best of both worlds and demonstrates good empirical performance. Reviewers are mostly very positive about the paper. Main pro is that analysis is important and this paper does a thorough job at it and draws some useful insights. There are several smaller issues with the presentation and the details of the content, but the authors did a good job addressing these in their responses. Overall, it
generated:This paper proposes a novel method for learning neural networks. The authors have addressed the concerns raised by reviewers.
original:In the paper, it introduces a forget-and-relearn framework to the iterative learning algorithm. It provides serval new insight that forgetting could be favorable to learning and validates the insights via image classification and language tasks. The idea is novel and inspiring. Although there are some debates on the experiment and the generality of the proposed method, I think authors answered those questions decently and many researchers would be interested in this direction.
generated:This paper proposes a novel method of learning to address the problem of solving a task. The reviewers agreed that the paper is well-written and well-motivated, and the empirical results show that the proposed method is interesting, and that the authors have addressed the concerns raised by the reviewers
original:This paper presents a hierarchical Bayesian approach to exploration in grid worlds. The paper considers the hypothesis that humans maintain a hierarchical representation when exploring a space, where the distribution over unknown space can be modeled with a structured probabilistic program. The paper compares the behavior of people during exploration tasks to the behavior of a Bayesian model under different distributional approximations. The results indicate that people can behave similarly to a sophisticated Bayesian model on small grid world domains. The reviews highlighted several concerns about the paper. One initial concern was that the experimental domain is too simple and small compared to real world environments encountered by robots or humans. However, this work is similar in scope to other exploration work
generated:This paper proposes a method for learning a deep neural network to learn a set of tasks. The authors show that the proposed method is well-motivated and well-developed. The method is evaluated on a large number of benchmarks. The reviewers have raised some concerns about the novelty of the
original:While one reviewer remained concerned about the possibility of convergence to bad equilibria and felt that the proposed method appears to be four minor changes from prior work (PAIRED), the authors demonstrate empirically that the proposed changes make a significant difference in their evaluation. Other reviewers were positive about this work and all others rated this work as an accept. Post rebuttal the most positive reviewer increased their score to an 8 and felt did a good job answering their concerns. They wanted to see an analysis of systems with larger numbers of agents, but felt that the current manuscript was more than sufficient to warrant acceptance, and fell into the category of a good paper with the additional ablations provided during the rebuttal
generated:This paper proposes a method to solve the problem of non-constrained RL. The authors propose a method that uses a stochastic approach to solve this problem.
original:All reviewers have converged to an unanimous rating of the paper, highlighting, in the paper or during the discussion, many strengths, including a compelling approach clearly relevant to applications and its solid range of experiments. A clear accept, and I would encourage the authors to push in the final version the experiments and discussions following the threads with reviewers (in particular, Vo8C and ULvk). Thanks also to authors and reviewers for a thorough discussion which helped to strengthen further the paper's content. AC.
generated:This paper proposes a novel approach to the problem of unsupervised reinforcement learning. The paper is well-written, well-motivated, and well motivated.
original:Wide agreement from the reviewers. Interesting theorems. Empirical work illustrates the theory. Claim and insight: failure of VAEs is caused by the inherent limitations of ELBO learning with inflexible encoder distribution. Good discussion pointed out related work and insights from the experiments.
generated:This paper proposes a method for learning learning-based neural networks to learn deep neural networks. The authors propose a method to learn a deep neural network based on a neural network-based method. The paper is well-written and well-motivated, and the empirical evaluation of the method
original:Dear Authors, The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted: - This paper does contribute solidly to a timely topic of theoretical understanding of sparisty recovery with deep unroling. - The original version had very limited experiments and only synthetic ones, which raised concerns about whether the setting is motivated and whether the algorithm works on actual real data. The revision fixed that to an extent with some experiments on real data. Yet, there are still some concerns that we suggest to be tackled for the final version: - The capacity analysis is carried out inside a strongly convex regime while the algorithm is advocated for noncon
generated:This paper proposes a method to improve the robustness of deep learning models. The authors propose a novel method to solve the problem of non-supervised learning. The method is novel and novel, and the empirical results are interesting. The paper is well-written and well-motivated
original:This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets. The strengths of the paper are as follows: + In terms of significance of the topic, the problem tackled in the paper is significant and highly relevant. + The motivation of using model ensemble is clearly illustrated via a figure and well justified with theoretical analysis. + Algorithmically, the paper proposes Adaptive Sampling and K
generated:This paper proposes a novel method to solve the problem of non-convex. The authors propose a method to improve the performance of multiple-task reinforcement learning. The paper is well-written and well-motivated, and the empirical results are strong. The reviewers agree that the
original:This paper addresses a meta-learning method which involves bilevel optimization. It is claimed that two limitations (myopia of MG and restricted consideration of geometry of search space) that most of existing methods have can be resolved by the MBG with a properly chosen pseudo-metric. The algorithm first bootstraps a target from the meta- learner, then optimizes the meta-learner by minimizing the distance to that target under a chosen pseudo-metric. The authors also establish conditions that guarantee performance improvements and show that metric can be sued to control meta-optimization. All the reviewers agree that the idea is interesting and experiments well support it. Authors did a good job
generated:This paper proposes a novel method for learning a deep learning model. The authors propose to learn the feature embedding networks with a large number of features using a deep neural network with a small number of nodes, which can be used to learn a class-based learning method. The method is
original:The paper makes some novel and interesting observation pertaining the relationship between data heterogeneity and personalization. Reviewers like the paper and ideas in general but raised several concerns. The rebuttal rectified several confusions and provided more clarification which convinced the reviewers that the paper is above bar for publication.
generated:This paper proposes a novel method of learning learning in RL. The authors propose a method to solve the problem of generalization in RL, which is an important problem. The paper is well-written, well-motivated, and well-developed. The reviewers agree that the paper is
original:The topic of learning reward functions from preferences and how to do this efficiently is of high interest to the ML/RL community. All reviewers appreciate the suggested technical approach and the thorough evaluations that demonstrate clear improvements. While the technical novelty of the paper is not entirely compelling, all reviewers recommend acceptance of the paper.
generated:This paper proposes a method to solve the problem of non-convex, non-supervised learning in deep learning. The method is novel, and the reviewers agree that the paper is well-written and well-motivated. The reviewers have raised some concerns about the novelty of the
original:This paper studies model-based RL in the setting where the model can be misspecified. In this case, MLE of model parameters is a not necessarily a good idea because the error in the model estimate compounds when the model is used for planning. The authors solve this problem by optimizing a novel objective, which takes the quality of the next state prediction into account. This paper studies an important problem and this was recognized by all reviewers. Its initial reviews were positive and improved to 8, 8, 6, and 6 after the rebuttal. The rebuttal was comprehensive and exemplary. For instance, one concern of this paper was limited empirical evaluation. The authors added 5 new benchmarks and also
generated:This paper proposes a novel method to solve the problem of embedding a deep neural network (SAN) based on a neural network based on an adversarial neural network to represent an arbitrary representation of an image. The paper is well-written and well-motivated, and the reviewers
original:This paper provides a normal map-inspired implicit surface representation involving a smooth surface whose high frequency detail comes from normal displacements. Reviewers were impressed with the results and theoretical discussion in the paper. The AC agrees. The authors were responsive to reviewer feedback and addressed some questions about parameter choice during the rebuttal phase, including new experiments/discussion in the supplementary document. Note the response to reviewer WHEF notes that the authors will be releasing data/code; the AC strongly hopes the authors are true to their word in that regard. The AC chose to disregard some comments from reviewer G54X regarding tests with noise, as this method appears to be tuned to computer graphics applications; the level
generated:This paper proposes a novel method to solve the problem of generating a non-convex data model with a large number of variables. The empirical results of the paper are interesting, and the empirical results are interesting. The paper is well-written, well-motivated, and is
original:This paper extends recent and very active literature on analyzing learning algorithms in the simplified setting of Gaussian data and model weights, with the main generalization being to allow for non-isotropic covariance matrices. The main technical results seem to be correct and slightly novel, though reviewers feel they are not innovative or unexpected enough to stand on their own. However, the main contributions of the paper are then to interpret these results to give phenomenological results (regarding double descent, etc.), and reviewers were unanimously happy with these. In the end, all reviewers were positive about the paper. The largest reviewer criticisms of the paper were technical issues (ot31) and lack of context of recent literature
generated:This paper proposes a novel method for learning learning models. The paper proposes to solve the problem of detecting the weak points in deep learning models, which can be learned by using a neural network to learn the same task in a large number of tasks. The authors propose a method that uses a
original:This paper is proposed to address neural network pruning at initialization with the help of meta-gradients considering the high-order relations between loss and optimization of trainable sub-network. The paper is well organized and written with the clear logic. The discussions of related works, as well as their limitations, are comprehensive. To verify the proposed method, the authors have tested it on various benchmarks with different settings. Overall, the meta-learning idea for model pruning is relatively new, which may bring more inspirations to the community.
generated:This paper proposes a method to solve the problem of generating graph-based graph classification. The authors propose a method that uses a gradient-based variational gradient function to predict the value of a set of variables. The paper is well-written, well-motivated, and well-
original:The paper develops a diffusion-process based generative model that perturbs the data using a critically damped Langevin diffusion. The diffusion is set up through an auxiliary velocity term like in Hamiltonian dynamics. The idea is that picking a process that diffuses faster will lead to better results. The paper then constructs a new score matching objective adapted to this diffusion, along with a sampling scheme for critically damped Langevin score based generative models. The idea of a faster diffusion to make generative models is a good one. The paper is a solid accept. Reviewer tK3A was lukewarm as evidenced by their original 2 for empirical novelty that moved to a 3.
generated:This paper proposes a novel method for using graph neural network models for graph neural networks (GNNs) on graph networks that can be trained on a large number of graph networks.
original:The reviewers were split about this paper: on one hand they appreciated the clarity and the experimental improvments in the paper, on the other they were concerned about the novelty of the work. After going through it and the discussion I have decided to vote to accept this paper for the following reasons: (a) the potential impact of the work, (b) the simplicity of the idea, and (c) promise of release of open source code. I think these things make the paper a strong contribution to ICLR. The only thing I would like to see added, apart from the suggestions detailed by the reviewers, is a small discussion on the carbon footprint of training such largescale graph
