the paper is very well-written and straightforward to follow. the paper is very well-written and straightforward to follow. the paper is very well-written and straightforward to follow........,"This paper tackles the problem of exploration in Deep RL in settings with a large action space. To this end, the authors introduce an intrinsic reward inspired by the exploration bonus of LinUCB. This novel exploration method called anti-concentrated confidence bounds (ACB) provably approximates the elliptical exploration bonus of LinUCB by using an ensemble of least-squares regressors. This allows ACB to bypass costly covariance matrix inversion, which can be problematic for high-dimensional problems (hence allowing it to be used in large state spaces). Empirical experiments show that ACB enjoys near-optimal performance in linear stochastic bandits. However, experiments on Atari benchmark fail to show any practical advantage of ACB over current methods, neither computation nor performance-wise. That being said, the proposed ACB approach is theoretically transparent, which contributes to advancing our theoretical understanding of usable intrinsic rewards in deep RL and can inform theoretically motivated directions for improvement and further research, while being on par with SOTA. I believe that this makes the contribution of this work strong enough for acceptance.",0.8022588491439819,0.1403529854918209
"a. d. roberts: paper shows that second-order methods are not better. he says paper shows that the convergence of the unadjusted LMC algorithm is not correct. roberts: paper could benefit from a clean organization, instead of leaving results to Section 4.",This paper provides a near-optimal analysis of the unadjusted Langevin Monte Carlo (LMC) algorithm with respect to the W2 distance. The main statement is that the mixing time is ~ d^{1/2}/eps under standard assumptions. The authors also give a nearly matching lower bound under these assumptions. The reviewers agreed that this is an interesting contribution obtained via non-trivial techniques. The consensus recommendation is to accept the paper.,0.8438910245895386,0.35703638643026353
a small number of minor concerns are raised about the proposed loss. the proposed loss can be learned with the universal method (ReLoss) without expertise on loss function design. the proposed loss is a correlation-based optimization that introduces differentiable Spearman's correlation coefficient.,"The paper presents an approach to learn the surrogate loss for complex prediction tasks where the task loss is non-differentiable and non-decomposable. The novelty of the approach is to rely on differentiable sorting, optimizing the spearman correlation between the true loss and the surrogate. This leads to a pipeline that is simpler to integrate to existing works than approaches that try to learn a differentiable approximation to the task loss, and to better experimental results. The paper is well written and the approach clearly presented. The reviewers liked the simplicity of the approach and the promising experimental results on a variety of challenging tasks (human pose estimation and machine reading).",0.8457894325256348,0.3903230935335159
s1) The paper is well-organized and easy to follow. Strengths: The major weakness of this paper is the experimental evaluation. The paper is well-organized and easy to follow. s3) The paper is well-organized and easy to follow. The figures (e.,"This paper studies the role of positional and relational embedding s for multi-task reinforcement learning with transformer-based policies, The paper is well-motivated, the experiment shows its effectiveness against other competitive methods. In the rebuttal period, the authors solved most of the reviews’ questions such as novelty and ablation studies. There are still some concerns about the generalizability of this approach for other tasks and more experiments are needed.",0.8391392230987549,0.36099608739217126
FP134 is a new approach to quantization. it is a new approach to training. the authors need to prove that FP134 can be applied to a wide range of models. FP134 is a new approach to training. it is a new approach to training.,"This paper introduces a method to determine which precision to use for the weights, as well as a quantisation method using hysteresis to improve performance with low-precision weights, including 4-bits. Reviewers tend to agree that the two points presented are useful and can have a large impact on the field. Generally, reviewers pointed out that motivations, notations and experimental studies could be improved. This has been partly addressed by the authors. I recommend to accept this paper for ICLR 2022.",0.8458852171897888,0.24119309335947037
"a new deformed sampling module that explicitly downscales the image into target resolution. the proposed method is end-to-end trainable, and can be plugged into different backbone networks. but when I read the experiment results, I find that it is not the module itself that brings the most difference.","Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors' rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.",0.8323901891708374,0.2219676108409961
False omissions: the paper provides a new view to parallelize the training task of machine learning models. Strengths: The paper provides a new view to parallelize the training task of machine learning models. Strengths: The paper provides a new view to parallelize the training task of machine learning models.,"This paper presents a way of using multigrid techniques to parallelize GRU networks across the time dimension. Reviewers are uniformly in favor of accepting the paper. The main strength is that the paper provides a new perspective on dealing with long input sequences by parallelizing RNNs across time. The main weaknesses are around the experiments: only CPU experiments are run, and sequences are not very long (max 128 length). All-in-all, though, it provides an interesting perspective that should be valuable to the community.",0.8532503247261047,0.48821631669998167
Cons The paper is largely clear and well-written. Cons The proposed methods are well-motivated and novel. weaknesses The paper is largely clear and well-written. The main issue with this paper is the lack of an intuitive explanation as to why this attack is better at finding sparse and effective adversarial examples than previous work.,"This paper introduces a technique to generate L0 adversarial examples in a black-box manner. The reviews are largely positive, with the reviewers especially commenting on the paper being well written and clearly explaining the method. The main drawbacks raised by the reviewers is that the method is not clearly compared to some prior work, but in the rebuttal the authors provide many of these numbers. On the whole this is a useful and interesting attack that would be worth accepting.",0.8782808780670166,0.4973774887621403
"the proposed proof approach is not easily generalizable, so it would be hard to say that this is a major discovery in the theory of certified robustness........................................","Thank you for your submission to ICLR. The reviewers ultimately have mixed opinions on this paper, but reading in a bit more depth I don't feel that the critical comments raised by the sole negative reviewer really raise valid points. Specifically, the fact that this reviewer directly asks e. g. for comparisons to Levine and Feiz 2019, when the paper (before its revisions) contains an entire section devoted to exactly this comparison, strikes me as not sufficient for a thorough review. However, while I'm thus going to recommend the paper for acceptance (it does present a notable, if somewhat minor, advance upon the state of the art in randomized smoothing), I also feel the paper is generally rather borderline for more straightforward reasons. Specifically, given the _very_ narrow focus of the proposed improvements (improvements to the bounds of randomized smoothing, for L0 perturbations, for Top-k accuracy), I ultimately don't think the paper presents that significant an advance in the field. The paper could go other way, thought definitely not doing so due to the issues that the sole critical reviewer takes.",0.8219960331916809,0.30882274359464645
The proof only applies to positive inputs. (i. (i.)..... (2019) and Malach et al. (2020). The overparameterization required is logarithmic in the approximation error and the network depth and number of parameters. (The proof ideas are not novel.) Weaknesses: The proof only applies to positive inputs. The proof ideas are not novel.,"The paper presents interesting new results for pruning random convolutional networks to approximate a target function. It follows a recent line of work in the topic of pruning by learning. The results are novel, and the techniques interesting. There are some technical issues that are easy to fix within the camera ready timeline (see comments of reviewers below). I would also suggest refining the title of the paper: the lottery ticket hypothesis has an algorithmic component too, which clearly is not covered by existence results.",0.8280321359634399,0.2712252939119935
-) -) -) a very large library. I find the paper very technical but grammar is not on par....... ..  -). . .       . . . Theta(U),"The paper introduces a pipeline to discover PDEs from scarce and noisy data. Reviewers engaged in a very thoughtful discussion with the authors. I read the extensive rebuttal, and I believe the authors have addressed the major concerns claimed by the reviewers. I ask the authors to make sure to include all the changes and additional experiments in the camera-ready version.",0.8082929849624634,0.16596194505691528
"False positive: the paper proposes a weighted GCSL approach. the paper is clear and readable, but there are some typos/weird phrase constructions. a value function is used without any superscript for the policy.","The authors introduce a method that improves goal-conditioned supervised learning (GCSL) by iteratively re-weighting the experience by a variable that correlates with the number of steps till the desired goal. The reviewers mention that the authors focus on an important problem, their method is simple and the empirical results are significant. However, they do point several flaws of the paper, the main ones being questionable theoretical claims and the clarity of the presentation. After an extensive discussion, most reviewers agree that the paper should be accepted but I do encourage the authors to take into account the comments by the reviewers for the final version of the paper and make the theory more clear.",0.8406766653060913,0.3837156556546688
Several novel topological loss functions are presented. The paper presents a novel loss function family the paper is not well-known to the machine learning community.. (1) is used. the paper is a novel contribution the paper the available a novel contribution that further a  better..   a) a) are the same.  i and j (1) is used.,"This paper proposes loss functions to encode topological priors during data embedding, based on persistence diagram constructions from computational topology. The paper initially had some expositional issues and technical questions, but the authors did an exceptional job of addressing them during the rebuttal period----nearly all reviewers raised their scores (or intended to but didn't update the numbers on their original reviews). The AC is willing to overlook some of the remaining questions. For example, concerns that topology isn't well known in the ICLR community (8muq) are partially addressed by the improved exposition (and it's OK to have technically sophisticated papers so long as some reviewers were able to evaluate them). And, future work can address scalability of the algorithm, which indeed does seem to be a challenge here (ey6b). In the final ""camera ready,"" the authors are encouraged to address any remaining comments and to consider adding experiments/discussion regarding scalability to larger datasets.",0.8106926083564758,0.3039720157782237
"aaron ramirez: PF-GNN is a powerful algorithm that can detect graph properties. he says it is a novel and important contribution, but it is a little complex. ramirez: it is a good idea to use a particle filtering algorithm to detect graphs. he says it is a good tool for detecting graphs that are not isomorphic.","This paper presents a neural version of individual-refinement (IR) architecture for improving the expressiveness of GNN in terms of isomorphism tests. As IR is the dominant approach of practical graph isomorphism tests, adapting IR to GNN is a novel and important idea. As IR suffers from the exponential number of branches, the paper adapts particle filtering algorithms to sample K paths to approximate the full version of the IR algorithm. Simulation and real-world datasets are used to demonstrate the improvement over base GNN. Strengths: + The paper is well written and easy to follow. + The originality of the paper is high since it is both technically rich. Adapting individual-refinement (IR) to Neural and GNNs is a novel and important contribution. The designed algorithm does improve over base GNNs. + The particle filtering algorithm is an elegant and low-complexity realization of the IR algorithm. Weaknesses: - PF-GNN is mainly evaluated on synthetic datasets, while only three real-world datasets are employed in total. It is not thus entirely clear how effective the proposed model is in real-world scenarios. The authors added a new real-world dataset, OGB-molhiv, during the discussion period. - The major weakness is the scalability and practical complexity. The designed model is T times deeper and larger than the base model, and K path sampling needs K times larger memory for parallel computation. Although the reviewers still have some concerns, such as “sampling method may judge isomorphic graphs as non-isomorphic”, the reviewers appreciate the author’s hard work on partially addressing the problem during the discussion period. We encourage the author to continue to improve the paper along this direction.",0.8325396776199341,0.39795042341575027
"authors simplify the loss derived by Sorrenson et al.; however, they do not make clear why this is necessary or desirable. authors provide theoretical support for the use of the pre-existing GIN model for ICA. iVAE is an important baseline in the field.","This paper proposes an identifiable nonlinear ICA model based on volume-preserving transformations. The overall approach is very similar to the GIN method published @ ICLR 2020. There is a weak consensus among the reviewers that this paper has some merit, although none pushed for acceptance. After reviewing the paper myself, I agree that the contributions here appear to be incremental, but the results do push this growing field of identifiable latent variable models forward.",0.850567638874054,0.3710208671788374
"MeLIBA and Deep Interactive Bayesian Reinforcement Learning via Meta-Learning. a few minor suggestions I'm not very familiar with this problem........ Two works which are not cited, seem to potentially be related:::.:.:: and....... There'","The paper presents a method for cooperative ad-hoc collaboration by learning latent representations of the teammates. The method is evaluated in three domains. All the reviewers agree that the method is novel and adds an interesting contribution to the important and difficult problem of the ad-hoc collaboration, making fewer assumptions about the team and the teammates. The next version of the paper should comment: - On the societal impact of the centralized training. - Wang et al, CoRL 2020, https://arxiv. org/abs/2003.06906, which addresses the cooperative tasks in the ad-hoc teams without privileged knowledge and assumptions about the teammates.",0.8063701391220093,0.20328783368070918
g. The paper does not provide the standard deviation of the results for the case of the COCOSeq benchmark. b. The paper does not provide the standard deviation of the results for the case of the COCOSeq benchmark. c. The paper does not provide the standard deviation of the results for the case of the COCOSeq benchmark.,"The paper sheds light on issues with BN in continual learning and proposes a quite simple, which is a strength, solution to fix it. The Authors first draw attention to the fact that using recalculated moments boosts performance and reduces forgetting, which serves as an argument that at least partially BN contributes to catastrophic forgetting in continual learning. Given that BN remains quite important in certain application areas such as vision, it is a strong motivation for the paper. The experiments are thorough and clearly show that CN is a practically relevant alternative to BN in continual learning. One weakness of the paper is that the method is poorly motivated, and relatedly, it has quite limited novelty. CN combines the strengths and weaknesses of BN and GN. Hence, it is not clear why it outperforms both, given that it still has the issue of BN that normalization statistics might become outdated. This is one of the weaknesses pointed out by 9jXz who recommended rejecting the paper. It would be also nice to compare to Mode Normalization /forum?id=HyN-M2Rctm. Other papers have suggested changing normalization for sequential learning. Changing batch normalization (to batch renormalization) was investigated in [1] in the context of continual learning. Relatedly, [2] proposes TaskNorm for meta-learning. Despite these issues, it is a solid contribution and it is my pleasure to recommend acceptance. In the camera-ready, please describe more clearly the design principles behind CN. [1] Rehearsal-Free Continual Learning over Small Non-I. I. D. Batches, https://openaccess. thecvf. com/content_CVPRW_2020/papers/w15/Lomonaco_Rehearsal-Free_Continual_Learning_Over_Small_Non-I. I. D. _Batches_CVPRW_2020_paper. pdf [2] TaskNorm: Rethinking Batch Normalization for Meta-Learning, https://arxiv. org/abs/2003.03284",0.7893309593200684,0.4070702688892682
a nitpick: the evaluation equips all baselines with minimal information about the presence of the rigid objects. a nitpick: what happens when the constraint is so complicated? a nitpick: what happens when the constraint is so complex? a nitpick: what happens when the constraint is so complex?,"The manuscript develops a new kind of graph neural network (a Graph Mechanics Network; GMN) that is particularly well suited to representing and making predictions about physical mechanics systems (and data with similar structure). It does so by developing a way to build geometric constraints implicitly and naturally into the forward kinematics of the network, while still allowing for effective learning from data. The manuscript proves some essential properties of the new architecture and runs experiments both with simulated particles, hinges, sticks (and their combination), as well as with motion capture data. Reviewers were generally impressed by the writing and clarity of the work, as well as the main results. In addition, in those cases where reviewers thought that the experiments were lacking, the authors delivered effective new experiments to address those concerns (e. g. looking at mocap and molecular datasets). One reviewer initially scores the manuscript as a Reject/3 on the basis of concerns about novelty and the scope of the theoretical and experimental contributions of the paper. However, they adjust their score 3->5 based on the rebuttal presented by the authors (including new experiments). The reviewer also downgrades their certainty (from 3->2) on the basis of the engagement from reviewers offering higher scores. Overall, the manuscript presents a promising contribution to the graph networks literature and I agree with the general consensus in favour of publication.",0.802353024482727,0.2706053372133862
It's more like an offline continual learning problem with the forgetting constraints but not an (online) continual learning problem. Weakness: The proposed setup is overly simplified. It's more like an offline continual learning problem with the forgetting constraints but not an (online) continual learning problem.,"The paper introduces the problem of continual knowledge (language) learning. The authors point out the interesting duality between continual learning and knowledge learning where: in knowledge learning one must avoid forgetting time-invariant knowledge (avoid forgetting in CL), be able to acquire new knowledge (learn new tasks in CL), and replace outdated knowledge (a form of forgetting and re-learning or adaptation). In their paper, the authors develop an initial benchmark for the task along with a set of baselines and provide empirical studies. The initial reviews were quite mixed. The reviewers seem to agree this work studies an interesting and fairly novel direction for continual learning of language. However, the reviewers did not agree on whether this initial stab at the problem was ""enough."" In particular, reviewer U9Hk argues that the formulation is ""oversimplified"" and the current experiments are limiting. After the discussion, the reviewers remained split with one high score (8), two borderline accepts (3), and one reject. So three reviewers believe that this manuscript is already a good contribution. The fourth reviewer disagrees, but the authors provided clear and convincing responses to many of their comments (and point to results already available in the appendix). Overall, this is a clear and reasonable first step considering this paper proposes a new CL problem. The reviewers and I believe that this is interesting and rigorous enough to be impactful and to warrant follow-up works. As a result, I'm happy to recommend acceptance. I imagine that if the community demonstrates interest in this line of work, there will be work both on methodologies to improve the proposed baselines, but also work proposing extensions to the problem in line with some of the comments of reviewer U9Hk. In preparing their camera-ready version I strongly encourage the authors to take into account the suggestions of the reviewers and your replies. In particular, your discussion regarding encoder-decoder and decoder-only LMs and the associated results would be good to discuss in the main text (even if the full results are in the appendix).",0.8146110773086548,0.3264027108748754
"the regularizations suggest many ways it could be used for other types of data that has a continuous output space. the reasoning behind the regularizations is overall reasonable. However, there could be more explanation, however, on why double decomposition is a good idea for the model.","The authors present a GAN for learning a continuous representation of disease-related image patterns from regional volume information generated from structural MRI images. The reviewers find the problem relevant and appreciate the proposed solution. They find the paper well-written and find the empirical results on Alzheimer brain MRIs relevant for the neuroscience community. The overall objective function includes several hyper-parameters. As pointed out as the main weak point by multiple reviewers this may hint at overengineering/overfitting to a data set. However, the reviewers also mention that the regularizers are all sufficiently well-motivated in the paper and the author response. Reviewers highlight comparisons on the real data as a strong result demonstrating that Surreal GAN was able to isolate two major sources/locations of atrophy in Alzheimer’s disease. Overall, the reviews are positive in majority.",0.8333505392074585,0.30382098971555627
"w2) In Sec 3. “Specifically, given a dataset of preferences D, the reward function is updated by minimizing the binary cross-entropy loss.” w3) The paper is well organized and easy to follow. s1) The paper proposes a semi-supervised reward learning pipeline aiming to reduce reward engineering efforts, which is important topic. w2) In Sec 3. “Specifically, given a dataset of preferences D, the reward function is updated by minimizing the binary cross-entropy loss.” r.","The topic of learning reward functions from preferences and how to do this efficiently is of high interest to the ML/RL community. All reviewers appreciate the suggested technical approach and the thorough evaluations that demonstrate clear improvements. While the technical novelty of the paper is not entirely compelling, all reviewers recommend acceptance of the paper.",0.8421840071678162,0.3449640343586604
True dummy: a gnn model can handle long-range interactions without increasing the depth of the GNN. gnn model can handle long-range interactions without increasing the depth of the GNN. gnn model can handle long-range interactions without increasing the depth of the GNN.,"The paper got four accepts (after the reviewers changed their scores), all with high confidences. The theories are complete and the experiments are solid. The AC found no reason to overturn reviewers' recommendations. However, the AC deemed that all the pieces are just routine, thus only recommended poster.",0.8161986470222473,0.15999671413252753
SSA is a robust training method. SSA is a robust training method. performance is very sensitive to spurious attributes. confirmation bias issue..........,"This paper presents a new method to decrease the supervision cost for learning spurious attributes using worst-group loss minimization. Their method uses samples both with and without spurious attribute annotations to train a model to predict the spurious attribute, then use the pseudo-attribute predicted by the trained model as supervision on the spurious attribute to train a new robust model having minimal worst-group loss. The experiments show promising results in this domain for reducing annotation cost. The reviewers vote to accept the paper, and some of them increased their scores during the discussions since the authors have addressed their concerns.",0.8382158279418945,0.2362049836665392
the paper is generally clear and the context is explained well. the paper is well written with a smooth introduction to SIPs and the related literature towards machine learning for integer programming. the paper contributions are primarily empirical. the paper is a good introduction to the topic and the paper is well written.,"This paper presents a conditional variational autoencoder (CVAE) approach to solve an instance of stochastic integer program (SIP) using graph convolutional networks. Experiments show that their method achieves high quality solutions with high performance. It holds merit as an interesting novel application of CVAEs to the ML for combinatorial optimization literature, as well as for the nice empirical results which show a very nice improvement. Two reviewers had a concern that the contribution is a bit narrowly focused toward MILP-focused journal rather than a general-purpose ML conference since the core contribution is the novel application. On the other hand, they believe that combinatorial optimization has received growing interest from the ML community in recent years. All three reviewers vote for borderline accept of this paper. The authors have addressed some of reviewers' concerns, hence some reviewers increased their scores throughout the discussion phase.",0.8351325988769531,0.3603183259921414
ridge regression based multi-class classification is a novel approach to generalization in deep learning theory. the authors are focusing on the leave-one-out computations of ridge regression based multi-class classification. the main novelty of the manuscript is the characterization of the double descent behavior of ridge regression based multi-class classification.,This paper proposes to use LOO to characterize the generalization error of neural networks via the connection between NN and kernel learning. The reviewers find the new results interesting. The meta reviewer agrees and thus recommend acceptance.,0.8558546900749207,0.3014809414744377
"authors need to clarify the utility of their 'prior knowledge' - how much of the recursive filter results are due to this prior knowledge. 'i would also recommend changing the terms to terms that are not as similar'. 'i'm not sure if this is a good idea, but it is a good idea'","This paper presents a method for inference in state-space models with non-linear dynamics and linear-Gaussian observations. Instead of parameterizing a generative model, the paper proposes to parameterize the conditional distribution of current latent states given previous latent states and observations using locally linear transitions, where the parameters of the linear mappings are given by neural networks. Under fairly standard conditionally-independence assumptions, the paper uses known Bayesian filtering/smoothing tricks to derive a recursive estimation algorithm and a parameter-estimation method based on a simple maximum likelihood objective. Overall, the reviewers found the idea to be novel and interesting and I agree. They also found the relation to the noise2noise objective worth highlighting. Several concerns were raised during the discussion period, which I believe the authors addressed satisfactorily. However, I think the authors should bring the assumed distinction between ‘supervised’, ‘self-supervised’ and ‘unsupervised’ upfront, as usually these types of models are trained using the noisy data (to which the authors refer to as unsupervised). Given the large body of literature on dynamical systems, filters and smoothers, I believe the paper will benefit significantly from more comparisons across a wider range of (and more realistic) datasets.",0.8162742853164673,0.27496691451718414
"?? I think that the neural collapse phenomenon is very important.... The paper is well-motivated by the authors. However, the performance is not the best.......","Based on the previously observed neural collapse phenomenon that the features learned by over-parameterized classification networks show an interesting clustering property, this paper provides an explanation for this behavior by studying the transfer learning capability of foundation models for few-shot downstream tasks. Both theoretical and empirical justifications are presented to elaborate that neural collapse generalizes to new samples from the training classes, and to new classes as well. The problem that this paper delves into is important. The paper is well-motivated, and well structured with a good flow. Both theoretical and empirical analyses of the paper are solid. Preliminary ratings are mixed, but during rebuttal, multi-round responses and in-depth discussions were carried out between authors and reviewers, and the final scores are all positive with major concerns well addressed. AC considers the paper itself and all relevant threads, and recommends the paper for acceptance. Authors shall incorporate all response materials into the future version.",0.8502259254455566,0.45101670920848846
"MIC: I think that the paper was easy to follow overall, and the MIC criterion is intuitive. RS: I think that the paper was easy to follow overall, and the MIC criterion is intuitive. RS: I think that the paper was easy to follow overall, and the MIC criterion is intuitive.","One way of avoiding catastrophic forgetting in continual learning is through keeping a memory buffer for experience replay. This paper addresses the problem of online selection of representative samples for populating such memory buffer for experience replay. The paper proposes novel information-theoretic criteria that selects samples that captures surprise (samples that are most informative) and learnability (to avoid outliers). They utilize a Bayesian formulation to quantify informativeness. They provide two algorithms: a greedy approach, and an approach that takes timing (when to) update memory into account based on reservoir sampling to mitigate possible issues with class imbalance. Pros: The paper is well written and organized. It was easy to follow. The formulation is novel and technically sound. The idea of taking learnability into account is novel and interesting. It provides a nice way of avoiding outliers and balancing surprising information. The authors presented the motivation for each part of the framework well. Cons: To understand the contribution of each component of the formulation and competing criteria, an ablation study is needed. Reviewers had several detailed suggestions and questions, including sensitivity to hyperparameters, additional citations, additional data sets beyond MNIST and CIFAR10, etc. In the rebuttal, the authors have addressed several of these concerns. Please make sure to include and incorporate reviewer suggestions in the final revised version.",0.8256100416183472,0.38583724169681466
g. I found the paper hard to follow at times. g. I am happy with the way you addressed my two main concerns and I would vote for accepting the paper. c. I found the paper hard to follow at times. d. I found the paper hard to follow at times. e. I found the paper hard to follow at times. f. I found the paper hard to follow at times. g. I found the paper hard to follow at times. f. I found the paper hard to follow,"Although the initial scores of the paper were not positive, the authors managed to properly address the questions/concerns of the reviewers and the changes they made to the paper convinced the reviewers' to update their scores. This clearly shows that there were flaws in the original presentation of the paper. So, I would recommend the authors to take the reviewers' comments into account when they prepare the camera-ready version of their work.",0.8396708965301514,0.30219683617663884
"despite this merit, the technical contribution of this paper seems to be limited by the considerations below. i. The authors claim that the resulting mutual information values are ""true"", i. The authors claim that the resulting mutual information values are ""true"", i. The authors claim that the resulting mutual information values are ""true"", i. The authors claim that the resulting mutual information values are ""true"", i. The authors claim that the resulting mutual information values are ""true"",","Initially, some reviewers have raised several points of criticism regarding certain aspects of the model whose novelty/significance was a bit unclear. After the rebuttal and the discussion phase, however, everyone agreed that most of these concerns could be addressed in a convincing way, and finally all reviewers were in favor of this paper. After carefully going over all the reviews, the rebuttal and the discussions, I fully agree with the reviewers and came to the conclusion that this paper indeed contains some interesting, novel and relevant contributions.",0.8205403089523315,0.31508084386587143
subGNN is a self-supervised learning method. it is a good model for subgraph representation learning. but the paper adds the self-supervised learning into the framework. it is more computationally efficient than the subGNN method without such SSL learning.,"This paper proposes a labeling trick for subgraph representation learning with GNNs. The proposed method, GLASS, improves on subgraph-level tasks. The topic of subgraph representation learning is relatively new, and this paper makes progress in that community which would be appreciated by other researchers interested in the same problem. The paper in the original submission state raised some concerns from the reviewers about unclear writing of the motivation and potential applications, technical novelty, and comparisons with existing approaches (even one that are not specifically designed for subgraph representation learning). It is good that the authors conducted additional experiments to show the effect of SSL (that the approach makes improvements without SSL). This and other clarifications from the authors convinced the reviewers to recommend acceptance.",0.8519822359085083,0.48422183717290557
the paper is interesting to a much broader community rather than just the biomedical use cases the experiments have shown. the paper is interesting to a much broader community rather than just the biomedical use cases the experiments have shown. the paper is focused on the comparison to BayReL a related approach mentioned throughout the paper but not described in the related work. The authors demonstrate the ability of MoReL to learn from unpaired data and in a setting with missing data.,"A deep Bayesian generative model is presented for multi-omics integration, using fused Gromov-Wasserstein regularization between latent representations of the data views. The method removes several non-trivial and practically important restrictions from an earlier method BayRel, enabling application in new setups, while still performing well. Reviewers discussed the paper with the authors, resolving misunderstandings of the differences from earlier work (esp. BayReL). The authors reported more extensive experiments in the rebuttal, though not comparisons. The main remaining weakness is that the contributions are in a very narrow field, or at least aplications have only been demonstrated in the narrow field of multi-omics data analysis. And even within that field, only in a narrow subfield. In a machine learning venue that is restrictive. Another issue is computational efficiency. The final decision then depends on how much weight we place on the novel contributions vs these weaknesses.",0.8328037858009338,0.3743526704609394
the paper is accepted. the experimental section is weak. the paper is generally well-written................,"Dear Authors, The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted: - This paper does contribute solidly to a timely topic of theoretical understanding of sparisty recovery with deep unroling. - The original version had very limited experiments and only synthetic ones, which raised concerns about whether the setting is motivated and whether the algorithm works on actual real data. The revision fixed that to an extent with some experiments on real data. Yet, there are still some concerns that we suggest to be tackled for the final version: - The capacity analysis is carried out inside a strongly convex regime while the algorithm is advocated for nonconvex sparsity recovery (see, e. g., the Discussion at the end of Section 2.1 ); - The analysis is relatively loosely connected to the adopted fist-order optimization procedure; - While the depth of network plays a role in the upper bound of Equation (15), its real impact on generalization gap looks quite limited. The above are just suggestions to be looked more carefully, but there are not necessary. The current consensus is that the paper deserves publication. Best AC",0.830532968044281,0.47718799528148437
"Ada and AugMix consistently perform worse than their counterparts (table 1, 2). Strength: Ada and AugMix are robust to some corruption types, but they fail at adversarial attacks. Strength: Ada and AugMix are robust to some corruption types, but they fail at adversarial attacks. Strength: Ada and AugMix are robust to adversarial attacks, but they fail at adversarial types like Fog. Strength: Ada and AugMix are robust to adversarial training, but","The paper proposes an adversarial data augmentation technique searching for adversarial weight perturbations of a corruption network (e. g. a pretrained image-to-image model). The goal is to achieve common corruption robustness as well as a non-trivial level of adversarial robustness. The authors claim state-of-the-art-performance on CIFAR10-C. Most reviewers had initial concerns which the authors could clarify in most cases. Finally, all reviewers argue for acceptance. Strengths: - no pre-defined corruption model necessary - extensive experiments on CIFAR10 and ImageNet with SOTA results - all reviewers agree that this paper would be valuable as a future reference Weaknesses: - the theoretical part is in my point of view rather misleading and should be completely rewritten as the authors lack here rigor concerning the setting they are working in (as also Reviewer 31sh criticizes). In particular the corruptions are not just a covariate shift (p(y|x) is invariant, only p(x) changes) as the corruptions mix the conditional distributions of different points. Thus under the given assumptions (which should be summarized at one point rather than being scattered over the text) the Bayes optimal classifer is invariant but not the Bayes optimal predictive probability distribution (which is clearly important for assessing uncertainty of the prediction). However, when training with the cross-entropy loss we are estimating the predictive probability distribution and thus the given statement about convergence of the risks makes no sense for me and the optimal parameters of the classifier need not be equal even if their classifications agree everywhere. Thus the required changes to fix this theoretical part go significantly beyond what the authors suggest in their rebuttal. - the improvements over AugMix+DeepAugment (7.83 mCE vs 7.99 mCE) are negligible and most likely not statistically significant While AdA has significantly higher adversarial robustness than AugMix+DeepAugment, it remains unclear if using AugMix+DeepAugment together with adversarial training for l_2 as done in [3] Kireev, Klim, Maksym Andriushchenko, and Nicolas Flammarion. ""On the effectiveness of adversarial training against common corruptions."" arXiv preprint arXiv:2103.02325 (2021). could have led to a similar result The paper provides an interesting approach to achieve robustness against common corruptions and all reviewers recommend acceptance. The theoretical part as written currently is misleading as discussed above - the authors have to make it completely rigorous (including proofs, formal statements/defininitions etc) or get rid of it. Minor weak points: - According to the leaderboard of RobustBench the SOTA for CIFAR10-C is by now taken by the NeurIPS 2021 paper Diffenderfer et al, A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness which achieves 96.56% standard accuracy and 92.78% mCE. This is 1.64% and 0.61% better than in the present paper and needs to be discussed as prior work. - for the adversarial robustness evaluation regarding ""AutoAttack & MultiTargeted"" you refer to Gowal et al (2020) but the robustness evaluation has to be properly discussed in this paper",0.8043938279151917,0.410895272422778
"Positives: The proposed approach is simple, effective, and query-efficient. The literature review seems quite thorough and does particularly nice covering the related work in this domain. Positives: The proposed approach is effective, and query-efficient. The authors provide some explanation in Section 4.4 on why do we need local gradient information.","The paper shows that the transfer attack is query efficient and the success rate can be kept high with the zeroth-order score-based attack as a backup. Experiments show state-of-the-art results. Pros: - Simple method based on a simple idea. - State of the art performance. Cons: - Proposal is a straightforward combination of two methods, and therefore technical contribution is marginal. - The threat model is easy (surrogate can be trained on the same datasets and use the same loss function) and questionable. Most of the experimental evidence shows that the research for this threat model is almost saturated (and the problem seems almost solved). This paper got a borderline score with reviewer's concerns above. I agree with the authors that the simplest method is best among those performing similarly, but the threat setting considered might be not very realistic as the authors admitted. I see the proposed method a kind of egg of Columbus in a negative sense. Namely, the authors found a shortcut to win a game that was created and adopted by the community. Perhaps this paper would give an impact on the small community and would make the community change the game. But to give an impact to a general audience, the authors should convince that there are some situations where the analyzed thread model is realistic and therefore the proposed method is really useful. Or, the authors could adjust the thread model to be more realistic. Serious discussion on the thread model would be a big plus to the marginal technical contributions. After discussion with SAC, and PC, our conclusion is that this paper effectively tells the community that the benchmark they are using is too simple, which alone is worthwhile publishing because this may move the community forward (even if the community is small).",0.8254103660583496,0.33312069484964013
"a. The paper is well-written. Various experiments are conducted (cross diverse domains), and hence, it shows the extensibility of their work. b. The paper is largely not self-contained, making it hard to fully grasp its merits. a. The paper is not self-contained, making it hard to fully grasp its merits.","This paper introduces Autoregressive Diffusion Models (ARDMs), which generalises order-agnostic autoregressive models and absorbing discrete diffusion. All reviewers appreciated the paper with a few also finding it very dense. The experimental section is a bit lacking in detail. This has to some degree been answered in the discussion and should also be included in the final version of the paper. Acceptance is recommended.",0.8436495065689087,0.3660221231835229
1.8B FLOPs compared with 8.9B of the proposed architecture. The proposed method requires less time to search for the optimal architecture. The proposed method requires only 7+5 GPU hours for seed+scaling stages. The final searched architecture has more FLOPs than existing methods.,"The paper introduces As-ViT, an interesting framework for searching and scaling ViTs without training. Overall, the paper received positive reviews. On the other hand, R1 rated the paper as marginally below the threshold, raising concerns about search on small datasets and issues regarding the comparison in terms of FLOPS/accuracy with other methods. The authors adequately addressed these concerns in the rebuttal, and helped clarify other questions by R2 and R3. R1 did not participate in the discussion after the author response nor updated his/her review. The AC agrees with R2 and R3 that the paper passes the acceptance bar of ICLR, as the unified approach for efficient search/scaling/training is novel and should be interesting to the ICLR audience.",0.8331426382064819,0.23884957656264305
"""Previous research (Liang et al., 2019; Du et al., 2021) suffers this problem, which unfortunately leads learning implausible parameter values"". [1] Li, Yifei, et al.","This paper introduces a differentiable yarn-level model of fabrics. The model is more detailed and physically realistic than proposed in earlier work, which may allow for applications to manufacturing guidance and textile design. The paper is generally well-written and contains detailed problem formulation and derivations. Experiments show it is possible to successfully learn a control policy and material parameters using the differentiable model.",0.8271101117134094,0.19483551383018494
"GOF-based OOD detection is well-founded, but a challenge in its own right.... Pros. Pros Pros Pros.,  is a challenge in its own right...... Cons. Cons... The paper is easy to follow and the idea of the paper is clear......","The paper investigates the use of flow models for out-of-distribution detection. The paper proposes to use a combination of random projections in the latent space of flow models and one-sample / two-sample statistical tests for detecting OOD inputs. The authors present results on image benchmarks as well as non-image benchmarks. The reviewers found the approach well-motivated and appreciated the ablations. The authors did a good job of addressing reviewer concerns during the rebuttal. During the discussion phase, the consensus decision leaned towards acceptance. I recommend accept and encourage the reviewers to address any remaining concerns in the final version. It might be worth discussing this paper in the related work: Density of States Estimation for Out-of-Distribution Detection https://arxiv. org/abs/2006.09273",0.8231630325317383,0.2579130116436217
ii) the paper is well written and the authors provide clear explanations at each stage. convolutional neural networks and the Vision Transformer architecture.... The paper is well written and the authors provide clear explanations at each stage...........................,"This work identifies an interesting bias that can occur when applying occlusion based interpretability methods to debug image classifiers. For context, the motivation behind many of these methods is that by occluding various parts of the image, one can ask counterfactuals such as ""what would the model have predicted if this object were not present in the image""? However, the authors note that when occluding pixels, classifiers are still functions of the occlusions themselves, so this process may introduce a bias as a result. This is most clearly demonstrated in Figure 2 where a convolutional architecture classifies various occluded images as ""jigsaw"" or ""crossword puzzle"", arguably due to the fact that scattered patch based occlusions resemble crossword puzzles. The authors then demonstrate that ViT models can be modified in a way to ask the above counter-factual in a more principled manner---namely by dropping image tokens within the transformer model, the resulting function doesn't take any occluded pixels as input. Reviewers all found the analysis quite insightful, and did not find any significant flaws in the experiments. During the rebuttal, the authors added numerous experiments to address concerns raised by reviewers regarding lack of datasets for which the method was run on. Unfortunately, only one of the reviewers acknowledged the rebuttal and did not raise their score citing doubts that the method may not work well on datasets with differing image statistics (e. g. medical imaging). After reading all of the reviews and rebuttal, the AC feels the authors have adequately addressed the most pressing reviewer concerns, and finds the presented analysis sufficient to warrant acceptance.",0.8084260821342468,0.3749143139769634
the overall presentation of this paper is both clear and straightforward. my major concern to this paper mainly arises from the use of an additional meta-learned set encoder. the authors selected the topic that the authors selected is important. the authors selected the topic that the authors selected is important.,"Reviewers all found the work well-motivated in addressing uncertainty, a topic that has not seen much focus in meta-learning and few-shot learning. They describe the challenges well: small sample sizes and OOD shift. They then propose a solution they find works well empirically to overcome these challenges based on a set encoder and an energy function respectively. The proposal is largely one of engineering components that have been found to work well in the literature. I'm sympathetic to this style of research (particularly in today's neural network research), although the reviewers raise a primary concern about whether the choices leading to the proposal are justified. In particular, two Reviewers argue that there are no clear ablations compared to alternative simpler approaches, and so the approach of selecting a Set Transformer is rather arbitrary. My perspective is that theory provides one sufficient but not necessary angle to do this, and I do find the authors' replies to the two reviewers convincing. In particular, they add a baseline to estimate covariances suggested by Reviewer Zz5v and they describe how the current baselines do in fact use the shrinkage suggestion by Reviewer QrCN. I recommend the authors use the reviewers' feedback to enhance their submission's clarity and overall quality.",0.8312633037567139,0.3571590520441532
The paper is easy to follow. Cons: The paper is easy to follow. The authors’ method is heavily based on the Slot Attention model. The proposed approach is heavily based on the Slot Attention model. The proposed approach could be relevant only for synthetic datasets.,"This work proposes a new framework that can learn the object-centric representation for video. The authors did a good job during rebuttal and turned one slightly negative reviewer into positive ones. The final scores are 6,6,8,8. AC agrees that this work is very interesting and deserves to be published on ICLR. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are also encouraged to make other necessary changes.",0.8408944010734558,0.28559700573484104
"weaknesses The novelty of this paper is limited. Besides, the authors do not provide further quantitative analysis like that in [1] under this new setting. Strengths: The findings in the paper are useful for researchers in this domain from two folds: Firstly, by open-sourcing the checkpoints, others can take advantage of the trained models.","The results reported in this paper and the model checkpoints released are of interest and broad utility to the community in the opinion of the NLP. While one reviewer was somewhat negative, most reviewers were in favor of acceptance of this paper, which expands the results from [1] to downstream tasks. The AC therefore recommends acceptance.",0.8556568622589111,0.39430490881204605
"a comparison to prior work is needed to judge the relative quality of CAD sketches. the method proposed in this paper utilizes a reasonable approach based on state-of-the-art architectures and techniques. a'sketchGen' approach is a good one, but unfortunately the authors do not compare their results to any of these methods.","The paper describes an approach for automatically generating CAD sketches, including both the primitives that describe the drawing, as well as the constraints that describe relationships between the primitives that need to be maintained even if the primitives are changed. This is an important problem that is starting to receive a lot of attention from the literature. Overall, the paper is very well executed and the results are quite compelling. There were some concerns about the relationship with the work by Willis et al. and other papers that were published around the time when this paper was submitted. There is still some novelty in this paper relative to those works as argued in appendix H, but it would have been really good to have a more quantitative comparison. However, the authors pointed out that this work was concurrent as opposed to prior work as per the ICLR reviewer guidelines. Overall, given the quality of this paper and the guidance given in the ICLR reviewer guide, most reviewers agree with the meta-reviewer that this paper should be accepted (the lowest reviewer still indicated it is above the acceptance threshold). However, there is some discomfort around not having an explicit comparison with very closely related work that ultimately was published before this paper.",0.8431063294410706,0.3466878541641765
-- Weakness Lack of comparison with other existing methods. Weakness The paper is very well-written and easy to follow. Weakness The paper is very well-written and easy to follow. Weakness Lack of comparison with other existing methods.. Weakness The paper is very well-written and easy to follow.,"This paper proposes a new time-varying convolutional architecture (ST-GNN) for dynamic graphs. The reviewers were positive about the presentation and detailed theory, especially on the stability analysis. The shared criticism was on experimental validation synthetic datasets that the reviewers did not find appealing. The AC believes that while the lacking validation concerns are legit, there is a lack of sophisticated dynamic graph benchmarks in the community yet, so the authors did their best effort to test their method. We thus recommend to accept the paper.",0.818899393081665,0.3677040234208107
True : True : a spherical CNN can scale to higher-resolution data. a spherical CNN can be used to generate a spherical signal. a spherical CNN can be used to generate a spherical signal.,"The submission develops a rotationally equivariant scattering transform on the sphere. Many developments in deep learning make use of spherical representations, and the development of a rotationally equivariant scattering transform is an important if not unexpected development. The reviews are split with half of the reviewers believing it to be slightly above the threshold for acceptance, and half believe it to be slightly below the threshold for acceptance. In the papers favor, it solves an important case of the scattering transform framework, which has been demonstrated to be important in diverse machine learning applications such as learning with small data sets, differentially private learning, and network initialization. As such, continued fundamental development in this area is valuable, especially in the context of representation learning, the focus of ICLR.",0.8294839859008789,0.3339226894080639
"The authors do not provide any explanation of the observed results (except for the influence of the data augmentation on the ""cold posterior effect"". Conclusions The experimental report is interesting for researchers who want to have a better intuition of the influence of the prior. The authors generalize their results on one FCNN to all FCNNs and on one CNN to all CNNs, on one task.","This paper provides some empirical investigation of the choice of the prior distribution for the weights in Bayesian neural networks. It shows empirically that, when trained via SGD, weights in feedforward neural networks exhibit heavy-tails, while weights in convolutional neural networks are spatially correlated. From this observation they show that the use of such priors leads to some improved performances compared to the iid Gaussian prior in some experimental settings. Reviewers have conflicting views on this paper, that have not been reconcilied after the author's response and the discussion. On the plus side, the paper is very well written, the experimental part is carefully conducted, and provides some insights on the choice of the prior in Bayesian neural networks, which could lead to further developments. On the negative side, the claims made in the introduction are not fully supported by the experiments (the claims have been slightly amended in the revised version), and the take-home message is not so clear. In particular, Bayesian approaches with the proposed priors still underperform compared to SGD without tempering. The authors could also have considered a broader sets of experiments. Overall, I think the contributions outweight the limitations of this paper, and I would recommend acceptance.",0.83151775598526,0.36187039232916307
a more detailed description of the algorithm box might help. the paper is well written and organized. it is being combined with Alpha Zero even though it is not off-policy. the novelty is that it is being combined with Alpha Zero even though it is not off-policy.,"The main detractor of this paper feels that the paper makes a relatively small technical and empirical contribution given existing results on HER (Andrychowicz et al., NeurIPS 2017). However, several other reviewers, who had more engagement in the discussion, were strong supporters. Having looked at the paper myself I thought the selection of experimental problems undermined the results. Experiments are most compelling when many unaffiliated groups compete on the same benchmarks. But the basic idea of integrating HER with AlphaZero, and a reasonable attempt at this, seems to be interesting enough to warrant a poster.",0.8218648433685303,0.3120164968073368
a few serious issues that need to be addressed. a lot of the paper is confusing to read. a lot of the work is a bit too technical. a lot of the work is a bit too technical. a lot of the work is a bit too technical.,"Kernel methods are among the most flexible and powerful approaches of our times. Random features (RF) provide a recent mechanism to also make them scalable due to the associated finite (and often small)-dimensional approximate feature map (in the paper referred to as linearization). The focus of the submission is the linearization of the softmax kernel (defined in (1)) while making sure that the obtained RF approximation is accurate simultaneously for the small and the large kernel values. The authors present a hybrid random feature (HRF, defined in (8)) construction parameterized by base estimators and weights, and show that specific choice of these parameters is capable of implementing the goal. Some of the HRF estimators are also accompanied by theoretical guarantees (Section 3). Their numerical efficiency is illustrated (Section 4) on synthetic examples and in the context of natural language and speech modelling, and in robotics. Scaling up kernel methods is a fundamental task of machine learning. The authors present a nice and valuable construction in this direction which can be of both theoretical and practical interest to the community. The submission would benefit from implementing the remarks of the reviewers to improve its clarity.",0.8108182549476624,0.22524296831753518
I think its conclusion is not surprising......................,"This paper presents a comparison and analysis of continual learning methods for pretrained language models. The authors categorise continual learning methods into three categories, those that use cross task regularisation, those that employ some form of experience replay of previous training examples, and those that dynamically alter the network architecture for each task. Evaluation results from representative examples of these three paradigms are then presented and analysed. In general methods that incorporate experience reply appear to perform the best, while analysis of the predictive power of individual layers of the pretrained models suggests that some network layers are more robust to catastrophic forgetting than others, and that this also varies across architectures (BERT, ALBERT, etc.). In general the reviewers agree that this is a well conducted study that provides an interesting contribution to an important area of research. They also generally agree that the many of the results are unsurprising given the properties of the algorithms explored and prior work in this area. The main point of difference then becomes how valuable it is to present a thorough study of existing algorithms that confirms our assumptions. I believe that the current work raises enough interesting questions to make it a useful contribution to researchers working in continual learning. In particular the results analysing the relative differences in catastrophic forgetting across different layers in models suggests interesting avenues for follow on work.",0.8146560192108154,0.2620265897777345
"the scheme presented in this paper is not new, but it is interesting and extends recent work that is focused on detecting whether a model relies on spurious correlation. the use of crowd source annotations is also helpful and interesting. the paper is well-motivated and well-written, making its readers easier to follow the core idea delivered in the paper.","The paper tackles the important problem of spurious feature detection in deep neural networks. Specifically, it proposes a framework to identify core and spurious features by investigating the activation maps with human supervision. Then, it produces an annotated version of the ImageNet dataset with core and spurious features, called Salient ImageNet, which is then used to empirically assess the robustness of the method against spurious training signals in comparison with current SOTA models. As pointed out by the reviewers, this work is not about causality and the definitions of causal and spurious features were originally vague and inaccurate. During the revision and discussion, the authors changed the terms ""causal"" features/accuracy to ""core"" features/accuracy. They also called the provided dataset ""Salient Imagenet"", instead of ""Causal Imagenet"", and changed the title to ""Salient ImageNet: How to Discover Spurious Features in Deep Learning?"". Following the prior discussion, we strongly recommend the authors discard any discussion about causality in the camera-ready version of the paper to avoid confusion. Further, we encourage the authors to consider the reviewers’ thoughts and comments in preparing the camera-ready version of their manuscript.",0.8338168263435364,0.3429562797149023
"Weaknesses I did not find any obvious weaknesses. Strengths: Contrary to previous work, the proposed method does not pre- or post-processing to make sure the returned graph is DAG. Strengths: Contrary to previous work, the proposed method does not pre- or post-processing to make sure the returned graph is DAG. Strengths: Contrary to previous work, the proposed method does not pre- or post-processing to make sure the returned graph is DAG.","The authors proposed an algorithm for sampling DAGs that is suited for continuous optimization. The sampling algorithm has two main steps: In the first step, a causal order over the variables is selected. In the second step, edges are sampled based on the selected order. Moreover, based on this algorithm, they proposed a method in order to learn the causal structure from the observational data. The causal structure learning algorithm is guaranteed to output a DAG at any time and it is not required any pre- or post-processing unlike previous work. There were concerns by two reviewers on the slight lack of novelty (""the proposed method of this paper is only a combination of well-developed techniques"") but I believe the proposed method is still worthwhile. In addition, the paper is overall well written and its experiment evaluation is thorough. It will be a nice addition to the field of differentiable causal discovery. My recommendation is to accept the paper as a poster.",0.8390979766845703,0.37471068691876197
"a. The paper investigates a topic that is interesting and timely. b. The paper includes exhaustive ablations. minor weaknesses The DAGGER reference in 4.1 is too tangential (neither necessary nor particularly useful). c. The paper presents the benefits of combined model-learning and proposal learning, in support of MPC.",The paper examines the advantage of using models in RL. The authors' rebuttals convinced us of the value of the paper.,0.8707605600357056,0.3658252104318568
a new task is solved without a huge number of interactions utilizing skills. a paper by a team of researchers tackles the problem of solving a new task without a huge number of interactions. a paper by a team of researchers tackles the problem of skill learning without a huge number of interactions utilizing skills.,"The reviewers agree that addressing long-horizon tasks with off-line learning and fine tuning afterwards from demonstrations is an interesting and relevant topic. The technical ideas about learning a relevance metric to select relevant off-line data, and to learn an inverse skill dynamics models. The experimental results are convincing, even if success rates are sometimes lower than expected. All reviewers recommend acceptance of the paper.",0.8372327089309692,0.2909852682302395
"The experimental section is very unconvincing, however.. the paper is well-written and organized. The experimental section is very unconvincing. The experimental section is very unconvincing......",The authors introduce a novel probabilistic hierarchical clustering method for graphs. In particular they design an end-to-end gradient-based learning to optimize the Dasgupta cost and Tree Sampling Divergence cost at the same time. Overall the paper presents solid results both from a theoretical and experimental perspective so I think it is a good fit for the conference and I suggest accepting it.,0.8208451867103577,0.30054102962215745
the authors do not provide any novel methodologies within the field of active learning or drug development. The authors do not provide any novel methodologies within the field of active learning or drug development. The paper is misleading. The paper is much more about gene knockdown than drug discovery. The results show rather uniform performance regardless of the active learning strategy employed.,"This paper introduces a benchmark for experimental design algorithms for an important cellular biological question, causal discovery of effective genetic knock-out interventions. It uses existing datasets. The paper was discussed by the reviewers after the authors correctly pointed out that methodological machine learning novelty is not a necessary condition for accepting papers. Two reviewers increased their scores and all are slightly positive. The benchmark was seen as valuable, and one reviewer even commented they might use it in their own research. However, the paper is still on the borderline as this benchmark is only a first step. It has not been shown yet that machine learning insights can be produced with it, as the authors have not actually used it for benchmarking yet. In other words, the benchmark can be considered a potentially excellent idea which has not been tested empirically yet. This seems a highly promising research direction and the authors are strongly encouraged to continue to providing the benchmarks and releasing the method to the community so that others can help them in that.",0.8371581435203552,0.3557560523351033
GraphENS is a proposed algorithm for graphs with a class-imbalance issue. the paper has multiple strong points and few weak points. Table 1 shows improvement on the manually imbalanced citation datasets. the gap of performance between all classes and the minor class is almost the same.,"Although reviews were initially a little polarized, they trend toward accepting the paper after rebuttal and discussion. The most negative review raised issues of datasets, baselines, and experiments, and various details that they find confusing. These concerns were not shared by the other reviewers for the most part. Following a detailed rebuttal the most negative reviewer ended up siding with the more positive reviewers.",0.8338292837142944,0.28659629449248314
I do not see a clear connection between the constrained optimization in Eq. (2) and (5). a policy is added to the collection.........,"In this paper, a new method is proposed to discover diverse policies solving a given task. The key ideas are to (1) learn one policy at a time, with each new policy trying to be different enough from the previous ones, and (2) switch between two rewards on a per-trajectory basis: the ""normal"" reward on trajectories that are unlikely enough under previoiusly discovered policies, and a ""diversity-inducing"" reward on trajectories that are too likely (so as to push the policy being learned away from the previous ones). The main benefit of this switching mechanism is to ensure that the new policy will be optimal, because the reward signal isn't ""diluted"" by the diversity-inducing signal as long as the policy stays far away from the previous ones. After the discussion period, most reviewers clearly recommended acceptance of the paper. One reviewer remained on the ""reject"" side though, especially due to an unconvincing theoretical analysis of the method, in spite of several back and forth with authors. I also had my own concerns regarding that part after reading the paper, and further discussions with authors eventualy led to a significant rewrite of the corresponding theorems and proofs. I believe the final version (shared in comments by authors after the dealine for paper revisions) to at least be technically correct, though the relevance of the theory w. r. t. practical usage of the method is still not entirely convincing (e. g., assumptions regarding the number of distinct global optima, and the need for positive rewards). That being said, in spite of these concerns regarding the practical significance of the theoretical analysis, I believe the paper has a strong enough empirical validation, and the method is (1) simple, (2) intuitively reasonable, (3) original due to the trajectory-switching mechanism, which makes me recommend acceptance.",0.8010553121566772,0.2949973065406084
paper is well written and easy to follow. but it's unclear how the p_j memory context m_j is obtained. it's unclear how the p_j memory context m_j is obtained. it's unclear how the p_j memory context m_j is obtained.,The paper presents a neural architecture based on neural memory modules to model the spatiotemporal traffic data. The reviewers think this is an important application of deep learning and thus fits the topic of ICLR. The writing and the novelty of the proposed method need improvement.,0.8306955695152283,0.26072704046964645
the paper provides a relative new framework for GNNs where the non-deterministic label trick is replaced by a deterministic one. the paper does not repeat these efforts and instead tries to showcase the broader application scenarios from Sec 4. results in the four application scenarios that the authors selected indicate little or no benefit of the label trick.,"The paper provides the theoretical justification for the ""label trick"" (using labels in graph-based semisupervised learning tasks). The authors performed a thorough evaluation of their analysis, which constitutes an experimental contribution. The authors provided a rebuttal that the AC finds to have reasonably addressed the reviewers' concerns. We recommend acceptance.",0.843985378742218,0.3973754247029623
the paper is a bit weak. One of my main concerns is the assertion that computing the directional derivative has “computational cost the same as the forward computation of the function” (see p. 2). I think this is a big problem and can be misunderstood by the reader. pdf Preliminary remarks The proposed method is surprisingly simple.,"This paper adapts a method called ""real-time recurrent learning"" for training recurrent neural networks. The idea is to project the true gradient onto a subspace of desired dimensionality along a candidate direction. There are a variety of possible candidates: random directions, backpropagation through time, meta-learning approaches, etc. The main strength of the paper is that it is a very simple idea that seems to have practical utility. While often presented in different contexts, it should be clearly noted by the authors that the general idea of using low dimensional directional derivatives for computational efficiency is fairly common in optimization. Reviewers mention sketch and project methods. This has also been looked, for example, in the context of Bayesian optimization, with [random selection](https://bayesopt. github. io/papers/2016/Ahmed. pdf) and [value of information based](https://proceedings. neurips. cc/paper/2017/file/64a08e5f1e6c39faeb90108c430eb120-Paper. pdf) criteria. Reviewers appreciated aspects of the paper, though had concerns about relations to sketch and project methods, computational costs, and experimental demonstrations and baselines. Through the rebuttal period, reviewers were mostly satisfied that the concerns about computational costs were well-addressed. A better job could still be done about describing relation to other work. There was also still some desire for more thorough experimental demonstrations and consistent baselines, as described in the reviews. The paper also could use some additional proof-reading as it contains several grammatical errors. On the whole, the paper makes a nice simple practical contribution. Please carefully account for reviewer comments in updated versions.",0.8215011358261108,0.36961608473211527
"The authors discuss an interesting problem, which consists in asking whether maximum entropy reinforcement learning can be thought as a robust problem... The paper is published in. The paper is not complete....","The reviewers thought this paper tackles an interesting question around whether MaxEnt RL already provides an important form of robustness. Such work helps us better understand the intersection between generalization, regularization and robustness. The reviewers had a number of comments, questions and clarifications and were generally satisfied with the detailed responses provided by the authors. There was some concern over the strength of the experiments and the authors also ran additional experiments. These addressed one reviewer’s concerns, though the other still thought the existing experiments were a bit too simple.",0.850447952747345,0.29559782445430755
The paper is technically sound and brings a novel perspective to adversarial training.....................,"This paper presents a probabilistic framework that explains why models trained adversarially are robust generators. It received fairly high initial scores. The reviewers thought the work was novel and interesting. They liked that the analysis provided a way to derive a novel training method and sampling algorithms. Reviewers confirmed their support of acceptance and I think this paper is clearly above the bar. Respectfully, I’d prefer that the authors don’t ask the reviewers to “raise your score”. It is up to the reviewers to make that decision.",0.8497502207756042,0.3316410593688488
The paper is well written and well presented. The paper is well written and well presented. The paper is well motivated and mathematically rigorous. 4. The paper is well written and well presented. 5. The paper is well written and well presented. 6. The paper is well written and well presented. 7. The paper is well written and well presented.,"This work studies the question of increasing the expressive power of GNNs by adding positional encodings while preserving equivariance and stability to graph perturbations. Reviewers were generally positive about this work, highlighting its judicious problem setup, identifying the right notion of stability and how it should drive the design of positional encodings. Despite some concerns about the discrepancy between the theoretical results and the empirical evaluation, the consensus was ultimately that this work is an interesting contribution, and therefore the AC recommends acceptance.",0.8124389052391052,0.2211472781544382
the paper is overall well written and the method is simple and easy to follow. weaknesses: The main weakness of this paper is the lack of a comprehensive comparison with existing backdoor attacks on NLP models. the proposed method uses a fixed set of trigger words and randomly inserts them into the input as the trigger.,"The paper presents a backdoor attack approach against pre-trained models that may affect different downstream languages tasks with the same trigger. The paper shows that the downstream models can inherit security holes from upstream pre-trained models. The paper is on the borderline and disagreement remains after discussion and author responses. In general, the new setting introduced in the paper is interesting and well-motivated. However, the options split in how realistic the setting is (e. g., use of uncommon trigger), the evaluation of stealthiness, and the novelty of the idea. After checking the paper, I believe the ideas and insights are justifiable for an ICLR paper and they differ significantly enough from the prior work. I do agree with reviewers that they are some limitations of the proposed techniques (mostly inherited from the prior work it based on). However, as backdoor attack in NLP is a relative new area, I would be more lenient on these weaknesses. The reviewers also provide constructive suggestions on how to improve the evaluation and writing. I hope the authors can address all the comments in the next revision.",0.8511002659797668,0.4820664231524323
". The paper is nicely written, and the theoretical results are well explained. Strength The paper is nicely written, and the theoretical results are well explained. Strength... .................................","The authors theoretically analyze the approximation of Korobov functions by neural networks, obtaining upper and nearly matching lower bounds, for shallow and deep networks, with different activation functions. The bounds are stronger than what can be proved for the more commonly studied Sobolev functions. But Korobov functions are a natural and fairly wide class of functions. This work makes a substantial step forward in clarifying what kind of functions are especially amenable to representation by neural networks. The reviewers also appreciated the clarity of the writing.",0.8320597410202026,0.24201059825718405
- Exclusion of MixUp from the augmentation pool. - Formulation of the probability and magnitude for EDA. Is there a specific reason that EDA is assigned with a single probability and magnitude parameter? - Training of the augmentation candidates. The paper is well-written and easy to follow.,"We appreciate the authors for addressing the comments raised by the reviewers during the discussion period, which includes providing more experimental results to address the concerns. We believe the publication of this paper can contribute to the important topic of data augmentation. The authors are highly recommended to consider all the comments and suggestions made by the reviewers when further revising their paper for publication.",0.8419732451438904,0.31169443701704347
(ii) The paper is well presented. (iii) The paper is well presented. (iv) The paper is well presented. Minor The paper is a bit hard to follow at times and needs to be severely improved both clarity-wise and grammatically. The paper is a good starting point for a more general approach to general AI. The paper is a good starting point for a more general approach to general AI.,"The authors present a new memory-augmented neural network that is related to the Kanerva machine of Wu et. al. The reviewers considered the ideas in the paper novel and interesting, but were concerned about presentation issues and literature review. The authors have improved both... however- authors: please even under limited space constraints, make more room for related work! Clarifying your contribution in the context of the literature is critical for reader understanding, and neglecting this almost had your paper rejected out of hand. I am voting to accept",0.8315039873123169,0.3184317803631226
the paper proposes a new latent variable model for clustering survival data. the novelty of the proposed work is modest. the experiments are well designed with good baselines and evaluation methodology. g. The paper is generally well written and easy to read due to the great efforts the author(s) have put.,"Four knowledgeable referees recommend Accept. I also think the paper provides a unique contribution to the field of deep survival models and I, therefore, recommend Accept",0.8575735092163086,0.30189119577407836
"the proposed method is reasonable and moderately novel. the experimental results are promising for both settings. however, there are unclear parts to be addressed or clarified. the proposed method is reasonable and moderately novel. the proposed method is free from the common issue of diverging from human language.","The reviewers are all weakly positive. The author response clarified important aspects of the paper. The new human evaluation was critical. However, the human evaluation result presentation is flawed: presenting Likert scores as means does not reflect them well. The authors should use something similar to a Gantt chart to fully reflect the distribution across Likert categories. Another detail in the human evaluation that are troubling: it does not reflect interaction with the system, but judgements through observation. Therefore, the human evaluation does not reflect the ability of the learned dialogue system to interact with users. Overall, the paper makes a nice, original contribution, but despite author improvement there are evaluation flaws (even if they are common in papers using these benchmarks).",0.8332252502441406,0.25469456808641555
"(Pro) Charformer is more computationally efficient than BERT and T5. (Mixed) Charformer is more computationally efficient than T5 and BERT, or more efficient in FLOPS. g. (Pro) Charformer is generally on par with Byte-level T5.","This paper introduces a soft gradient-based subword tokenization module (GBST) that learns latent subword representations from characters. GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. The resulting model was tested on GLUE, and several cross-lingual tasks. The performance is competitive with ByteT5 and often similar to subword models while being more efficient in FLOPs and RAM. Reviewers are mixed on this. The negative reviewer points to how this not being a real tokenizer and does not produce a tokenization, that experiments that use the base model do not address bigger scales, and that there is a lack of code which is important for this kind of work, and the resulting accuracy gains are not significant and the method being not interpretable. The positive reviewers like the extensive experiments, the efficiency improvements and flexibility / simplicity of the GBST module. The authors seemed to have addressed most of the reviewer issues by providing larger scale experiments and code. I believe the results are fairly strong, since one would not expect a big performance difference in a learned tokenization method, but rather efficiency or flexibility gains. The paper is generally well-written though details about the convolution should be included in the text (and not just the code). Recommending accept.",0.824849545955658,0.3692365747622468
a graphical scheme of the method architecture would be nice. the paper is clear and well-motivated. the paper is not clear to identify other parts of the paper that support this claim. the paper is clear and well-motivated. the paper is a good start.,"This paper proposes an extension to learning a representation: it motivates, proposes and evaluates a new regularizer term that promotes smoothness via enforcing the representation to be geometry-preserving (isometry, conformal mapping of degree k). Comparisons with a standard VAE and FMVAE (Chen et al. 2020) are shown and experiments are provided on CelebA with several different attributes as target classification tasks. The paper has received extensive reviews and the authors have successfully answered most of the concerns raised, mostly regarding comparisons to other techniques that try to introduce a regularization based on the properties of the Jacobian of the decoder network. The appendix has been extended as a result of the rebuttal and the paper could be accepted. Notes: I find the formulation based on the notion of the isometric decoder somewhat surprising as the encoder is a key object of interest that controls the nature of the representation. The authors should clarify the assumption 3 in 3.3 better by the consideration of potentially dim(z)<<dim(x) , how the isometry of the decoder effects the encoder, Additionally, for the latent space flattening an ablation using SVD (merely a linear mapping for i(⋅) ) could be considered. Reviewer ZGHS has noted that they raise their grade to 6 in their comment, but this is still not currently reflected.",0.8175046443939209,0.3320162629708648
Weak Points The theoretical guarantees provided in this work are based on several technical assumptions which are hard to verify in real contexts. The question is: Is this proposed method an acceptable solution in a legal context?. Weak Points The theoretical guarantees provided in this work are based on several technical assumptions which are hard to verify in real contexts,"The article proposes an approach to alter the approximate posterior distribution in order to remove some of the information (unlearning). The approach is applicable when the approximate posterior is obtained via (stochastic gradient) MCMC methods. Unlearning is done by shifting the approximate true posterior by some value delta, which is found via optimisation with an influence function. The approach is novel, tackles an important problem and is mathematically sound. Reviewers have highlighted some of its limitations. In particular, the modified posterior is obtained by translating the original posterior, without changing its shape; many aspects of the data may therefore not be forgotten. The authors have partially addressed this concern in their response and provided additional experiments. Although there is still disagreement amongst reviewers, I recommend acceptance. A Minor comment: I would not present MCMC merely as a ""machine learning algorithm"" (p.1), nor as a ""sampling based Bayesian inference method"" (p.1 and p.2). MCMC is a generic approach to approximate high-dimensional integrals and obtain samples approximately sampled from some target distribution, dating back from the work of Metropolis (1953) and Hastings (1970). Its application to Bayesian inference/machine learning problems came much later, see e. g. the excellent review of C. Robert and R. Casella: A short History of MCMC: Subjective recollections from incomplete data. Statistical Science, 2011.",0.8038989305496216,0.23461265570949763
"despite the real author's efforts, the paper is very hard to read and requires an advanced mathematical background. despite the real author's efforts, the paper makes a reasonable contribution to the field. despite the real author's efforts, the paper is very hard to read and requires an advanced mathematical background.","The paper makes a significant contribution in the rather sparse and challenging field of convergence analyses of actor-critic style algorithms, under the linear MDP structural assumption, showing that there is a natural bias towards being high-entropy. As one of the reviewers points out, although it is unlikely that the strategy actually proposed is amenable to implementation, the paper nevertheless provides a clean and novel analysis of convergence of learning by eschewing the usual mixing time type assumptions often found in the theoretically-oriented RL literature. Based on this strength of the paper, I am glad to recommend its acceptance.",0.8399600386619568,0.4634307473897934
a) Statistical evaluation: a good number of experiments have been conducted and presented in the main paper. (EE1) Difficulty of directly comparing with published results: a good number of experiments have been conducted and presented in the main paper. (EE2) Difficulty of directly comparing with published results: a good number of experiments have been conducted and presented in the main paper.,"This paper introduces a novel approach for out of distribution detection that generates scores from a trained DNN model by using the Fisher-Rao distance between the feature distributions of a given input sample at the logit layer and the lower layers of the model and the corresponding mean feature distributions over the training data. The use of Fisher-Rao distance is novel in the context of OOD, and the empirical evaluations are extensive. The main concerns of the reviewers were the limitations of the Gaussianity assumption used in computing the Fisher-Rao distance and the use of the sum of the Fisher-Rao distances to the class-conditional distributions of the target classes rather than the minimum distance. These concerns were addressed satisfactorily in a revision. In terms of technical novelty, experimental evaluation and novelty, the paper is above the bar of acceptance.",0.8088996410369873,0.39133366346359255
eq. g. is a softer approach in which semantically similar instances are grouped together. the paper tackles an interesting problem that is under-studied. it presents clear novelties respect to prior art in terms of the approach. the paper does tackle the problem following the same approach (a variant of KD applied to this problem),"Four experts reviewed the paper. All but Reviewer HSTU recommended acceptance. The authors clearly did a great job with the rebuttal, which convinced two reviewers to raise their scores above the acceptance threshold. Notably, the reviewers found the newly added experiments impressively strong. The rebuttal also addressed some clarification questions. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance. As mentioned by the reviewers, some experiments and discussions in the rebuttal should be included in the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!",0.8238710761070251,0.2513157224489583
the regularization strength is stronger. I think the proposed method is interesting and worth further study. I think the submission is somewhat limited..............,"The paper introduces a method to train neural networks based on so-called stability regularisation. The method encourages the outputs of functions of Gaussian random variables to be close to discrete and does not require temperature annealing like the Gumbel Softmax. All reviewers agreed that the proposed method was novel and of interest. The authors conducted extensive experiments. They also adequately addressed the concerns raised by the reviewers (e. g., theoretical foundation and computational cost).",0.8497171401977539,0.3231234488387903
a number of works have been done that also investigate unrolled alternating minimization on a sparse S and a general A. The paper presents an extension of unrolled alternating minimization that preserves the structure of the variable A. the paper concludes that the proposed method is not a better fit for the current situation.,"The paper develops an unrolled version of the PALM algorithm for sparse blind (or semi-blind) source separation. The unrolled version includes a soft-thresholding update, in which the thresholding parameter and one of the weight matrices is learned from data, with a least squares dictionary update, in which the step size is learned from data. The paper provides experimental results showing that this LPALM algorithm is less sensitive to the choice of hyper parameters (since step sizes, etc. are learned from data), and to the choice of the initial dictionary (perhaps since the W matrices are learned from similar examples). It also improves over PALM on experimental data from an astronomy problem. Reviewers expressed appreciation for the paper’s experimental results, and detailed investigation of the parameterization of unrolled PALM. They also highlighted some issues in the initial submission's exposition -- in particular, the setting of the problem (what kind of training data is available, what is the relationship between the mixing matrices A at training and at test time), and a clearer explanation of why it makes sense to learn fixed matrices W^{(k)} which do not depend on A (given that A may change at test time). The revision improved the clarity of the paper, addressing most of these concerns. The submission contributes to the discussion on how to unroll dictionary learning / blind source separation algorithms, how the unrolled algorithm should be parameterized, and demonstrates good results on multispectral data analysis.",0.8358722925186157,0.4183716575304667
'naive' approach could be adapted to fill out hessian matrix. but it seems to be a much more efficient baseline. 'no experimental results' for efficiency of proposed approach. 'no experimental results' for efficiency of proposed approach.,"This paper tackles the problem of feature interactions identification in black-box models, which is an important problem towards achieving explainable AI/ML. The authors formulate the problem under the multi-armed bandit setting and propose a solution based on the UCB algorithm. This simplification of the problem leads to a computationally feasible solution, for which the authors provide several theoretical analyses. The importance of the learned interactions is showcased in a new deep learning model leveraging these interactions, leading to a reduction in model size (thereby competing against pruning methods) as well as an improvement in accuracy (thereby competing against generalization methods). Although the proposed approach essentially builds on the specific UCB algorithm, it could likely be extended/modified to other (potentially more efficient) bandit strategies. A drawback of this work resides in the experiments being entirely synthetics. In order to close the gap with practice, experiments on real datasets of higher dimensionality should be conducted.",0.833407998085022,0.3202694904591356
ICLR community is working on a new approach to forecasting. the proposed method is simple and straight-forward. the only difference is the additional learnable parameters (gamma and beta). the method is also very similar conceptually. the authors only evaluate their method with MSE/MAE metric.,"This paper introduces the ""reversible instance normalization"" (RevIN), a method for addressing temporal distribution shift in time-series forecasting. RevIN consists in normalizing (subtracting the mean and dividing by the standard deviation) each layer of of deep neural network in a given temporal window for a given instance, and de-normalizing by introducing learnable shifting and scaling parameters. The paper initially received one weak accept and two weak reject recommendations. The main limitations pointed out by reviewers relate to the limited novelty of the approach, the positioning with window normalization methods and hybrid methods in times series, and clarifications on experiments. The authors' rebuttal did a good job in answering the main concerns: rV5fo increased its grade from weak reject to clear accept, and RuPmn maintained its weak acceptance recommendation. The AC carefully read the submission. The AC considers that the idea is simple yet meaningful. The large set of experiments are well conducted and conclusive. The rebuttal successfully answers to relevant issues raised by reviewers, regarding ablation studies (for highlighting the importance of the learnable de-normalization), the impact of the temporal window, the comparison to hybrid approaches and the difference with respect to Adaptive normalization. The AC thus acknowledge that this submission draws important take-home messages for the community, and therefore recommends acceptance.",0.8267431259155273,0.28048610091209414
paper identifies flaws in multiple high-profile NLP interpretability papers. it proposes an alternative mechanism for ranking neurons containing linguistic attributes. paper presents a detailed experiment showing it empirically. it is difficult to read. and the paper is difficult to follow.,"This paper analyses interpretation methods that use probes to evaluate the information in individual neurons of a deep network and shows that it confounds probe quality and ranking quality, and encoded information and used information. The paper proposes a new method which does not suffer from the same drawbacks. The reviewers were positive about this paper, and the discussion between the reviewers and authors resulted in the authors adding multiple clarifications. I ask the authors to try to optimize the paper for clarity further. I recommend acceptance.",0.8564338088035583,0.35522533804178236
True : StarQE is a simple but effective question-answering model...... the authors prepare a new dataset for the hyper-relational KG QE problem.. the authors propose a reasonable solution to the hyper-relational KG QE problem.,"This paper presents a query embedding approach for answering multi-hop queries over a hyper-relational knowledge graph (KG). The main contributions are a new dataset (WD50K-QE) for this task and a simple but sensible extension to an existing model for query embeddings to also handle relation qualifiers. Reviewers wJVm and Bute note that the reification and StarQE models perform similarly. While this is not a negative result, as the authors note, it does raise the question of the relative pros and cons of the two methods. I hope the authors can add a discussion of when one might prefer StarQE over the conceptually simpler reification method in the final version. The authors addressed Reviewer frRt’s concerns about faithfulness and backwards compatibility (though more evidence on purely triple-based tasks would be nice here). Reviewer GQAR also raised some concerns about writing, but the other reviewers mostly found the paper to be well written and well motivated and I tend to agree. Overall, while there are some very good suggestions on how the paper can be extended and improved, I find the current contributions to be substantial enough to warrant a publication.",0.838187038898468,0.4160290630534291
"the paper proposes to apply the idea of hypersolvers to numerical optimal control. in experiments, the method is shown to perform on par with the higher-order ODE solvers. my main issue with the paper is the limited application and its relevance to the ICLR community.","The authors propose a novel hypersolver framework for solving numerical optimal control problems, learning a low order ODE and a neural network based residual dynamics. They compare their framework with traditional optimal control solvers on a number of control tasks and demonstrate superior performance. The reviewers are in consensus that the paper makes significant contributions that are validated by the experimental results. The only concern was that the experiments are largely on low dimensional systems, but the reviewers agreed that the results are still worthy of acceptance.",0.8815049529075623,0.46082417418559396
the idea of progressive growing of GANs is convincing. but there are two drawbacks for this submission. the time series data is difficult to fit due to high volatility. the authors would like to see a more detailed explanation of the problem.. if the authors could explain why the data augmentation leads to degradation of the forecasting performance.,"This paper adapts the idea of progressive growing of GANs to time series synthesis. The reviewers thought that the idea was well motivated. DRP7 initially expressed concern w. r. t. novelty. They were also concerned with the lack of certain baselines. The authors responded, highlighting its contributions w. r. t. Evaluation (Context-FID score) and extensiveness of the evaluation. The authors also added missing references but pushed back on the additional baselines. DRP7 raised their score. Reviewer pbaT was also positive about the work though had some questions and suggestions for improving clarity. They had initially given a low score for “correctness” but raised this, indicating they were satisfied their clarity concerns were addressed. Reviewer RLDM (whose code-name happens to match a ML conference) thought the work was novel and appreciated the introduction of a new metric for evaluating the quality of generated time series data. They remarked on the thoroughness of the experiments and the quality of the presentation. They asked some clarifying questions to which the authors provided a response. Reviewer 4v5L also had a concern with novelty, felt the loss function was “heuristic” and didn’t see the utility of the FID-based score. They also presented several clarifying questions. The authors provided a lengthy response to that reviewer’s concerns, having run additional analysis, and the reviewer upgraded their score in response. With all reviewers on the accept side of the fence I am inclined to recommend acceptance. Please note 4v5L’s comment that “the paper still needs significant edits to reflect the points in the reviewer responses”.",0.8273016214370728,0.327494926361934
The model is well structured. Conclusion..... the paper is well written and easy to understand. I commend the authors for designing and implementing this highly complex agent architecture....,"The current paper presents a new method for communication and cooperation in multi-agent settings. Specifically, the authors propose to model other agents' intentions and internal states using ToM nets and using these predictions to then decide how to communicate/coordinate. The authors present experiments in two multi-agent cooperation tasks (multi-sensor multi-target coverage and cooperative navigation), compare against 4 previous methods (TarMAC, I2C, MAPPO and HiT-MAC) and perform the necessary ablations studies and find that their method achieve better rewards in both environments. All reviewers have found the present study to be novel with convincing experimental findings. Reviewers have raised some concerns however a great deal of those have been addressed by the authors during the rebuttal and many of these points have now been incorporated in the paper. Having read the paper and considering the reviews I agree with the reviewers that this manuscript will make a good addition to the program of ICLR and as such I recommend its acceptance.",0.8434239029884338,0.31113799971838796
the learning path is very simple. The theoretical explanation and the illustrative toy experiments are very suggestive of what may actually be happening with ESKD. Cons: The theoretical explanation and the illustrative toy experiments are very suggestive of what may actually be happening with ESKD. Questions and suggestions for improvement: Experimental results need to be improved. The memory requirements for storing smoothed labels may be prohibitive.,"The authors made substantial improvements to the originally submitted manuscript; however, reviewers initially remained reluctant to support the paper for acceptance based on the degree to which they were confident in the underlying arguments / position taken by the authors and the evidence provided to support their position and arguments. There are also concerns about the significance of the gains in performance afforded by the proposed approach. During the author response period two reviewers became satisfied with the additions and modifications leading to an increase in the final score. It will be critical for the authors to try to add ImageNet results if possible in addition to other promises made to reviewers. The AC recommends accepting this paper.",0.83026123046875,0.301806329190731
Cons: The paper is well-organized and easy to follow. Cons: The paper is well-organized and easy to follow. Cons: The paper is well-organized and easy to follow. Strength: The paper is well-organized and easy to follow. Strength: The paper is well-organized and easy to follow.,"The authors study the problem of video classification and propose a new module which promises to increase accuracy while keeping the computational overhead low. The main idea is not to share the spatial convolution weights over different time steps, but allow some modulation based on pooled local and global frame descriptors. The resulting module can be used as a drop-in replacement for spatial convolutions in existing models and yields competitive performance on multiple video action recognition and localisation benchmarks. The reviewers appreciated this challenging setting and the simplicity of the main idea. They found that the paper was clearly written, well organised, and easy to follow. The reviewers raised some issues in connections with related work and the empirical evaluation which were successfully resolved during the discussion phase. Given that computational efficiency remains as one of the most challenging topics in video understanding, I believe that this technique will be relevant for the larger video understanding community. I strongly suggest that the authors incorporate the feedback received during the discussion, especially the GFLOPS vs accuracy plots, and further clarify the relationship to existing work.",0.8237752914428711,0.5345667088171467
"the more general subspace of policies alluded to in the introduction / title. the ""line"" of policies. the results are interesting. the method is intuitive but could benefit from further exploration.. the Gym benchmark tasks. I am concerned about","The paper tackles the problem of generalizing to a new environment by learning a small set up anchor policies (even just 2 for the final approach) which span a sub-space that can be searched efficiently in a new environment. The discussion and additional experiments managed to convince most reviewers that the method indeed works as the authors had hypothesized (especially regarding functional diversity). At the moment the analysis is mainly based on empirical observations, it would be good to also have a thorough theoretical analysis of the method.",0.8382040858268738,0.38873934994141257
paper relies entirely on empirical evidence for its claims. main weakness of this paper is the experiments section. the experimental results are also a bit limited. a) It is misleading to highlight both max accuracy and max comms savings values since the two correspond to different settings and thus cannot be achieved simultaneously.,"This paper considers the problem of on-device training for federated learning. This is an important problem since, in real-world settings, the clients have limited compute and memory, and local training needs to be efficient. The paper shows that the standard sparsity based speed-up techniques that consider top-K weights/activations during forward and/or backward pass do not work well in the federated setting and proposes several solutions to mitigate this issue. The proposed solutions are demonstrated to work well on several datasets. In their initial assessment, given that this is largely an empirical insights driven paper, the reviewers mainly expressed concerns about the experimental evaluation (e. g., only one dataset CIFAR10 and one architecture ResNet18) and lack of more baselines (e. g., Federated Dropout). The authors responded in detail to the reviews and also conducted additional experiments and the reviewers and authors engaged in discussion. As the discussion converged, the reviewers agreed that the revised manuscript addresses their key concerns and their assessment, on an average, are now learning largely towards a borderline accept. I also read the reviews, the discussion, and read the paper. I think the paper is a good initial attempt at providing a general approach to enable on-device federated learning when the clients are lightweight devices (e. g. edge devices). Even though the study is somewhat preliminary, the current manuscript, after the revision during the discussion phase, is significantly improved version of the original submission and does address the key concerns from the reviewers. Overall, I would rate the paper for a borderline acceptance.",0.8239042162895203,0.3553310252726078
"TreeHEM is used to reduce the number of Gaussians in the mixture. Weakness: Compared with other methods, such as PointNet and PointNet++, the effectiveness and practicability of this method cannot be sufficiently demonstrated. g. would not perform better when matching the memory footprint instead of the number of parameters. g. would not perform better when matching the memory footprint instead of the number of parameters.","This paper presents a deep learning method that aims to address the curse-of-dimensionality problem of conventional convolutional neural networks (CNNs) by representing data and kernels with unconstrained ‘mixtures’ of Gaussians and exploiting the analytical form of the convolution of multidimensional Gaussian mixtures. Since the number of mixture components rapidly increases from layer to layer (after convolution) and common activation functions such as ReLU do not preserve the Gaussian Mixtures (GM), the paper proposes a fitting stage that fits a GM to the output of the transfer function and uses a heuristic to reduce the number of mixture components. Experiments are presented on MNIST (2d) and ModelNet10 (3D), which show competitive performance compared to other approaches such as classic CNNs, PointNet and PontNet++ methods. There is somewhat an overall consensus on the novelty of the proposed approach and its potential to pave the way for further research. There were, however, several issues raised by the reviewers in terms of clarity, memory footprint and computational cost that limits the applicability of the method to more complex datasets. While the authors expanded on the dense fitting in their comments and in the revised version of the paper, it still remains unclear the role of the negative weights, as the dense fitting stage seems to constrain all the weights to be positive. In terms of memory footprint, the authors refer to the theoretical footprint and their implementation does not match this. Finally, it is acknowledged by the authors that the computational cost is a limitation that hinders the method from achieving competitive performance in more complex tasks.",0.8396012783050537,0.4203323479741812
"a unified explanation of why SimSiam/BYOL does not collapse is important. the paper attempts to get more insights of it with empirical evidence. the paper is not the essential part of the research. the paper is not a complete work, but a good one.","This paper provide an explanation why contrastive learning methods like SimSiam avoid collapse without negative samples. As the authors claimed, this is indeed a timely work for understanding the recent success in self-supervised learning (SSL). The key idea in this submission is to decomposes the gradient into a center vector and residual vector which respectively correspond to de-centering and de-correlation. Such an explanation is interesting and novel. The empirical results are solid and convincing. During the rebuttal stage, the concerns from the reviewers are well resolved, and the writing of the new version is significantly better than the original one.",0.849223792552948,0.40955680112044013
"the paper explores an important problem and propose an interesting model that goes in the right direction. it is a fairly straightforward yet novel modification to a vanilla Transformer that introduces concepts, which the user has to provide during training time. these concepts are then attended to via cross-attention against queries, which as usual come from the input.","This paper proposes a generalization of the standard Transformer attention mechanism in which keys and queries represent abstract concepts (which must be specified a priori). This in turn yields ""concept embeddings"" (and logits) as intermediate network outputs, providing a sort of interpretability. Reviewers agreed that this is a simple (in a good way) and interesting approach, and may lead to follow-up work that builds on this architecture. Some concerns regarding the relation of this method to prior work—in particular the ""Concept Bottleneck"" model—were raised and addressed in discussion; the authors might incorporate additional discussion in future drafts of the work.",0.850511372089386,0.38887637605269754
"Cons: A solid paper, well-motivated, and well-presented. Cons: A solid paper, well-motivated, and well-presented. Cons: A solid paper, well-motivated. Pros: A solid paper, well-motivated. Pros: A solid paper, well-motivated.","The paper presents an approach to predict relations between node pairs in heterogeneous graphs, with application to recommendation and knowledge-base completion. The author's approach is to compute similarities between subgraphs that are neighborhoods of nodes where the relation holds or not to score a relation. The authors use graph neural networks to scores these subgraphs. The type of subgraphs that are considered are pairs of nodes, 3- and 4- cyles to make inference and training tractable. The paper lies in the stream of work that combines logical reasoning and neural network, even though in that particular instance it mostly combines graph mining techniques and neural networks. The reviewers unanimously liked the presentation of the paper and the high empirical performance. The rebuttal addressed most of the remaining concerns.",0.8020306825637817,0.376496119584356
"i. The paper is amongst very few works considering the so-called Markovian contexts. CMDPs is a powerful framework capable of modeling a broad range of applications. i. The paper is well-written and mostly well-organized, and presents an adequate literature review.",The paper proposes a Bayesian approach to learning in contextual MDPs where the contexts can dynamically vary during the episode. The authors did well in their rebuttal and alleviated most of the reviewers' concerns. During the discussion there was an agreement that the paper should be accepted. Please take all reviewer comments into account when preparing the final version.,0.863354504108429,0.3402093447744846
"I am not sure whether OT can generalize from one dataset to another. I am not sure whether OT can generalize from one dataset to another. g. If the paper applies OT, I am not sure whether OT can generalize from one dataset to another. -> ""... The paper outperforms the state-of-the-art post-hoc adjustment methods in terms of recognition accuracy. The training of OTLM seems to depend on the convergence of Sinkhorn’s algorithm. The","All reviewers agreed that the idea proposed by the paper is interesting and is well-motivated for handling long-tailed recognition problems. As suggested by the reviewers, it seems important that the limitations the paper be addressed in the final version of the paper.",0.835817813873291,0.2805477795191109
"False positive: acoustic model is non-standard, which makes it even more relevant to compare to other alternative models. False positive: acoustic model is non-standard, which makes it even more relevant to compare to other alternative models.","This paper suggests using a conditional prior in conditional diffusion-based generative models. Typically, only the score function estimator is provided with the conditioning signal, and the prior is an unconditional standard Gaussian distribution. It is shown that making the prior conditional improves results on speech generation tasks. Several reviewers initially recommended rejection, but after extensive discussion and interaction with the authors, all reviewers have given this work a ""borderline accept"" rating. Criticisms included that the idea is too simple or obvious to warrant an ICLR paper. I am inclined to disagree: simple ideas that work are often the ones that persist and see rapid adoption (dropout regularisation is my favourite example). Like the authors, I believe simplicity is an advantage in this respect, rather than a disadvantage. Of course, simple ideas do require extensive and convincing empirical validation to be worth publishing at ICLR. After the authors' updates, I believe the work meets this bar. Another issue raised by several reviewers is the limited theoretical justification for the approach. However, combined with the simplicity of the method, I believe the empirical results of the revised version sufficiently justify the approach on their own. Nevertheless, I would recommend that the authors consider further how they could address this issue in the final version of their manuscript, as they have already begun to do during the discussion phase. Another way to strengthen the paper further would be to demonstrate how the generic approach can be applied in a different domain (e. g. conditional image generation), but I do not consider this addition necessary for the work to warrant publication. In light of this, I am recommending acceptance.",0.8077934384346008,0.27555270098673645
a detailed ablation study on the sensitivity of some hyper-parameters is also included. the paper is nicely-structured and very well-written. the paper is well-written and well-organized. it is a good mixture of formulas for mathematical rigor and textual/graphical explanations.,"This paper presents a method for target side data augmentation for sequence to sequence models. The authors of the paper use a relatively straightforward method to generate pseudo tokens that are used for enhanced training. The authors present results on dialog generation, MT and summarization where automatic metrics show improvements. For really robust results, I would have preferred to see more human evaluations since BLEU and ROUGE are metrics that the NLP community is moving away from. Overall, the majority of the reviewers are happy with the paper and there is significant back and forth between the reviewers and authors that have improved the paper; I think the authors went to significant lengths to allay all concerns from the reviewers and the paper should be accepted.",0.8336282968521118,0.3294659577310085
The paper is very good. Weakness The paper is very good. The paper is very good... Cons: The paper makes an insightful connection between X3d's conv layer and transformer block. Weakness..  Weakness,"The paper presents an approach for spatio-temporal representation learning using Transformers. It introduces a particular architecture design, which shows an impressive computational efficiency. The reviewers agree that the experimental results are strong, and unanimously recommend the paper for acceptance. The reviewers find their concerns regarding the details of the approach/setting address after the authors' response. We recommend accepting the paper.",0.8392491340637207,0.40740203857421875
the goal seems to be to learn a non-stationary policy from trajectories as in imitation learning under a causal framework. I think the paper is an interesting paper. I have lowered my review slightly after reading the other reviews. I think the paper is an interesting paper.,"All three reviewers suggest acceptance of the paper. The authors study an interesting problem (understanding non-stationary and reactionary policies) and propose a solution to the problem which compares favorably to baselines in experiments. However, some of the reviewers also criticize unclarities in the presentation of the paper and the made assumptions. The authors clarified those points quite well in their rebuttal. Further concerns regarded design decisions and the comparison to failure cases of baselines. The authors addressed those in their rebuttal and promised to include corresponding material in their updated paper. Hence I am suggesting acceptance of the paper. Nevertheless, I would like to urge the authors to carefuly revise their problem presentation in the paper in order to improve clarity and add the promised additional insights to the final version of the paper.",0.8492516279220581,0.4401417765766382
"The theoretical results help to understand the gap between the linear and non-linear factorization gap...... The paper is well written and easy to follow. Besides,...","The paper considers matrix and tensor factorization, and provides a bound on the excess risk which is an improved bound over the bounds for ordinary matrix factorization. The authors also show how to solve the model with standard gradient-based optimization algorithms, and present results showing good accuracy. The method can be a bit slow but this depends a bit on the number of iterations, and in general it achieves better accuracy in a similar amount of time to other baseline algorithms. The reviewers raised a few points, such as jdoi noting the tensor experiments were for small tensors and should include the method Costco as well; other reviewers mentioned more methods as well. The authors seemed to address most of these concerns in the rebuttal, adding more experiments and more details on timing. 26KD mentioned the optimization procedure was unclear, but the revision includes pseudocode in the appendix that clarifies. Overall, the paper has both a theoretical and algorithmic contribution, and would be of interest to many ICLR readers.",0.8478794693946838,0.30549342859359013
False omissions: the writing is not very clear in some places. omissions: the paper is a good contribution. a pretrained signature-based autoencoder is a natural improvement over standard techniques. omissions: the paper is a good contribution.,"This paper proposes a novel method for training neural rough differential equations, a recent model for processing very long time-series data. The method involves a lower-dimensional embedding of the log-signature, which is obtained via pretrained autoencoder to reduce overhead. The results show significant and consistent improvements over previous methods on long time-series data. Overall, the reviewers and I all agree that this paper offers a novel and impactful contribution leading to significant improvements over previous state-of-the-art methods for training neural rough differential equations. I recommend acceptance.",0.8536317944526672,0.3880761370062828
"proposed method is a preconditioning gradient update such that F and G are updated following standard gradient descent ascent update. the experimental results are somewhat weak and have only been applied for toy examples. the authors make a further fine-grained characterization of the dynamics, using invariant function analysis.","The paper studies the convergence of a generalized gradient descent ascent (GDA) flow in certain classes of nonconvex-nonconcave min-max setups. While the nonconvex-nonconcave setups are computationally intractable and GDA is known to converge even in some convex-concave setups, the paper argues that (generalized) GDA can converge on what is dubbed ""Hidden Convex-Concave Games"" in the paper and argued that it contains GANs as a special case. The reviewers all found the paper interesting and a worthy contribution to the literature on nonconvex-nonconcave zero-sum games. Main concerns expressed by the reviewers were w. r. t. the lack of convergence rate established for the considered dynamics, novelty compared to existing work, and practical usefulness of the considered scheme, as it involves preconditioning/matrix inversion. The authors made an effort to address all the concerns, to the extent possible. Given the complexity of nonconvex-nonconcave min-max setups, their importance in GAN training, and the insightful perspective on hidden convexity/concavity in typical problem instances, I would like to see this paper published at ICLR. I would, however, strongly advise the authors to take all of the reviewers' comments into account when preparing a revision.",0.8169155120849609,0.29432106514771783
. (2019). a).... (2019).....................................,"This paper studies off-policy learning of contextual bandits with neural network generalization. The proposed algorithm NeuraLCB acts based on pessimistic estimates of the rewards obtained through lower confidence bounds. NeuraLCB is both analyzed and empirically evaluated. This paper received four borderline reviews, which improved during the rebuttal phase. The main strengths of this paper are that it is well executed and that the result is timely, considering the recent advances in pessimism for offline RL. The weakness is that the result is not very technically novel, essentially a direct combination of pessimism with neural networks. This paper was discussed and all reviewers agreed that the strengths of this paper outweigh its weaknesses. I agree and recommended this paper to be accepted.",0.798584520816803,0.12015068010077812
a) a collision-aware endpoint sampling. b) a collision-aware endpoint sampling. c) a collision-aware endpoint sampling. d) a collision-aware endpoint sampling. e) a collision-aware endpoint sampling. e) a collision-aware endpoint sampling. e) a collision-aware endpoint sampling. e) a collision-aware endpoint sampling. e) a collision-aware,"This paper proposes a joint multi-agent trajectory prediction framework for multiple agents using a ""heatmap"" estimation approach employing a hierarchical strategy and sparse image generation for for efficient inference. The method takes a set of predicted trajectories for each agent produces reorderings. The work yields a top result on a competitive leaderboard. While multiple reviewers were initially concerned about the paper not making a single major contribution, the author response discussion helped to clear up the degree of novelty. Further experiments provided during the review also led to multiple reviewers increasing their score. In the end, all reviewers recommend acceptance of this paper. As such the AC recommends accepting this paper.",0.7845174074172974,0.2114623205824977
lack of formal system to define terms is an important source of confusion in CL research. lack of such a system is an important source of confusion in ML research. a fourth solution would be to naturally replace grand claims with precisely defined setups. ad-hoc created datasets are highly complex data distribution shift problem.,"This review paper presents a way of comparative assessment of continual learning. Reviewers all agreed that this work is interesting, unique with comprehensive coverage of the CL space. The proposed categorization, CLEVA-Compass, and its GUI have great potential to facilitate future CL work.",0.8299648761749268,0.2859010649845004
"-The paper provides a thorough empirical investigation of their algorithm, which shows the advantage of  -SDDP. Strengths: -The application to MSSO is interesting. Weaknesses: -Novelty of the overall framework is relatively limited. -How do RL algorithms, which focus on infinite horizon problems, get ported to the finite horizon case? -The discussion of MSSO vs MDP is bizarre to me. g. g., [4,","This paper applies deep learning to a problem from OR, namely multistage stochastic optimization (MSSO). The main contribution is a method for learning a neural mapping from MSSO problem instances to value functions, which can be used to warm-start the SDDP solver, a state-of-the-art method for solving MSSO. The method is tested on two typical OR problems, inventory control and portfolio management. The reviewers think that the idea is interesting, the empirical results are impressive, and the paper is well-written. However, there are reservations on its relevance to the ICLR community.",0.8371842503547668,0.34384904280304907
aaron carroll: paper is well written and well organized. but there are unclear statements that need further clarification. carroll: it would be great to see the performance of the algorithm when the curves converge. he says the derivations heavily borrow from existing work (e. DemoDICE is not better than BC for beta=0. carroll: the paper is well written and well organized.,"The paper presents a method for learning sequential decision making policies from a mix of demonstrations of varying quality. The reviewers agree, and I concur, that the method is relevant to the ICLR community. It is non-trivial, the empirical evaluations and theoretical analysis are rigorous, resulting in a novel method that produces near optimal policies from more readily available demonstrations. The authors revised the manuscript to reflect the reviewers' comments.",0.8347001671791077,0.29786065965890884
"the authors state that ""Due to the high cost of training JT-VAE and HierVAE, we did not tune their hyperparameters and instead used the default values."" The paper is throughout very well-written and clear. The paper is throughout very well-written and clear.","Most reviewers were positive about the paper, seeing that the proposed method is practical and has convincing experimental performances. One reviewer was a bit negative and raised questions about clarity. After the authors responded, the negative reviewer didn't respond further. After reviewing all the comments, the AC feels that there is enough support from reviewers to accept this paper.",0.8465191125869751,0.3512137606739998
False positive: the proposed algorithm is a classical clustering algorithm. False negative: the proposed algorithm is a classical clustering algorithm. Accelerated K-medoids is an improved version of Greedy K-medoids. the authors compare the empirical computational time of the Greedy K-medoids and the Accelerated one.,"This paper deals with the important topic of active transfer learning. All reviewers agree that while the paper presents some shortcomings , it is considered to be a worth contribution.",0.8330641388893127,0.14283057767897844
"if not, The paper is well-written and is easy to follow. The paper is well written and is easy to follow. r... r.. r. Despite the simplicity of Fish, it achieves non-trivial improvement on benchmarks such as IRM. r.","This paper proposes a new method for domain generalization. The main idea is to encourage higher inner-product between gradients from different domains. Instead of adding an explicit regularizer to encourage this, authors propose an optimization algorithm called Fish which implicitly encourages higher inner-product between gradients of different domains. Authors further show their proposed method is competitive on challenging benchmarks such as WILDS and DomainBed. Reviewers all found the proposed algorithm novel and expressed that the contributions of the paper in terms of improving domain generalization is significant. A major issue that came up during the discussion period was that we realized that the presented results on WILDS benchmark are misleading. In particular, the following statements in the manuscript are false because on ""CivilComments"" and ""Amazon"", Fish utilizes a BERT model (Devlin et al., 2018). However, other methods at WILDS benchmark use DistilBERT (Sanh et al., 2019): - Section 4.2: ""For hyper-parameters including learning rate, batch size, choice of optimizer and model architecture, we follow the exact configuration as reported in the WILDS benchmark. Importantly, we also use the same model selection strategy used in WILDS to ensure a fair comparison."" - Appendix C2: ""Results: We compare results to the baselines used in the WILDS benchmark over 3 random seed runs in Table 10. All models are trained using BERT (Devlin et al., 2018)."" Authors explained that the mismatch is because at the time they evaluated their model, an earlier version of WILDS benchmark was available but they later updated other methods' results on a newer version of WILDS benchmark. Of course, I do not think that this explanation makes the misleading statements OK. Authors promised to do the following for the camera-ready version to make sure it is not misleading: - Using ""Worst-U/R Pearson r"" as the comparison measure for ""PovertyMap"" - Submitting their method to WILDS benchmark making sure everything matches the baselines and then reporting the results on ""Amazon"" and ""CivilComments"" datasets. Therefore, I recommend acceptance and I hope that authors would stick to their promise and update the manuscript to include these changes.",0.810697615146637,0.2743238403901949
a-OSE is a good example of a pretrained language model. a-OSE is a good example of a pretrained language model. a-OSE is a good example of a pretrained language model. a-OSE is a good example of a pretrained language model.,"This paper presents work on open-world object detection. The main idea is to use fixed per-category semantic anchors. These can be incrementally added to when new data appear. The reviewers engaged in significant discussion around the paper with many iterations of improvements to the paper. Initial concerns regarding zero-shot learning were addressed, as were remarks on presentation and claims. In the end the reviewers were split on this paper. I recommend to accept the paper on the basis of the semantic topology ideas and the thorough experimental results. The remaining concern centered around the evaluation protocol used in the paper, which follows that in the literature (e. g. Joseph et al. CVPR 21). While this is not a fatal flaw, it is an issue with how this genre of methods is evaluated. It would be good to add discussion to the final paper to highlight this as an opportunity for future work in the field to address. Specifically, as a reviewer noted ""after detecting ""unknown"" objects in T1, the (hypothetical) annotation process provides boxes for ALL objects of some new classes instead of only for those that have been correctly detected (localized and marked ""unknown"").""",0.7995722889900208,0.20664328483066388
a hidden-parameter-MDP (HiP-MDPs) encodes the time-varying aspect of the dynamics. a strong argument is that the model is a 'deep-net' and that it is a 'deep-net'. a weak argument is that the model is a 'deep-net' and that it is not a 'deep-net',"The paper addresses a few very important points on sequential latent-variable models, and introduce a different view on meta-RL. Even though the methods that this paper poses are incremental, it is such a hot-debated topic that I would prefer to see this published now.",0.8355391025543213,0.32686019192139304
"a. The paper is well-written and presents an interesting approach to solving TSP. the only weakness is that it states that combinatorial approaches, e. The paper is well-written and presents an interesting approach to solving TSP that can outperform other existing approaches.","In this paper, a novel machine learning-based method for solving TSP is presented; this method uses guided local search in conjunction with a graph neural network, which is trained to predict regret. Reviewers disagree rather sharply on the merits of the paper. Three reviewers think that the paper is novel, interesting, and has good empirical results. Two reviewers think that the fact the results are not competitive with the best non-learning-based (""classic"") solvers mean that the paper should be rejected. This area chair believes that research is fundamentally not about beating benchmarks, but about new, interesting, and sound ideas. The conceptual novelty of this method, together with the good results compared with other learning-based methods, is sufficient for accepting the paper.",0.8543055653572083,0.41169759941597783
RMSE is low enough to suggest a perfect fit. Strong Points Strong Points Strong Points.. Figure 4 is difficult to follow.... t. t. t...  and. t. t. t.,"The paper examines the approach of modeling aleatoric uncertainty by fitting a neural network, that estimates mean and variances of a heteorscedasitic Gaussian distribution, based on log likelihood maximization. The authors identify the problem that gradient based training on the netgative log likelihood (NLL) may result in suboptimal solutions where a high predicted variance compensates for the predicted mean being far from the true mean. To solve this problem, the authors suggest to adjust the log likelihood objective by weighting the log likelihood of each single data point by the corresponding beta-exponentiated variance estimate. This adjusted objective is referred to as beta-NLL. All reviewers agreed that the identified problem and the proposed solution are interesting, that the paper is well written and organized, and that the contributions are significant and somewhat new. The main criticism was on the side of the empirical evaluation. It was criticized that the empirical analysis did not compare the proposed method to other approaches to solving the same problem, that the identified problem and the proposed method should be also analyzed on high-dimensional data, that the results on the synthetic experiments could be improved by investigating more than a single run and by incorporating the the MSE in corresponding Figure 1, and that standard errors were not reported. Based on the reviews the authors added several new experiments and investigations in the revised version of their manuscript to improve their empirical analysis: 1) new experiments on high-dimensional data sets were conducted applying variational autoencoders on MNIST and Fashion MNIST and performing Depth-map prediction from images on the NYUv2 dataset. 2) For comparison several baseline methods were added to the experiments on the UCI and the dynamics datasets. 3) Three more UCI datasets (carbon, superconductivity, wine-white) were included in the empirical analysis. 4) An evaluation of calibration of predictive variance for the heteroscedastic sine dataset was added. 5) Several more repetitions of the experiment represented in Figure 1 were conducted. (6) An analysis of undersampling behavior on FetchPickAndPlace was added. Moreover, the authors reported two errors in their previous experiments that they discovered and corrected. All reviewers were satisfied with the changes in the revised version and the answers to their specific questions and increased their scores accordingly, now commonly voting for acceptance. The paper should therefore be accepted.",0.7894980311393738,0.18410778986290097
a diagram or table for the UNET showing layer numbering may be useful. a diagram or table for the UNET showing layer down-sampling rates may be useful. a diagram or table for the UNET showing layer numbering may be useful. a diagram for the UNET showing layer numbering may be useful.,"The paper proposes using the intermediate representation learned in a denoising diffusion model for the label-efficient semantic segmentation task. The reviewers are generally positive with the submission. They like the simplicity of the proposed algorithm. They also like the effort of the paper in verifying the intermediate representation learned by a diffusion model is semantically meaningful and can be used for segmentation. Initially, there was some concern about the size of the validation set, which is addressed by the rebuttal. Consolidating the reviews and rebuttals, the meta-reviewer agrees with the assessment of the reviewers and would like to recommend acceptance of the paper.",0.819140613079071,0.24233420689900714
"FWSVD is a very simple approach to model-based training. it is not clear whether the proposed method comes with any guarantees. if so, will the new method always be better than SVD? -) I've updated my score to 6.","The paper studies the problem of task-specific model compression obtained from fine-tuning large pre-trained language models. The work follows the line of research in which model size is reduced by decomposing the matrices in the model into smaller factors. Two-step approaches apply SVD and then fine-tuned the model on task specific data. The present work makes the observation that after the first step (the SVD compression) the model can dramatically lose its performance, due to the mismatched optimization objectives between the low-rank approximation and the target task. The work provides evidence backing this claim. The paper proposes to address this problem by weighting the importance of parameters for the factorization according to the Fisher information. Experimental evaluation shows that the proposed method can achieve better results than variants that use truncated SVD of the weight matrices. The paper is well written and easy to read. The method is simple and effective and can be applied to in a wide range of settings. The authors provided a thorough response which clarified several points. This led Reviewer Kuwu to increase the score to 6. All three reviewers agree that the main observation in the work is interesting and informative for researchers and practitioners working on the problem. Reviewer jnTC points out that the paper would have been stronger if it included theoretical exploration of the reasons behind the ""importance of low SVs"" phenomenon. Reviewer Kuwu and jnTC consider the results marginally novel. Reviewer Kuwu considered the significance of the reported results to be limited, and put the work marginally above the acceptance threshold. Reviewer jnTC disagrees with this view, considers and appreciates the generality of the method and the fact that it can work well even for compressed models, while improving in accuracy by a few percent over competing approaches which result in similar parameter counts. The AC agrees with Reviewer jnTC. Overall all reviewers consider the paper borderline but recommend accepting the paper. The AC overall the topic important (reducing the footprint of language models), the method simple and well motivated. The empirical evaluation is very thorough and shows clear gains across a large number of settings.",0.8231820464134216,0.35290542226284743
"The empirical evaluation is thorough as it considers multiple test problems and benchmark methods. Strengths The paper is well-written and presents an interesting approach to solving MOCO problems. Strengths The paper is well-written and presents an interesting approach to solving MOCO problems.. There are some minor typos (e. g., ""a exceptionally"" -> ""an exceptionally"", p. 4). The paper is well-written and presents an interesting approach to solving MOCO problems. There are some minor typos","This paper develops a ``preference-conditioned” approach to approximate the Pareto frontier for Multi-Objective Combinatorial Optimization (MOCO) problems with a single model (thus dealing with the thorny problem that there can be exponentially-many Pareto-optimal solutions). It appears to provide flexibility for users to obtain various preferred tradeoffs between the objectives without extra search. The basic idea is to use end-to-end RL to train the single model for all different preferences simultaneously. The technical soundness and practical performance are strong. This work's approximation guarantee depends on the ability to approximately solve several (weighted) single-objective problems. This may be challenging due to the NP-hardness of the latter. However, this limitation seems to also apply to other end-to-end learning-based approaches. One area where the novelty is somewhat limited is that the paper borrows some number of ideas from neural single-objective optimization. The contribution overall seems noteworthy for hard multi-objective problems.",0.8172099590301514,0.35689296500964296
a scalar is used to predict z_s from c_t. a scalar is used to predict z_s from c_t. a scalar is used to predict z_s from c_t.,"This paper proposes a prototypical contrastive predictive coding by combining the prototypical method and contrastive learning, and presents its efficient implementation for three distillation tasks: supervised model compression, self-supervised model compression, and self-supervised learning via self-distillation. The paper is well-written, and the effectiveness of the proposed method is validated through extensive experiments. Reviewers generally agree the paper has clear merits despite some weaknesses for improvement. Overall, I would like to recommend it for acceptance and encourage authors to incorporate all the review comments and suggestions in the final version.",0.8068939447402954,0.14325389824807644
"a causal graph motivated adversarial learning approach is proposed. the strength of this paper is in the formulation of adversarial examples as a causal problem. the paper derives a method based on the graph criterion. the paper has a lot of strengths, but the results are not convincing.","The paper shows a causal perspective to the adversarial robustness problem. Based on a causal graph of the adversarial data creation process, which describes the perceived data as a function of content and style variables, the authors identified that the spurious correlation between style and label is the main reason for adversarial examples. Based on this observation, they propose a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. Experiments on MNIST, CIFAR-10, and CIFAR-100 datasets show that the proposed method is better than two baselines, Madry and TRADES. Overall, the paper contains interesting ideas and tackles an important problem. Due to some concerns regarding the clarity and motivation of the paper, we strongly recommend the authors take the reviewers' comments to heart and incorporate their thoughts in preparing the camera-ready version of their manuscript.",0.8559608459472656,0.5126201522847017
a smooth reformulation is presented which can be solved using standard riemannian gradient descent algorithm. Weakness: - Generalization to multiclass sensitive attribute is missing. Strength: The paper is nicely written and easy to follow. Weakness: - Generalization to multiclass sensitive attribute is missing. Strength: - The paper is nicely written and easy to follow.,"This paper considers the problem of distributionally robust fair PCA for binary sensitive variables. The main modeling contribution of the paper is the consideration of fairness and robustness of the PCA simultaneously, and the main technical contribution of the paper is the provision of a Riemannian subgradient descent algorithm for this problem and proof that it reaches local optima of this non-convex optimization problem. The results will be of interest to those working at the intersection of fair and robust learning.",0.8323949575424194,0.338707893093427
"the budget of GIA is not directly comparable. the budget of GIA is not directly comparable. However, in practice, this may not be realistic as some features might be harder to manipulate. The novelty of this paper is somewhat weak, it is quite natural to penalize the homophily change brought by the perturbations.","The reviewers agree that this paper studies an important problem, provides theoretically analysis to understand graph injection attack. The authors propose a new regularizer to improve the attack success. Extensive experimental results also show the effectiveness of the proposed method.",0.8421515226364136,0.275176572923859
"e. I believe that this is a well-written paper, with solid motivation, and thoughtful care in crafting the approach. existing work in Hierarchical RL that learns separate high- and low-level policies do not make this perfect transition assumption. e. I am also wondering if it might be possible to reuse established research on multi-agent modelling.","This paper proposes a cognitive science-inspired interaction setting between two agents, an ""architect"" and ""builder"", in which the architect must produce messages to guide the builder to achieve a task. Unlike other related settings (such as typical approaches in MARL, HRL, or HRI), the builder does not have access to the architect's reward function, and must learn to interpret the architect's messages by assuming the architect is telling it to do something sensible. At the same time, the architect determines what is ""sensible"" by building a model of the builder's behavior and planning over it. This setting is common particularly in human-agent interactions, where humans may not be able to either (1) accurately communicate a scalar reward or (2) provide demonstrations, but can still provide information that the agent ought to be able to learn from. The paper demonstrates that the learned communication protocol generalizes well to new settings. While this paper generated a lot of discussion, the reviewers did not come to a consensus on whether the paper should be accepted or rejected, with those in favor of the paper maintaining it should be accepted and those not in favor maintaining that it needs work. I have therefore done a particularly close read of both the paper and the discussion in order to weigh the pros and cons brought up by the reviewers. The positive reviews clearly indicate that this work is insightful and of interest to researchers in the ICLR community (in fact, all reviewers mentioned they found the work interesting and well-written). In particular, Reviewer hMeT wrote: ""I am positive about this framework as it presents a better model for multi-agent communication, especially enriching the communication among agents over the fixed, restricted reward-based communication protocol in traditional RL."" I am inclined to agree with this assessment and find the communication setting studied in this paper to be much more ecologically valid for human-agent interaction settings than having humans communicate scalar rewards or provide demonstrations: humans are typically poor at the former and may not have the same embodiment to achieve the latter. The negative reviews focused on a few cons: (1) the assumption that the architect has access to a ground-truth environment model, (2) confusion about differences from other related fields (e. g. feudal RL, MARL), and (3) lack of analysis of the communication protocol. I have considered these points, but do not feel any of them are fatal flaws: (1) From the perspective of human-agent interaction, I think it is very reasonable to assume that a human architect would have a good model of the world and would be generally proficient at solving tasks in the world. Making this approach work in the setting where the architect is *also* learning how the world works seems squarely in the domain of future work. (2) The authors have done an extensive job of clarifying the differences between these related areas, and as discussed above, other reviewers found the way in which AGP is different to be insightful and ecologically valid. (3) This is potentially the most serious con: as the discussion with Reviewer BHGy brought up, the learned communication protocol may just be a simple mapping between messages and environment interactions. After further discussion in which the authors argued that learning a simple mapping is not a problem---the main question is how to even induce such a mapping in the first place---the reviewer acknowledged that this is not a fatal flaw but that makes the results somewhat less interesting. In summary, the positive reviews highlighted the interestingness and insightful nature of the questions studied in this paper and have convinced me that this paper will be of interest to the ICLR community as it has provides a new perspective on the problem of agent-agent interaction (particularly for the special case of human-agent interaction). The negative reviews did highlight a few limitations of the paper, but I expect these can be addressed by future work and do not feel they outweigh the interestingness of the problem. In light of this, I recommend acceptance as a poster. Suggestion for the authors: I found the discussion with Reviewer BHGy to be particularly insightful and helpful in understanding the aims of the paper. I would encourage you to incorporate some of this into the camera-ready version of the paper, and perhaps to lean more heavily on the special case of human-agent interaction as motivation of this work (as also hinted at by Reviewer hMeT).",0.8214849233627319,0.4406927740709349
"Generally, the writing quality is not good and hard to read. tanh tends to work better. Almost ironically, I couldn’t find any direct definition of what comprises phase collapse, which made matters all that much harder. -The results are not state of the art.","This paper proposes that the superior performance of modern convolutional networks is partly due to a phase collapse mechanism that eliminates spatial variability while ensuring linear class separation. To support their hypothesis, authors introduce a complex-valued convolutional network (called Learned Scattering network) which includes a phase collapse on the output of its wavelet filters and show that such network has comparable performance to ResNets but its performance degrades if the phase collapse is replaced by a threshold operator. Reviewers are all in agreement about the novelty and significance of the work. They also find the empirical results compelling. The main weakness of this work which was highlighted by all reviewers is clarity. The paper can be significantly improved in terms of the writing. While I am recommending acceptance, I strongly recommend authors to take reviewers' feedback into account and improve the writing significantly for the final version so that more people would benefit from this paper and build on it in the future.",0.8283875584602356,0.3697292150131294
g. The Librispeech 960 numbers are quite strong. g. The synthetic noise is used during training as well. the authors promise to release code upon acceptance of the paper. a... paper is well written with several strengths.,"This paper proposed a self-supervised speech pre-training approach, by the name of SPIRAL, to learning perturbation-invariant representations in a teacher-student setting. The authors introduced a variety of techniques to improve the performance and stabilize the training. Compared to the popular unsupervised learning model wav2vec 2.0, better WERs were reported using SPIRAL with a reduced training cost. All reviewers considered the work solid with sufficient novelty but also raised concerns regarding the generalization under unseen real-world noisy conditions and missing decoding details. The authors responded with new Chime-3 results and updated LM decoding results. The new results show that, after a bug fix, SPIRAL can outperform wav2vec 2.0 when no external LM is used. Overall the proposed approach is technically novel. The experiments are extensive and the results are compelling. In addition, the training time can be significantly reduced compared to wav2vec 2.0. All reviewers are supportive. So I would recommend accept.",0.834686815738678,0.2753561359721345
"-The proposed method provides some useful results, but there are some questions needed to answer. Strengths: -The proposed method provides some useful results, but there are some questions needed to answer. Weak points: -The proposed method provides some useful results, but there are some questions needed to answer.","To solve imbalance classification problem, this paper proposes a method to learn example weights together with the parameters of a neural network. The authors proposed a novel mechanism of learning with a constraint, which allows accurate training of the weights and model at the same time. Then they combined this new learning mechanism and the method by Hu et al. (2019), and demonstrated its usefulness in extensive experiments. I would like to thank the authors for their detailed feedback to the initial reviews, which clarified most of the unclear points in the manuscript. Overall the paper is well written and the effectiveness was demonstrated in experiments. Since the contribution is valuable to ICLR2022, I suggest its acceptance.",0.8299959301948547,0.31854312920144623
the results are very interesting.. the results are very interesting. a more compelling set of visual examples. a discriminator is a discriminator is a discriminator is a discriminator is a discriminator work is discriminating motion features..... is. -.... - - - - work is a discriminator - work is work is,"This work tackles video generation using implicit representations, and demonstrates that using these representations enables improvements to long-term coherence of the generated videos. Reviewers praised the writing, the thorough experimental evaluation, and the strong quantitative results. Some concerns were raised about a lack of discussion of relevant related work, novelty/significance, model architecture, and a lack of qualitative examples, many of which the authors have tried to address during the discussion phase. Several reviewers raised their ratings as a result. Personally I certainly believe that exploring implicit representations for video is important, and I know of no published prior work in this direction, which amplifies the potential significance of this work. Even if results are qualitatively worse than previous work in some ways, this exploration is still valuable and worth publishing. While the paper ultimately received one reject rating, another reviewer chose to champion this work and award it the highest possible rating. Combined with the other positive reviews, this provides plenty of convincing evidence for me to recommend acceptance. That said, given the rating spread, I would like to encourage the authors to consider the reviewers' comments further as they prepare the final version of the manuscript. Especially providing more qualitative results would be a welcome addition.",0.7958204746246338,0.2981271060449736
"a number of typos and phrasings that could be improved. the paper's results seem well executed, but there are limitations. a limitation seems to be that of implicit assumptions made by the method. a similar approach has been proposed for HACO.","The paper presents a new algorithm for augmenting RL training with human examples, and this is applied to learning safe driving policies. This algorithm is properly tested and compared to other relevant algorithms with favorable results. Reviewers agree that this is good work and that it should be published. Reviewers had multiple questions, which were in my opinion answered satisfactorily by the authors. Notably, the authors ran additional tests against other human-in-the-loop RL algorithms with good results. In sum, this seems to be a solid paper, worth accepting.",0.8479002118110657,0.340733228251338
or translate) during training. they perform multiple experiments using two pretrained models (mBERT and XLM-R) and show that their proposed approach improves over multiple strong baselines....: The paper is well written and the authors perform a nice analysis/investigation of smaller research questions in Section 6. The paper,"This paper proposes X-Mixup, a model that considers the source languages and target languages together for cross-lingual transfer. The designed model takes a pair of sentences (or the translated sentences) in a source language and a target language as the input and computes the cross-attention between them. The empirical results are convincing. Reviewers think this paper is well-written and the idea is interesting.",0.8400449156761169,0.38533390685915947
"the paper is well written and the results are well supported. The paper is well written and the results are well supported. the paper is well written and the results are well supported.. and Evolutionary Computation............. r....[4], which is the most famous QD algorithm. [3]. [1]...","This paper introduces a novel quality-diversity algorithm, ""Evolutionary Diversity Optimization with Clustering-based Selection (EDO-CS)"", and applies it to reinforcement learning. A bandit approach (UCB) is used to select which cluster to sample parents from. The QD algorithm can be evaluated on its own, outside of the RL context, and if so it should be compared to the several approaches to niching and other standard diversity preservation approaches in evolutionary computation that rely on clustering. (And the authors should make an effort to connect to the niching literature in particular.) However, the use of the algorithm for RL makes it possible to use behavioral features as the space in which to cluster, separating it from standard diversity preservation methods. The resulting algorithm is relatively simple and the empirical results are good. Some of the main concerns for reviewers included the bibliography, which the authors promptly acted on by citing several suggested papers and comparing their approach where relevant. There was also discussion about the exact novelty of the paper, for example as compared to the CVT-MAP-Elites algorithm, but this was clarified by the authors. Reviewers agree that the paper is easily to follow and well-written. Based on this, it seems that the paper makes a clear contribution to QD methods for RL, and is worth accepting.",0.8210296034812927,0.43561542306154494
"e. Run time is also lower when image resolution increases, compared with CasMVSNet and others. e. The fairness of the experimental setup is in question. a) On the union of DTU and Blended-MVS datasets.",All reviewers recommended accept after discussion. I am happy to accept this paper.,0.8454235792160034,0.1277879737317562
"section 3 is quite easy to follow.. the problem of understanding statistical limit of offline RL algorithms, under linear representations................","In this paper, the authors motivate the paper well by the gap between the upper bound of the popular offline RL algorithm and the lower bound of the offline RL. By exploiting the special linear structure, the authors designed a variance-aware pessimistic value iteration, in which the variance estimation is used for reweighting the Bellman loss. Finally, the upper bound of the proposed algorithm in terms of the algorithm quantity is proposed, which is more refined to reflexing the problem-dependency. These results are interesting to the offline RL theoretical community. As the reviewers suggested, several improvements can be made to further refinement, e. g., - The intuition about the self-normalization in the algorithm exploited to improve the upper bound should be introduced. - The discussion in Sec 3.3t about the insight of the improvement of the upper bound is not sufficient. - The extra computational cost about the variance should be discussed.",0.8358197808265686,0.4240871565416455
"NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search. ""The authors name a lot of related works, but compare only few of them in the experiments. If the embedding layer is added, the model size will increase a lot, and the compression ratio will decrease.","This paper reviews a number of parameter decomposition methods for BERT style contextual embedding models. The authors argue for the application of Tucker decomposition to the attention and feedforward layers of such models. Evaluation is performed for a range of models on the GLUE benchmark. Further ablation studies indicate that the distillation procedure employed is crucial for obtaining competitive results and the raw decomposition approaches are ineffective at directly approximating the original pre-trained model. Strengths: The reviewers generally agree that the methods explored and results presented in this paper are interesting and could be of use to those deploying large embedding models. The authors review a range of possible decomposition methods and use this to motivate their approach. The resulting levels compression are high while maintaining good performance, while the ablation study clearly shows the contribution of the various steps of the training pipeline. Weaknesses: The main weakness identified by the reviewers is the incremental nature of this work in comparison to previous works applying various decomposition and compression techniques to neural networks. They also highlight that many of the techniques discussed early in the paper are not compared in the evaluation. The authors have effectively responded to this issue by providing further comparisons and justification for their modelling choices (e. g. not compressing the embedding layers). Overall, despite the incremental nature of this work, I believe that there are enough though provoking ideas and results presented to warrant publication. Interestingly, as the authors emphasise in their response, the ablation study highlights that this work is not really about approximating the original models weights, as all of the work appears to be being done by the distillation procedure in concert with the choice weight decomposition. In general I wonder whether this paper would be better presented as exploring a structured distillation procedure rather than weight compression.",0.8300735354423523,0.3846145773927371
the CLOP layer is relatively easy to use and the results are clear and easy to follow. the results are clear and easy to follow............,"This paper presents a novel regularization technique for CNNs based on swapping feature vectors in the final layer. It is demonstrated that this simple technique helps with generalization in supervised learning and RL with image inputs. Following the author rebuttal, all reviewers agreed that the simplicity of this method and the nice empirical performance it obtains is important to report to the community. In this respect, I agree with the reviewers, and recommend acceptance. One important issue that came up during the discussion is how much this work is related to RL, and the authors SL experiments helped to put the contribution in a broader context. Indeed, one way to see the results of this work is that if such performance improvement is obtained in the Procgen benchmark with just image-based regularization, perhaps this benchmark is not very suitable for studying generalization in RL (where we expect that more sophisticated techniques would be required). In addition, I can think of RL domains (e. g., Tetris, which was mentioned in the discussion) where I would not expect the proposed method to help. It would be good if the authors discuss these issues in some capacity in their final version. Please take all reviewer comments into account when preparing the final version.",0.8168785572052002,0.28530605509877205
"e., RPN+CLIP, ViLD-image/text/ensemble) The empirical results are very encouraging. but the authors need to clarify that the training details are slightly different. e., the authors need to clearly specify that the settings are slightly different.","the aim of this work is to produce an open-vocabulary detector. The approach is via knowledge distillation from existing large-scale V+L models, and the evaluation is based on novel classes with LVIS. The reviewers were generally happy with the work (approach and results), but there were substantial points of clarification during discussion that need to be properly integrated into the final manuscript.",0.8369575142860413,0.33633161087830865
I do not see how this paper would be able to be made clearer. the paper is clear and easy to follow. the paper is clear and easy to follow.. The paper is clear and easy to follow... The,"**Summary** This paper proposes a novel offline model-based meta-RL approach called MerPO. MerPO combines conservative value iteration with proximal RL policy iteration. The proposed method is novel despite having some similarities to approaches like COMBO. The paper compares against it in the experiments. The paper provides both empirical and theoretical justification for the proposed approach. **Final Thoughts** Overall, I think the authors did a pretty good job at addressing the reviewers' concerns. Overall, I think this is an interesting contribution to the ICLR community. The reviewers were all positive about this paper. For the camera-ready version of the paper, I would recommend the authors to go over the reviewers' concerns again and make sure that those concerns are addressed in the paper too as they did in the rebuttal. Some captions are pretty short; for example, see the captions of figure 6 and figure 7. I would recommend the authors add more description to the captions in the camera-ready version of this paper.",0.8118997812271118,0.32559347063810984
"NNGP is a relatively recent development. but the paper is a bit complex and difficult to follow. the paper is well-written overall but I often found it difficult to follow properly. the paper is not a complete work in itself. it is a good paper, but I would like to see it expanded.","This paper presents a new formulation for the infinitely wide limiting case of deep networks as Gaussian processes, i. e. NNGPs. The authors extend the existing case to incorporate a scale term at the penultimate layer of the network, which results in a scale mixture of NNGPs or a Student-t process in a specific case. This formulation allows for a more heavy tailed output distribution which e. g. can be more robust to outliers. The four reviews averaged just above borderline, with a 5, 8, 6, 6. The reviewers found the approach to be sensible, technically correct and timely given the recent literature. They found the experiments to be compelling for the most part, demonstrating the added robustness of this approach over the baseline NNGP. The main concern raised by the reviewers is that the work is incremental, given that both NNGPs and Student-t processes are already established.",0.8272529244422913,0.3480899571017786
the paper is well-written and enjoyable to read. The technical strengths may not be strong enough. The authors use worst-case analysis of the convolutional structure in Theorem 2's proof. the authors use a gaussian distribution of the random variables (and Gauss-Lipschitz concentration e. i. [5-6]).,This paper focuses on understanding how the angle between two inputs change as they are propagated in a randomly-initialized convolutional neural network layers. They demonstrate very different behavior in different settings and provide rigorous measure concentration results. The reviewers thought the paper is well written and easy to read with nice theoretical results. They did raise a variety of technical concerns that were mostly addressed by the authors rebuttal. My own reading of the paper is that this is a nice contribution. I therefore agree with the reviewers and recommend acceptance.,0.8362328410148621,0.3778641285995642
"--- Main Review --- Q is set low, but what happens if you only sample from Q? Strengths: --- --- --- - - no experiments to study hallucination/memorization. Strengths   Knowledge ---     .?.. Strengths limited evaluation - no experiments to study hallucination Strengths","The authors study the problem of open-ended knowledge-grounded natural language generation, in the context of free-form QA or knowledge-grounded dialogue, focusing on improving the retrieval component of the retrieval-augmented system. By retrieving more relevant passages, the generations are more grounded in retrieved passages. Pros: + The paper is clearly written and motivated. + Presents a straightforward approach that shows improvement over a strong baseline. + A strong paper focuses on a rather under-explored problem of knowledge-grounded open-ended generation, proposing novel objective, significant empirical improvements on two datasets in multiple metrics. + The authors included human evolution results to support their findings. + The authors did a good job addressing several questions raised during review period and added several new experiment results and discussions to strengthen their findings. The reviewer team was generally satisfied. Cons: + Several related work on knowledge guided dialog response generation is missing in the paper. Although the paper's focus is on retrieval based QA systems, the main focus is on open domain generation, which has overlaps with dialog response generation research. So the authors should cite the following papers in their paper: [1] Dinan, Emily, et al. ""Wizard of wikipedia: Knowledge-powered conversational agents."" arXiv preprint arXiv:1811.01241 (2018). [2] Zhou, Kangyan, Shrimai Prabhumoye, and Alan W. Black. ""A dataset for document grounded conversations."" arXiv preprint arXiv:1809.07358 (2018). [3]Zhan, Haolan, et al. ""CoLV: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation."" Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021. [4]Zhao, Xueliang, et al. ""Knowledge-grounded dialogue generation with pre-trained language models."" arXiv preprint arXiv:2010.08824 (2020). + There are several related work concerning with generation of a response given a relatively small set of evidence text such as the following ones: [5]Lian, Rongzhong, et al. ""Learning to select knowledge for response generation in dialog systems."" arXiv preprint arXiv:1902.04911 (2019). [6]Kim, Byeongchang, Jaewoo Ahn, and Gunhee Kim. ""Sequential latent knowledge selection for knowledge-grounded dialogue."" arXiv preprint arXiv:2002.07510 (2020). Although these work do not include a retrieval part, the authors should cite and discuss similarities and differences to [5] & [6] in their paper.",0.7608179450035095,0.25376012081901234
-. -The paper is very well written with the 3 goals clearly stated and how they were addressed. The paper is very well written with the 3 goals clearly stated and how they were addressed. The paper is very well written with the 3 goals clearly stated and how they were addressed.,"This work tackles an important clinical application. It is experimentally solid and investigates novel deep learning methodologies in a convincing way. For these reasons, this work is endorsed for publication at ICLR 2022.",0.8318678140640259,0.21933938811222714
4.2 is a very interesting paper. it addresses an important problem of how to fit rapidly growing model sizes into slower growing device memory. Much of the material in 3.4 is prior work and so could be written more briefly and systematically. Weakness: There is no strong weakness of this paper.,"The paper proposes a new pipeline-parallel training method called WPipe. WPipe works (on a very high level) by replacing the two-buffer structure of PipeDream-2BW with a two-partition-group structure, allowing resources to be shared in a similar way to PipeDream-2BW but with less memory use and less delays in weight update propagation across stages. The 1.4x speedup it achieves over PipeDream-2BW is impressive. In discussion, the reviewers agreed that the problem WPipe tries to tackle is important and that the approach is novel and interesting. But there was significant disagreement among the reviewers as to score. A reviewer expressed concern about the work being incremental and difficult to follow. And while these were valid concerns, and the authors should take note of them when revising their paper, I do not think they should present a bar to publication, both based on my own read of the work and also in light of the fact that other reviewers with higher confidence scores did not find novelty to be a disqualifying concern. As a result, I plan to follow the majority reviewer opinion and recommend acceptance here.",0.8328096866607666,0.32287783455103636
"the paper is well organized. The authors only say it is a large family of distributions while it is unclear and not so intuitive for readers. the results are novel, interesting, and non-trivial. the paper is well organized. I like the flow of the paper.","The provides a complexity theoretic look at GANs. The exposition is multi-disciplinary, and in my personal opinion, it is an interesting look at the GANs in the context of random number generators.",0.8565265536308289,0.31542015075683594
I think this claim should be more precise. Figure 10 (b - e) the proposed method is efficient and extensive experiments are provided to prove the effectiveness. (i) The manuscript is incomplete as some discussions are inconsistent with each other. (ii) The authors are not clear on their conclusions.,Most of the reviewers think this paper is clearly a valuable addition to ICLR based on the convincing theoretical analysis and extensive experimental results. Please refer to reviewers's review for more detailed discussions of the pros and cons of the paper.,0.8502947092056274,0.35104934871196747
0.7. Pros: 1. The paper is well written. Pros: 2. The paper is well illustrated. Pros: 3. The paper is well illustrated. (1) The paper is well illustrated. S3. The overall pipeline is simple.,"This paper received 5 quality reviews, with 3 of them rated 8, 1 rated 6, and 1 rated 5. In general, while there are minor concerns, the reviewers acknowledge the contribution of applying Knowledge distillation to the problem of monocular 3D object detection, and appreciate the SOTA performance on the KITTI validation and test sets. The AC concurs with these important contributions and recommends acceptance.",0.8396068811416626,0.3256860516965389
a) Adding more comparisons against other techniques would be great. b) Adding more experiments to compare EXACT with other techniques would be great. c) Adding more experiments to compare other techniques would be great. d) Adding more experiments to compare EXACT with other techniques would be great.,"There are numerous known methods for memory reduction used in CNNs. This paper takes two such---quantization (Q) and random projection (RP)---and applies them to GNNs. This is a novelty, but I agree with the reviewers: on its own this novelty would not be ""surprising"" enough to report at ICLR. The paper further goes to show empirically that these methods, when applied to a reasonable set of datasets, do indeed produce their predicted memory reductions (unsurprising) with a small ( ≈0.5% ) drop in accuracy (surprising, in the sense of not being something one could predict without doing the experiment). All of the above is in one sense ""just engineering"", with only a small inventive step. Any real-world deployment of GNNs would, if an army of engineers were available, naturally implement quantization and RP in order to see what kind of improvements they might make. This would be just two more hyperparameters (R,B) to add to the sweep, and the deployment would vary them until the required accuracy was achieved in the minimum time (OOM is a red herring - one would vary batch size, other compression, or ship values to CPU in order to make progress). However, ""simply adding two more hyperparameters"" is a significant increase in the deployment burden, which is where the paper's third contribution comes in: the theoretical analysis of the effects of the two processes, with straightforward but nonobvious calculations of the effect on gradient variance of the two processes, and, usefully, their interaction. The value of this theory is twofold: first, it gives us new tools to analyse such processes; and second, it allows us to be much more judicious in the selection of these hyperparameters. In all, the reviewers' objection of no great novelty in porting ideas from CNN to GNN is sustained; but the authors' claim as to the value of the theory is sustained, and no reviewer provides prior art to dispute the novelty of the theory calculations. The revised paper has already expanded the key sections in Appendix E, and added welcome experiments which strengthen the paper. I would encourage a final copy (and certainly the poster presentation) to emphasize some of the insights over the raw experimental numbers. As the authors hint, those numbers are subject to vagaries of what PyTorch happens to implement, while the underlying analyses are a little longer lasting. Some other comments: A lot of discussion time was spent on the question of whether 0.5% is negligible. This is entirely application dependent, and is part of the hyperparameter/architecture tuning process. - the extra time overhead of swapping ""can go up to 80%, which is not feasible in practice"". Not so: if choosing between OOM or 1.8x slowdown, I will of course choose the latter. - ""for a fair comparison, we do not change any hyperparameters"" Again, not relevant: in a real application (which is where this paper contributes), we of course change the learning rate when batch size changes. - ""the accuracy drop of sampling may be greater than EXACT"" Again, whether that drop is too much depends entirely on the actual application. And please do take a look at typos/grammar/English etc.",0.7945932745933533,0.35195055454969404
". The authors proposes on analyzing a special setting of finding a mixed Nash equilibrium, by first observing the convergent behavior of the limiting dynamics. the authors proposes on analyzing a special setting of finding a mixed Nash equilibrium, by first observing the convergent behavior of the limiting dynamics.","The authors first consider a mean field two player zero sum game and consider quasistatic Wasserstein gradient flow dynamics for solving the problem. The dynamics is proved to be convergent under some assumptions. Finally, the authors provide a discretization of the gradient flow and using this proposes an algorithm for solving min-max optimization problems. They use this algorithm for GAN's as the main example. Experimental results claim that the algorithm outperforms langevin gradient descent especially in high dimensionas. This paper sits right at the border. But subsequent to the author response, one of the reviewers has updated the score and seems more positive about the paper. In view of this, I am leaning towards an accept.",0.8364652991294861,0.3408307870849967
"(e.g. that p(|X) is always 1 and that the proposed ""wildcards"" are only used for the beginning part). Weak points The experiments are not real. strengths The paper is well written overall, easy to understand with clear definitions and examples. Weak points mostly the idea is based on SPRING (dynamic time warping, DTW) which is mentioned in the paper and applied for CTC.","This paper proposes an extension of CTC by considering the wild-card to adjust the label missing issues during training. The authors propose to minimize the loss over all possible sub-segments of the input to automatically align the one that matches the available transcript. It is empirically proved to significantly improve performance over CTC even if up to 40-70% label sequence is missed (overall performance similar to the complete label case) across different tasks. As agreed by the reviewers, the paper is well presented and the problem is interesting to a broad community. Dynamic time warping with unconstrained endpoints itself is not a new idea and a classical topic for speech recognition (e. g. word spotting). The contribution of the paper is the formal introduction of the approach to CTC and to give experimental results to confirm the effectiveness. Also the use of simulated data weakens the paper a bit. The decision is mainly based on the clear presentation and fair experimental justification.",0.8369622826576233,0.44286344274878503
the regret scaling is not needed. a. the problem setting is intriguing. the problem setting is more of an Elimination-style algorithm? I would suggest that the authors consider adding at least some simple baseline to their experimental results...,"This paper tackles a bandit problem that incorporates three challenges motivated by common issues encountered in online recommender systems: delayed reward, incentivized exploration, and self-reinforcing user preference. The authors propose an approach called UCB-Filtering-with-Delayed-Feedback (UCB-FDF) for this problem and provide a theoretical analysis showing that UCB-FDF achieves the optimal regret bounds. Their analysis also implies that logarithmic regret and incentive cost growth rates are achievable under this setting. These theoretical results are supported by empirical experiments, e. g. using Amazon review data. The main concern with this paper is that the considered challenges have all been tackled already in different bandit settings, so the novelty here is that they are being tackled altogether. It would be more convincing if experiments included baselines from these existing settings to motivate the need for a new strategy rather than simply relying on methods that have been proposed previously to address each of these problems independently; the experiments currently contain only a baseline for bandits with self-reinforcing user preference, which has been added during the rebuttal phase.",0.8220506906509399,0.4314502691850066
The approach is elegant and extends the augmentation search space The policy search phase (the exploration pass) is elegant and extends the augmentation search space. The paper is well-written and easy to follow........,"Reviewers agreed that this work is well-motivated and presents a novel approach for data augmentation around the adaptive augmentation policies. There were some concerns around the lack of ablation studies and unclear performance improvements, which were addressed well by the authors’ responses. Thus, I recommend an acceptance.",0.864727258682251,0.33166151493787766
the performance is boosted by the transformer. Cons: The authors have addressed related work. Cons: The authors have addressed related work. Strengths: The authors have addressed related work. Strengths: The authors have properly addressed related work. Strengths: The authors have properly addressed related work. Strengths: The authors have properly addressed related work. Strengths: The authors have properly addressed related work. Strengths: The authors have properly addressed related work. Strengths,"The paper received two accept and two marginally accept recommendations. All reviewers find value in the proposed supervised semantic segmentation methodology (making self-supervised representation learning towards dense prediction tasks like segmentation or clustering without explicit manual supervision) and appreciate the experimental gains, but had (mostly practical) criticism that was reasonably well addressed in the rebuttal.",0.8001987338066101,0.3032366469502449
g. This work builds up a formal explanation framework for retrieval/metric learning. the empirical contribution is their proposed fast-kernel algorithm. the experimental contribution is their proposed fast-kernel algorithm. the authors perform experiments on two object detection benchmarks.,"The initial reviews for this paper were 6,6,6, the authors have provided a rebuttal and after the rebuttal the recommendation stayed the same. The reviewers have reached the consensus that the paper is borderline but they have all recommended keeping it above the acceptance threshold. Following the recommendation of the reviewers, the meta reviewer recommends acceptance.",0.8292015194892883,0.20194988312820594
"paper presents interesting ideas on how to utilize the original topological structures among different domains. very detailed theoretical analysis is provided which makes the result be persuasive. but the authors don’t show the strict analysis to this basic assumption. some intuitive analysis would not be enough. if the proposed method can replace the original adversarial works, this paper could be rated a high score for acceptance.","This paper proposes to leverage topological structure between domains, expressed as a graph, towards solving the domain adaptation problem. Reviewer n4Lk thought the ideas were interesting, appreciated the theoretical analysis and indicated that the experiments were “well thought out”. The reviewer asked for more detail on Lemma 4.1 and suggested that a proof be provided for Proposition 4.1. They asked for more justification on why the change of task for the discriminator from classification to generation would improve performance. The authors responded to these comments, clarifying the proof of Lemma 4.1 in the appendix. They clarified that proposition 4.1 can be derived from Corollary 4.3 or Corollary 4.4. On the point of classical vs. enhanced discriminator the authors provided additional experiments. Reviewer rNQp commented that the method was easy to follow and noted the theoretical and empirical analysis. They expressed some concern that previous work on graph-based domain adaptation was inadequately addressed. Like reviewer n4Lk they seemed unconvinced that the proposed graph discriminator was an improvement over past SOTA and questioned its novelty. In terms of claims about novelty and competitiveness relative to previous works I would have liked to see the reviewer make specific references rather than criticize in general terms. The authors’ responded to the reviewer, adding a recent entropy-based method (SENTRY) to the experiments and showed that their method outperformed this ICCV 2021 work by a large margin. They responded to the reviewer’s remarks about the original discriminator and variants, pointing out that this was already established in the paper. They used the other reviews to dispute the claim of lack of novelty. Reviewer uDYW felt that the work was novel and interesting. Like rNQp they thought the paper was clear. They questioned the practical advantage over baselines. The authors responded to the reviewer’s question about using a data graph. They responded to the question about parameter tuning and computational cost. They addressed the question about limited improvements in real-world datasets. I had some difficulty motivating the reviewers to engage in the discussion and acknowledge the authors’ response. The authors also politely attempted to nudge the reviewers to consider their updated results. In my opinion, the author responses have addressed most of the reviewer concerns and I don’t see any critical issues remaining. Therefore I think that this paper should be accepted as a poster.",0.8247480392456055,0.4068570700287819
"e., clustering, finding LTH, and pruning) is well motivated. weaknesses The empirical evaluations are not self-contained, somewhat weak, and incomplete: Table 1 is not referred to. a common theme across the Related works and other sections is the highlighting of computational cost involved in complex pruning heuristics.","This paper studies structured pruning methods, called kernel-pruning in the paper which is also known as channel pruning for convolutional kernels. A simple method is proposed that primarily consists of three stages: (i) clusters the filters in a convolution layer into predefined number of groups, (ii) prune the unimportant kernels from each group, and (iii) permute remaining kernels to form a grouped convolution operation and then fine-tune the network. Although the novelty of the method is not high, it is simple and effective in experiments after the supplementary sota results in the long rebuttal. Majority of reviewers increase their ratings after the rebuttal (though one reviewer promised this but forgot to act), while some reviewers have concerns on the fairness to other authors by adding lots of new results in unlimited rebuttal and refuse to check more. In terms of the top end of performance, a reviewer thinks that ""the authors haven't quite exceeded the results from existing works (""Discrimination-aware channel pruning for deep neural network"" and ""Learning-compression” algorithms for neural net pruning"" for CIFAR-10 and many others on ImageNet)"". In all, this work indeed lies on the boundary. After a discussion with other committee members, we recommend the acceptation of this work, if the authors could incorporate all the new results in rebuttal and get the reproducible codes released in the final version.",0.8158019781112671,0.33459260917845224
a number of open questions are open about the proposed method. the method is well written and easy to follow. it's unclear how general the proposal is. the method is not explained very well. a 'transfer to new goals' experiment is not shown to improve the performance.,"This paper proposes a new bilinear decomposition for universal value functions. The bilinear network has one component dependent on state and goal and another component that depends on state and action. The experiments with the DDPG algorithm in robot simulations show that the proposed architecture improves performance data efficiency and task transfer over several baseline algorithms, including improvements on earlier bilinear decompositions. The reviews noted several aspects of the paper could be improved, and the author response addressed several of these concerns. Multiple reviewers appreciated the insights from the experiment added in section 4.5 on a simple grid environment, which enabled a direct interpretation of the vector fields used in the method. Several aspects of the presentation were clarified based on the reviewers comments. Additional details were also provided on the problem specification and the solution methods. During the discussion, the reviewers agreed that the revised paper presented a useful addition to the literature. Four knowledgeable reviewers indicate to accept the paper for its contribution of an effective network architecture for a goal-conditioned universal value function approximator. The paper is therefore accepted.",0.839202880859375,0.31572862938046453
"Cons:: I think that there could be a sub-categorization of the training methodology ""Dataset"" into ""optimization objectives"" and ""datasets"" for a more controlled study. I think that each point provides evidence for the authors' conclusion, that no single training methodology should dominate the field.",This well written and well motivated paper has been independently reviewed by four expert reviewers. They all voted for the acceptance with three straight accepts and one marginal. The feedback provided to authors was constructive and the authors responded comprehensively. I recommend acceptance of this work for ICLR.,0.8289929032325745,0.15471232496201992
"inverse multiquadric KT. a KT-based analytic kernel. Secondly, the paper was written in a clear manner. I have some concerns regarding the significance of the results....","The focus of the paper is kernel thinning, i. e. the extraction of a core set from a sample with good integration properties meant in MMD (maximum mean discrepancy, hence worst case) sense. Particularly, the authors propose generalizations of the kernel thinning method (Dwivedi and Mackey, 2021) which relax the assumptions imposed on the kernel (k) and the target distribution (P), and possess tighter performance guarantees. Designing compressed representation of samples for integration is a fundamental problem in machine learning and statistics with a large number of successful applications. As assessed by the reviewers, the authors deliver important new theoretical insights in the area which can be also of clear practical interest. They also pointed out that the self-containedness of the paper could be improved and additional intuition would help the dissemination of the results among the members of the ICLR/ML audience.",0.8332581520080566,0.32302984808172497
the paper the motivation for all V&L experiments looks weak the motivation for all V&L experiments looks weak the paper... weaknesses:.. It is not clear whether CLIP works with grid-like features extracted from convolutional neural networks.,"Reviewers are in agreement that this work is a useful, clear, documentary piece of work that shows the utility of CLIP on a number of popular V+L tasks. There is a somewhat persistent concern that simply demonstrating that a stronger visual encoder leads to improvements downstream is not an insightful result on which the community can build.",0.842915952205658,0.3298830290635427
-I feel that the formulation of Theorem 1.1 is somewhat unclear. -I feel that the citations need to be improved. -I feel that the theorems do not really support the claim that only large step size leads to balancing.,"The paper studies gradient descent for matrix factorization with a learning rate that is large relative to the a certain notion of the scale of the problem. In particular, they show that the use of large learning rates leads to balancing between the two factors in the factorization. The discussion between the authors and the reviewers was fruitful in dispelling some of the reviewers' doubts and at the same time improving the paper. The paper seems to make some contribution on a relevant problem for the ICLR community. However, even in the restricted settings they consider, the problem does not appear to be completely solved. That said, I agree with the majority of the reviewers that the step forward seems enough to warrant the acceptance. I would still encourage the authors to take into account the reviewers' comments in preparing the camera-ready version. In particular, in the internal discussion it was suggested that the presentation of the paper could be improved by clearly stating the limitations of the current approach (e. g., the assumption of convergence in Theorem 5.1, a better discussion on large vs small learning rates w. r. t. the balancing effect).",0.8491638898849487,0.3367039794102311
I think Sequential Output is a factor. Sequential Output is a factor. the paper is well-written and nicely structured............,"This paper explores why adversarial examples do not transfer well in adversarial examples on automatic speech recognition systems. The authors propose a number of potential causes that are then quickly evaluated in turn. This could be an excellent paper, but in its current form, it is borderline. The main problem with the paper is that it proposes a number of causes for the limited transferability, and then evaluates each of them with one quick experiment and just a paragraph of text. In particular, none of the results actually convince me that the claim is definitely correct, and many of the experimental setups are confusing or would have other explanations other than the one variable that is aiming to be controlled for. That said, even with these weaknesses, this paper raises interesting and new questions with an approach I have not seen previosuly. So while I don't believe the paper has done much to actually demystify transferability, it does take steps towards performing scientific experiments to understand why it is so hard. And these experiments, while not perfect, can serve as the basis for future work to extend and understand which factors are most important.",0.8316843509674072,0.33425804817428195
"this paper proposes a distributed GCN training on large graphs named PipeGCN. the idea seemed simple and straightforward, and experiments show that the algorithm can achieve up to 2.2x speedup. the setup of the largest dataset is not practical. the paper provides novel theoretical proof of the convergence of GCN.","The paper proposes PipeGCN, a system that uses pipeline parallelism to accelerate distributed training of large-scale graph convolutional neural networks. Like some pipeline-parallel methods (but unlike others), PipeGCN involves asynchrony in the sense that its features and feature-gradients can be stale. The paper provides theoretical guarantees on the convergence of PipeGCN in the presence of this staleness, which is a nice contribution in itself. In discussion, the reviewers found the work to be well-executed and sound. All reviewers recommended acceptance, and I concur with this consensus.",0.8523246049880981,0.5014327824115753
NPR is computationally tractable and has nearly optimal regret upper bound. a similar regret term (see http://arxiv).. the regret guarantee is limited.........  NPR is computationally tractable and has nearly optimal regret upper bound.. . (). ( ().....,"This paper studies the design and analysis of contextual bandits algorithms, combining the ideas of neural network models (Zhou et al, 2020 and Zhang et al, 2020) and reward perturbations (Kveton et al, 2019, 2020); this has the computational advantage of avoiding inverting large covariance matrices, as is done in the other neural contextual bandits algorithms. Although the reviewers think that the papers need to do a better job in highlighting differences and extra challenges in the current work compared to prior works, they also acknowledge that this paper is the first that combines the above two ideas. The reviewers also acknowledge that the additional experiments in the rebuttal period help clear the concern the reviewers have about why all regret curves look linear. However they also pointed out, that comparison with the FALCON+ algorithm (Foster et al, 2020) may be slightly unfair, as the algorithm retrains the neural network after every new iteration. Overall, the reviewers think that the pros outweight the cons, and they lean towards acceptance.",0.7913107872009277,0.21541777114783014
". The proposed defense effect is quite intuitive. However, the perturbation constraint is a bit hard for computation. the reviewer is confused with the implementation and the efficiency. Weakness (1) Can the authors give a detailed description of how to implement I-BAU in practice and its time complexity?","This paper investigates defense against backdoor attacks for models that have already been trained. It proposes, in particular, a min-max formulation for backdoor defense, in which the inner maximum seeks a powerful trigger that leads to a high loss, while the outer minimum seeks to suppress the ""adversarial loss"", so as to unlearn the injected backdoor behaviors. To solve the minimax, the authors also propose a method, Implicit Backdoor Adversarial Unlearning (I-BAU). In addition, the authors also provide theoretical analysis including the convergence bound and generalization bound. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method. The proposed method is interesting and the implementation is nice. Overall, there is a fundamental flaw in the formulation: if the trigger is not additive (where there are many such examples of poisoning attacks that are not additive) this approach should fail completely. Not having experiments that discuss such triggers that are not additive is a significant flaw in the presentation of the paper. Another flaw is that the trigger is assumed to be small norm. Unlike adversarial examples attacks (at test time), there is no reason for backdoor triggers to be of small norm. Given that the defense critically relies on these two flawed assumptions, and the extent of how the proposed algorithm is sensitive to these assumptions are not properly addressed in the experiments, this paper is on the border line.",0.8316084742546082,0.35209309546784917
"I'm not sure.? li,l is not the auxiliary objective.... The abstract is reversed first name and last name........","This paper concerns ensemble methods in deep reinforcement learning, examining several such methods, and proposes to address an important issue wherein ensemble members converge on a representation of approximately the same function, either by their parameters converging to an identical point or equivalent points that give rise to the same function. The authors propose a set of regularization methods aimed at improving diversity, and benchmark these augmentations on five ensemble methods and a dozen environments. 3 of 4 reviewers generally praised the method's simplicity and generality, and found the experiments convincing. Reviewer a9sA describes it as ""clearly written and easy to follow"", although others found clarity lacking in parts. There was agreement among these 3 reviewers that this was an interesting problem to tackle. Reviewer TfGq notes that this method lacks theoretical justification or guarantees, but that as a largely empirical paper this is perhaps of secondary importance. Reviewers 6miY and a9sA had questions about the precise choice of metrics, hyperparameters and seeds; the resulting discussion cleared up many of these concerns. The most critical reviewer, i4M1, disputes the existence of the phenomenon at all, saying that ""Neural networks converge to different solutions given the initialization is different and multiple local minima."" The remainder of i4M1's criticisms seem centered on the choice of environments and the number of seeds (also raised by other reviewers). The issue of seeds has been addressed partially and the authors have committed to strengthening their results in this regard. Reviewer i4M1's statement on the convergence of neural networks to different minima matches a bit of dated folk wisdom about neural networks, but the AC disputes this. The authors have cited a study from before the DL era properly began that identifies this issue and Section 5 addresses these criticisms directly. In practice, modern neural networks, especially with non-saturating activations, tend to be surprisingly consistent across random seeds when trained against the same data stream, and more recent work posits that the loss landscape is less riddled with local minima than with saddle points (see e. g. Dauphin et al, 2014). _Equivalent_ minima are of course common due to scaling and permutation symmetries but SGD has a well documented preference for low norm solutions in the former case, and the authors' have chosen methods that would at least conceivably overcome these issues, by focusing on summary statistics of the representations rather than their precise values (and indeed, CKA is designed with these concerns in mind). Despite i4M1's incredulity I am inclined to agree with the majority of reviewers and view the paper as a worthwhile contribution to the body of knowledge (purely empirical though it may be) on both NN ensemble methods and DRL ensembles in particular. The introduction of measures from economics is clever and original, and the results are promising. A more exhaustive study on the entire Atari57 benchmark but can appreciate the resource problem this poses, and find that the suite of considered environments, combined with the augmentation of 5 different DRL ensemble methods, strikes a good balance. I concur on the issue of seeds and would encourage authors to include as many as possible for the camera ready, but on balance would recommend acceptance.",0.7741061449050903,0.20462026466664515
the authors claim that RWPE is better. The paper discusses the important topic of how to better utilize the structure information in message-passing-based graph neural networks. ICML (2021).. The proposal is simple & straightforward. The proposed framework is very general. The paper is simple.,"This work adds the positional encoding (akin to those in transformers, but adapted) to GNNs. In their reviews, reviewers raised a number of concerns about this work, in particular, lack of novelty, lack of ablations to demonstrate the claims of the paper, lack of comparison to previous work (e. g., position-aware GNNS, Graphormer and GraphiT which would appear very related to this work), lack of motivation (e. g., the introduced positional loss do not actually improve performance), and whether the experimental results were really significant. During the rebuttal, the authors replied to the reviews, to address. the concerns that they could. Of the reviewers, unfortunately only one reviewer elected to respond to the authors. It is disappointing that the four other reviewers did not respond and overall the reviewers did not discuss this paper further. The authors chose to highlight privately to the AC that two reviewers who scored the paper unfavourably did not respond. The authors then argued this should be taken into account in the score (presumably to make acceptance more likely)--however, two favourable reviewers also did not respond (not highlighted by the authors). I understand this kind of private request to the AC to dismiss unfavourable reviews (especially if they do not respond) is becoming common--I find it unhelpful--I can see who and who has not responded. Nonetheless, looking at the responses to the original concerns of the reviewers highlighted above, I believe the authors have adequately addressed the concerns of the reviewers. Therefore i recommend acceptance but only as a poster.",0.8189035058021545,0.28583556776627517
DIP-TL performs strangely poorly compared to PG-DLR and ZS-SSL-TL? r. Table 1 suggests that DIP-TL performs strangely poorly compared to PG-DLR and ZS-SSL-TL. r. Table 1 suggests that the low score has nothing to do with the pretrained network used for DIP-TL? r. Table 1 suggests that DIP-TL performs strangely poorly compared to PG-DLR and ZS,"The paper considers the problem of accelerated magnetic resonance imaging where the goal is to reconstruct an image from undersampled measurements. The paper proposes a zero-shot self-supervised learning approach for accelerated deep learning based magnetic resonance imaging. The approach partitions the measurements from a single scan into two disjoint sets, one set is used for self-supervised learning, and one set is used to perform validation, specifically to select a regularization parameter. The set that is used for self-supervised learning is then again split into two different sets, and a network is trained to predict the frequencies from one set based on the frequencies in the other set. This enables accelerated MRI without any training data. The paper evaluates on the FastMRI dataset, a standard dataset for deep learning based MRI research, and the paper compares to a trained baseline and an un-trained baseline (DIP). The paper finds their self-supervised method to perform very well compared to both and shows images that indicate excellent performance. It would have been even better to compare the method on the test set of the FastMRI competition to have a proper benchmark comparison. Here is how the discussion went: - Reviewer pt6r is supportive of acceptance, but notes a few potential irregularities, such as the method pre-trained on brain and tested on knees performing better than the method pre-trained on knees and tested on knees, and not providing a comparison of the computational cost. The authors added a table to the appendix revealing that the computational costs are very high, much higher than for DIP even. The reviewer was content with the response and raised the score. - Reviewer mBMk argues that the contribution is too incremental compared to prior work, in particular relative to the results of [Yaman et al., 2020], and also argues that the idea of partitioning the measurements is not new. The authors argue in response that their approach of partitioning the measurements is new, and the reviewer was inclined to raise the score slightly, but still thinks that the novelty on the technical ML side remains limited, and doesn't want to back the submission too much, and did not raise the score at the end in the system. - Reviewer 19v3 has the concern that the all elements used (transfer learning, plug-and-play, etc) are well known techniques and have been applied before to MRI, and therefore thinks that the paper does not clear the bar for acceptance. The paper points out that while those ideas might be applied for the first time to MRI, they have been used before in other image reconstruction problems, in particular denoising. I've read the paper in detail too, and am somewhat on the fence: I think it's very valuable to see that a clever application of self-supervised learning works so well for MRI. I agree with the reviewers that the technical novelty is relatively small, but on the other hand this is the first time that I see self-supervised learning being applied that successfully to MRI. I don't share the concern about novelty --- yes, the paper's approach builds on prior work, but it's not clear from the literature how well such a well tuned self-supervised learning approach would work. What I would have liked to see in addition to the experimental results is a proper evaluation on the FastMRI dataset: An advantage of the FastMRI dataset is that it provides a benchmark and if researchers evaluate on that benchmark (on the testset/validation set) we can compare different methods well. The paper under review doesn't do that, it only evaluates on 30 test slices, and thus it's hard to benchmark the method. Also, the paper would benefit from more ablation studies. In conclusion, I would be happy to discuss this paper at the conference, and think that other researchers in the intersection of deep learning and inverse problems would be too, and therefore recommend acceptance.",0.7958428859710693,0.30295146107673643
Adaptive Neyman-Pearson Lemma: the symbol S has already been used to denote the state space. I found several places that are quite confusing. the authors mentioned that the Adaptive Neyman-Pearson Lemma is a non-trivial result.,"The reviewers appreciated the treatment of the topic of certifiable robustness done in this work and although they had a number of concerns, I feel they were adequately addressed by the authors.",0.8415313959121704,0.23091958339015645
"the paper is aggressive, suggesting that we can stop doing contrastive learning and focus on meta-learning. Strengths: Mathematical formalization of the connection between contrastive learning and meta-learning. Strengths: Mathematical formalization of the connection between contrastive learning and meta-learning.","This paper was borderline, based on the reviews. The paper points out an interesting connection (somewhat known but not in this specific version) and good experimental results. However, numerous reviewers raised concerns that the paper was lacking a comparison to prior work connecting unsupervised learning and meta-learning, most notably, Hsu et al. (2019). After reading the revised version of the paper, the authors address this issue and also all the other reviewer comments. In relation to prior work they clarify that they focus on the contrastive unsupervised case and also do a good job in answering other reviewer concerns relative to novelty and results. I would also like to point out, as reviewers also did that the previous title was a bit aggressive and provocative. Gladly the authors agree to change it to a more scientific `The Close Relationship Between Contrastive Learning and Meta-Learning”. Overall I think the authors have done a good effort on addressing the reviewer concerns and I think the paper would be interesting for ICLR readers.",0.839394748210907,0.385321534342236
a paper on bias-variance decomposition is confusing. a paper on bias-variance decomposition is very relevant. a paper on bias-variance decomposition is very interesting. a paper on bias-variance decomposition is very interesting.,"The main contribution is a way of analyzing the generalization error of neural nets by breaking it down into bias and variance components, and using separate principles to analyze each of the two components. The submission first proves rigorous generalization bounds for overparameterized linear regression (motivated in a general sense by the NTK); there are settings where this improves upon existing bounds. It extends the case to a matrix recovery model, showing that it's not limited to the linear regime. Finally, experimental results show that the risk decomposition holds empirically for neural nets. The numerical scores would place this paper slightly below the cutoff. The reviewers feel that the paper is well written and have not identified anything that looks like a critical flaw. They have a variety of concerns, mostly centered around whether the results apply to practical situations. Specifically, they're worried about (1) the theory not applying directly to neural nets, (2) the high-noise setting being less relevant for modern deep learning, and (3) whether there's a realistic situation where it improves over past bounds. Regarding (1), the theory covers not only the linear regime, but also the nonlinear matrix recovery regime; combined with the empirical results, this seems pretty solid by the standards of a DL theory paper. Regarding (2), even though the most common benchmarks indeed have low label noise, the high-noise regime still seems worth understanding (after all, we'd like our nets to work in domains like medicine). I haven't dug deeply enough to properly evaluate (3), but the author response seems believable to me. Overall, the paper strikes me as creative and well-executed. Regardless of whether the theory is easily extendable to neural nets, this seems like an interesting paper that can be built on in future work. I recommend acceptance.",0.8139593601226807,0.3189474418759346
Cons: Very clear and descriptive figures and writing. Cons: Very clear and descriptive figures and writing. Strengths: Very clear and descriptive figures and writing. The proposed algorithm is significantly worse than the SSL based approaches such as DGL and MVGRL on Cora.,"The paper proposes a novel approach to graph representation learning. In particular, a graph auto-encoder is proposed that aims to better capture the topological structure by utilising a neighbourhood reconstruction and a degree reconstruction objective. An optimal-transport based objective is proposed for the neighbourhood reconstruction that optimises the 2-Wasserstein distance between the decoded distribution and an empirical estimate of the neighbourhood distribution. An extensive experimental analysis is performed, highlighting the benefits of the proposed approach on a range of synthetic datasets to capture structure information. The experimental results also highlight its robustness across 9 different real-world graph datasets (ranging from proximity-oriented to structure-oriented datasets). Strengths: - The problem studied is well motivated and the method proposed is well placed in the literature. - The method is intuitive and the way that the neighbourhood information is reconstructed appears novel. - The empirical comparisons are extensive. Weaknesses: - Some of the choices in matching neighborhoods seem a bit arbitrary and not sufficiently justified. - The scalability of the proposed method is questionable. The method has a high complexity of O(Nd^3) (where N is the number of nodes and d is the average node degree). The authors address this problem by resorting to the neighborhood sampling method (without citing the prior art), which is only very briefly discussed in the paper. - The reviewers have also expressed concerns about the fixed sample size q. The question of how the neighbour-sampling is handled when a node has less than q neighbours remains unanswered.",0.8181836605072021,0.2923763386373009
"the paper addresses an important problem in face verification: unfairness of classifiers with respect to demographic groups (different FPRs and FNRs) The main strengths of this paper are that the propsoed method is a unsupervised method, and does not require retraining.","All reviewers agree that the presented approach to fair calibration of face verification models is interesting and needed in the field. The method does not require access to sensitive attributes for calibrating, which makes it sustainable. The reviewers are satisfied with the presented experimental studies in most cases. The rebuttal addressed a large majority of additionally raised questions. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a poster.",0.8583894371986389,0.3842069447040558
"the paper provides some nice insights, especially treating the problem of cross-lingual transfer as largely a domain transfer problem. the paper can also add this mitigation of prior class shifts into the mix, which yields slight improvements. the paper is a limited set of tasks; having additional results on a wider range of tasks.","The paper presents a domain adaptation approach based on the importance weighting for unsupervised cross-lingual learning. The paper first analyzes factors that affect cross-lingual transfer and finds that the cross-lingual transfer performance is strongly correlated with feature representation alignments as well as the distributional shift in class priors between the source and the target. Then the paper designs an approach based on the observations. Pros: + The paper is well written and the proposed approach is well motivated. + The analysis about which factors affect cross-lingual transfer is interesting and provides some great insight. Cons: - As the reviewer pointed out, the experiments for verifying the proposed approach are relatively weak. Overall, the paper presents nice insights to connect cross-lingual transfer with domain adaptation. All reviewers lean to accept the paper and I also found the paper is in general interesting.",0.8510513305664062,0.5268891680364807
"the paper doesn't motivate the problem, and I am struggling to see any direct application. despite it does demonstrate the claimed contributions, to what extent the proposed method would work and generalize is unknown. multiple baselines are tested and the proposed model CPL shows significant gain over these.","This paper proposes a new dataset called ComPhy to evaluate the ability of models to infer physical properties of objects and to reason about their interactions given these physical properties. The paper also presents an oracle model (named oracle because it requires gold property labels at training time) that is modular and carefully hand designed, but shows considerable improvement over a series of baselines. The reviewers for this submission had several concerns including: (a) [VByS] ""concerns are about the complexity that the proposed method can handle""\ (b) [VByS] ""the method is only demonstrated on a simple synthetic dataset""\ (c) [8BUA] ""I am struggling to see any direct application""\ (d) [8BUA] ""choosing 4-videos as reference"" -- why use ref videos, why use 4\ (e) [8BUA] ""Baselines showing results with ground-truth object properties should be reported""\ (f) [3cQE] ""no innovation in the type or structure of questions asked""\ (g) [3cQE] ""neither the CPL framework nor the implementation of any module is novel""\ (h) [DJEq] ""The only difference is that this paper infers hidden properties instead of collisions""\ (i) [DJEq] ""The dataset is not comprehensive enough"" -- only 2 properties and simplistic and synthetic videos\ The authors have provided detailed responses to these concerns and I discuss these below. The authors have addressed (c),(d) and (e) well in their rebuttal. I don't think (a) is concerning. The proposed model is not expected to solve the dataset entirely inspite of having access to gold properties at training time. As the authors mention, this indicates the complexity of the task at hand. The authors also address (f) well. I dont think there is any need for innovation in the structure of questions asked. QA is merely a mechanism to probe the model, and using CLVERER style questions seems appropriate. I disagree with the sentiment behind (g). The proposed oracle model clearly inherits modules from past works and assembles them to suit the needs of the dataset. It is this assembly that differentiates it from past works. This is true of most papers in our field, including ones that are widely acknowledged to be important papers. The underlying modules in proposed networks are rarely novel, but their assembly can lead to improvements on benchmarks. Furthermore, the oracle model, isnt the central contribution of this work. The dataset is, and hence, the requirement for novelty is reduced. The oracle is meant to serve as a guideline to show what one may achieve given gold labels at training, and it serves that purpose well. Re (h), my takeaway is that inferring properties based on their dynamics and without any link to their appearance is an important step, and past datasets do not exhibit this characteristic. And thus, in spite of being a limited differentiation from CLEVERER, I think this is interesting. Re: (b) and (i) I do agree with some aspects of these, with the reviewers. I think its still valuable to have a dataset with synthetic videos, given that models today are unable to solve this dataset. Moving to more realistic videos is a next step. However, as the reviewer [DJEq] points out, it would be desirable to add more physical properties and add more complex scene elements like ramps. That would have added a lot more diversity to the dataset -- visually, with regards to physical properties and with regards to the types of reasoning required. Having said that, I believe that the dataset in its present form is still valuable to the community, and hence I recommend acceptance. I think adding more physical properties and scene elements will have made this a much stronger submission.",0.8172553181648254,0.32503711363231697
"the actual method can be improved to make it easier for the reader to understand different steps of the pipeline. the interaction of different components is not clear. r. r. The paper proposes an interesting direction of estimating ""instance-based"" noise transition matrix. r. The paper proposes a new direction of estimating ""instance-based"" noise transition matrix.","To tackle the problem of classification under input-dependent noise, the authors proposed the posterior transition matrix (PTM) to achieve statistically consistent classification. Specifically the information fusion approach was developed to fine-tune the noise transition matrix. Experiments demonstrated the effectiveness of the proposed approach. I would like to thank the authors for the detailed feedback to the initial reviews and also further feedback to the reviewers' additional questions. Many concerns were clarified by the feedback, and the additional experiments still demonstrate the effectiveness of the proposed method. The issue of data augmentation still remains, which should be at least experimentally investigated, but the contribution of the current manuscript is still valuable to be presented as ICLR2022.",0.8474719524383545,0.35172336655003683
"The paper is well-written and solid in terms of empirical studies. the paper is well-written and solid in terms of empirical studies. Cons: The paper is overall well-written and solid in terms of empirical studies. Strengths: The paper is very relevant to the NAS community. Cons: The paper addresses arguably one of the main challenges in current neural architecture search, the search space design.","This paper makes the important, albeit somewhat unsurprising, finding, that cell-based NAS search spaces, and in particular the DARTS search space, include some operations that are much better than others. Reducing the search space to these allows even random architectures to yield good performance, similarly to the findings of ""Designing Network Design Spaces"", https://openaccess. thecvf. com/content_CVPR_2020/html/Radosavovic_Designing_Network_Design_Spaces_CVPR_2020_paper. html This paper received mostly positive scores (5,6,6,8). While I agree with the negative reviewer that it would be good to study this on other benchmarks as well, I follow the positive reviewers in recommending acceptance. I encourage the authors to fix the remaining typos (there are still many) and to open source their code. This would increase the paper's impact a lot. Finally, I would like to ask the authors to avoid protraying the misconception that we don't need large and powerful search spaces. In fact, as already hinted on in Section 6, we *do* need larger and more exciting search spaces in order to discover entirely novel architectures. Also the multi-objective nature of NAS is not to be undervalued, so the take-away of the paper should *not* be that we should design NAS benchmarks with really small & strong search spaces, but that, given a specific problem and objective, it may be prudent to evaluate whether the whole power of a given NAS search space is needed or whether it can be reduced to its essential parts.",0.8153576850891113,0.4523979525674473
LReLu.. LReLu+DKS..................,"This paper seeks to find an answer to some quite interesting research question: can deep vanilla networks without skip connections or normalization layers be trained as fast and accurately as ResNets? In this regard, the authors extend Deep Kernel Shaping and show that a vanilla network with leaky RELU-family activations can match the performance of a deep residual network. Four reviewers unanimously suggested acceptance of the paper. There were concerns about the clarity or marginal performance improvement. However, they all including myself agree: achieving the competitive performance with the vanilla deep model itself can be seen as a big contribution and the clarity has been improved to some extent through revision.",0.7781803607940674,0.13106679380871356
"paper provides further insight into possible degenerate optima that can be obtained by VAEs, both linear and nonlinear, during training. without a central theorem or a clear theme that ties together the provided theorems, the current work feels somewhat disjointed.","The paper analyzes the behavior of VAEs in modeling data lying on a low dimensional manifold. It formally proves some of the conjectures/informal-statements in an earlier work by Dai and Wipf (2019) in the case of linear VAE and linear manifold, and disproves the same for the nonlinear case. In particular, it proves, by analyzing the objective and its gradient-flow dynamics, that VAE captures the intrinsic dimension of data distribution correctly. For nonlinear cases, the paper shows a counterexample to the conjecture in (Dai & Wipf; 2019) where the support of VAE generators is a superset of that of data distribution. Two of the reviewers had raised following specific concerns -- (i) Theory only considers linear encoders and linear/1-hidden layer nonlinear decoders, (ii) The convergence behavior during training is only provided for linear VAEs, (iii) Some statements in the introduction/abstract misrepresent the results in (Dai & Wipf; 2019). However the authors have adequately addressed (i) and (ii) in their response -- the paper shows that the correct manifold will only be recovered in the linear case; in the nonlinear case, even for simple manifolds (1-hidden layer) the correct manifold is not recovered as shown by the counterexamples. Authors have also promised to modify the statements in the abstract and introduction to address the concern in (iii). Other two reviewers are largely positive about the paper. The paper makes an important contribution to the VAE literature in further clarifying VAEs' behavior while modeling low dimensional manifolds, and will be a good addition to the conference program.",0.8217397332191467,0.41268959641456604
a paper introducing a new optimizer based on hand-wavy intuitions. strengths The paper is well-written and easy to follow. weaknesses Table 1 shows that models with different percentages of redundant parameters have similar performance. a paper introducing a new optimizer based on hand-wavy intuitions.,"The paper observes that the number of redundant parameters is a function of the training procedure and proposes a training strategy that encourages all parameters in the model to be trained sufficiently and become useful. The method adaptively adjusts the learning rate for each individual parameter according to its sensitivity (a proxy for the parameter's contribution to the model performance). The approach encourages the use of under fitted parameters while preventing overfitting in the well-fitted ones. Experimental results are presented covering a wide range of tasks and in combination with several optimisers, showing improvements in model generalization. The paper is very well written and easy to follow (as mentioned by Reviewers NSqH, 4pzE and sSHP). The authors provided a strong rebuttal including new experiments, like training using CNN based architectures (as requested by Reviewers sSHP and MzBV). Reviewer sSHP requested these results to be reported with STD, the AC encourages the authors to do so for the camera ready. Reviewer MzBV points out that the paper could be improved by giving a motivation of the update rule and proving convergence. However, still recommends accepting the paper due to the novelty in the idea of not taking redundant parameters as something inevitable and devising an effective strategy to improve it. This idea was also appreciated by the other reviewers. While the AC agrees that adding these points would improve the work, it takes as valid the point made by the authors. Namely, that the intuition behind the update rule is quite clear, and many other reasonable variants were ablated (in Appendix A.4.4). Furthermore, the empirical evidence shows that the method improves generalization. Reviewer NSqH points out that while SAGE improves the model’s generalization performance for lightly compressed models, its performance becomes more susceptible to pruning when the model is compressed heavier. While the authors responded with good points, the AC encourages them to follow the reviewer’s advice and incorporate further experiments studying this issue (e. g. other datasets). In sum, the paper proposes a simple and effective method that is able to improve generalization of large scale models. All four reviewers recommend accepting the paper. The AC agrees and encourages the authors to incorporate the requests mentioned above.",0.8239387273788452,0.44142991583794355
"the method has a novel technical contribution, that utlizes iterative SDE denoising for guided stroke sketch input + perturbed noise, with photorealistic output via the reverse of the SDE. The paper has compared with a few state-of-the-art baselines and demosntrates its effectiveness in producing more faithful editing results.","Thank you for your submission to ICLR. This paper presents a technique for image synthesis based on stochastic differential equations and a diffusion model. This looks to be a very nice idea with good results. After discussion, the reviewers converged and all agreed that the paper is ready for publication---the most negative reviewer raised their score after the author rebuttal, from a weak reject to weak accept. The rebuttal clearly and concisely addressed several concerns of the reviewers. I'm happy to recommend accepting the paper.",0.8367258310317993,0.33381043622891104
"The authors develop 3 kinds of spurious signal detection reliability measures: Known Spurious Signal Detection Measure (K-SSD), Cause-for-Concern Measure (CCM), False Alarm Measure (FAM). Weaknesses The authors develop 3 kinds of spurious signal detection reliability measures: Known Spurious Signal Detection Measure (K-SSD), Cause-for-Concern Measure (CCM), False Alarm Measure (FAM). The manuscript seems to be quite packed","This paper demonstrates that current post-hoc methods to explain black-box models are not robust to spurious signals based on three metrics especially when the spurious signals are implicit or unknown. Technical novelty is limited because the paper presents primarily empirical results instead of novel machine learning techniques. However, the problem is very important and timely, and significance to the field and potential impact of the presented results to advance the field are high as reviewers emphasized. There are ways to further improve the paper, including the clarity of presentation, although the authors improved in the revised manuscript. Overall, this paper deserves borderline acceptance.",0.8146514296531677,0.37186161875724794
"GM-NAS outperforms its Few-Shot and One-Shot counterparts in terms of the accuracy of derived architectures. the proposed partioning method further improves the performance of Few-Shot NAS. the introduced techniques are resonable, like gradients matching, cosine distance, and etc.",All reviewers give acceptance scores. One reviewer also commented that they would like to increase their score from 6 to 7 (which isn't possible in the system). I encourage the authors to add the substantial new results generated during the rebuttal into the paper.,0.8269597291946411,0.2249625821908315
the paper draws attention to the commonly used baseline method-random pruning and finds that it can be very effective for training a sparse network. it also finds that it can outperform dense counterparts in other favorable aspects. but this paper is not the first to examine the effectiveness of random pruning.,"### Summary This paper builds on previous work on sparse training that shows the many modern sparse training techniques do no better than a random pruning technique that selects layer wise rations, but otherwise randomly selects which weights within a layer to remove. The key difference in this work is to take these existing results and scale the size of the network to show that as the size of the network increases, the smaller -- as measured in pruning ratio -- a matching subnetwork becomes. ### Discussion #### Strengths Places an emphasis on simple techniques #### Weakness Significant overlap with previous work. Prior already demonstrated the equivalence of random pruning and contemporary pruning at initialization techniques. ### Recommendation I recommend Accept (poster). However, I do want to stress that there is a significant overlap with previous work. The paper does appropriately attribute observations to previous work. However, there is some risk that readers may misinterpret the title and claim results as a wholly new observation about random pruning, where the reality is instead much more nuanced. Given that the work points to new methodological directions on considering scaling the network as an additional parameter to consider in pruning observations, I do believe these results -- even if narrower in scope that can be interpreted -- provide value to the community.",0.8408365845680237,0.46557193828953636
"s3.2 is not very clear. Weaknesses: The proposed method builds on the top of the GLAT method, which limits its generality. the authors examined the effectiveness of non-autoregressive NMT on multilingual translation, and obtained encouraging supervised translation performance.","This paper proposes several innovations for machine translation. The reviewers had several questions about the claims that were made and the authors addressed these and also acknowledged that some of their formulations (e. g. 'better') would need to be qualified. Overall, there are several interesting ideas that have been put together in a sensible way, but the story is not super consistent. The detailed exchanges between the reviewers are authors are commendable!",0.8462956547737122,0.271478349963824
a. The authors need to make notations much simpler for better understanding to readers. b. The authors need to clarify the contributions of the proposed method. c. The authors need to make notations much simpler for better understanding to readers. d. The authors need to make notations much simpler for better understanding to readers.,"DictFormer is a method to reduce the redundancy in transformers so they can deployed on edge devices. In the method, a shared dictionary across layers and unshared coefficients are used in place of weight multiplications. The author proposed a l1 relaxation to train the non-differentiable objective to achieve both higher performance and lower parameter counts. All reviewers ended up giving the paper a score of 6 after increasing their scores during discussions. While the results are strong (better performance at much lower parameter counts), the paper is not clearly written. Several reviewers noted that the paper is difficult to understand and has a few unresolved points. For example, the method also ended up performing better than the base transformer model that DictFormer is supposed to compress. There seems to be a lack of understanding about what part of the model delivered the improvements. One reviewer said that this is potentially a great paper that deserves to be better explained. The basic concept of sharing a dictionary across layers should be simple enough to explain well and deserve a better explanation than eq 5. The authors promise to release the code, which would be necessary for a full dissemination of this work. I recommend accept.",0.8239169716835022,0.31034590086589253
6)The paper is well-organized and well-written. Weaknesses: W1. The authors only discuss the convergence results for the simple bilinear games problems. The only results are for GDA and biline cases. The authors should discuss the differences between these methods.,"This paper proposes to use Anderson Acceleration on min-max problems, provides some theoretical convergence rates and presents numerical results on toy bilinear problems and GANs. After the discussion, the reviewers agreed that this paper makes a nice contribution to ICLR. Some concerns were originally expressed in terms of incrementality of the theoretical results with respect to previous work (KCYs, gBHU), but the authors have well clarified their contributions in the discussion, and have updated their manuscript accordingly. There were also initial concerns about the related work coverage, but this was also properly addressed in the rebuttal, with additional experimental comparisons as well as extended related work section, as well as an additional convergence result for convex-nonconcave problems.",0.8510450720787048,0.3867026217281818
"if this is true, the results in this paper are less interesting since it would require exponentially number of neurons for the global convergence. if this is true, I feel the results in this paper are less interesting since it would require exponentially number of neurons for the global convergence.","This paper studies optimization of over-parametrized neural networks in the mean-field scaling. Specifically, when the input dimension in larger than the number of training samples, the paper shows that the training loss converges to 0 at a linear rate under gradient flow. It's possible to extend the result by random feature layers to handle the case when input dimension is low. Empirically the dynamics in this paper seems to achieve better generalization performance than the NTK counterpart, but no theoretical result is known. Overall this is a solid contribution to the hard problem of analyzing the training dynamics of mean-field regime. There was some debate between reviewers on what is the definition of ""feature learning"" and I recommend the authors to give an explicit definition of what they mean (and potentially use a different term).",0.8251899480819702,0.40787194172541297
a. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e. e,"*Summary:* Study gradient flow dynamics of empirical and population square risk in kernel learning. *Strengths:* - Empirical results studying several cases in MSE curves. - Explaining / solving certain phenomena in DL using kernels. *Weaknesses:* - More motivations would be appreciated. - Technical innovation not so high. *Discussion:* Ud7D found that the main strength of this paper is the take-home message rather than innovations. They concluded 7 might be appropriate for the evaluation. This opinion was seconded by WyHh who considered 7 the most appropriate rating. 5uQz also found that 7 would be the most appropriate rating. qXRH maintained concerns about the novelty of the work and rating 5. Nonetheless, they agreed the study is valuable and would not oppose acceptance. *Conclusion:* Three reviewers found this paper is definitely above the acceptance threshold (suggesting rating 7) and one more reviewer found it marginally below the acceptance threshold however not opposing acceptance. I found the general impressions from the discussion well described in a comment from Ud7D, who indicates that although this is not a breakthrough paper, it is a nice paper showing that a lot of DL phenomena are can be explained by Kernels. I conclude that the paper makes a sufficiently valuable contribution and hence I am recommending accept. I suggest the authors take the reviewers’ comments carefully into account when preparing the final version of the manuscript.",0.7820869088172913,0.10070799981292926
"the lower bound looks quite nice. Weaknesses: Although the lower bound looks quite nice, there appears to be a large gap to the lower bound. Weaknesses: Although the authors showed polynomial sample complexity for learning Markov games, there appears to be a large gap to the lower bound. Weaknesses: Although the authors showed polynomial sample complexity for learning Markov potential games, there appears to be a large gap to the lower bound. Weaknesses: Although","This paper proposes algorithms for learning (coarse) correlated equilibrium in multi-agent general-sum Markov games, with improved sample complexities that are polynomial in the maximum size of the action sets of different players. This is a very solid work along the line of multi-agent reinforcement learning and there is unanimous support to accept this paper. Thus, I recommend acceptance.",0.8298802375793457,0.26309646144509313
"a kernel regressor is equivalent to a kernel regressor. the first phase is aligning the kernel spectrum with the training labels. this constructs a data-dependent kernel, known as the NTK. the paper is the first to show the ""silent alignment"" effect on deep linear networks.","The authors make a case for a phenomenon of deep network training that they call the ""silent alignment effect"": that, while the training error is still large, the NTK associated with the network aligns its eigenvectors with key directions in ""feature space"". They support this with non-rigorous theoretical analysis of linear networks, and extensive experiments with real networks on real data. The consensus view was that this paper provides novel and useful insight into training dynamics, in particular regarding feature learning.",0.8719897866249084,0.4497724287211895
a std: a std: a std: a std: a std: a std: a std: a std: a std: a std: a std: a std: a std: a std: a std: a std:,"This manuscript makes an interesting observation: there is no reason why planning-based methods like MDPs must be limited to physical or grounded environments. One can plan about more abstract textual domains. It adapts the standard methods from planning to such text domains in a fairly straightforward way. The fact that concepts from MDPs map to these problems directly is an asset: ideas could flow between these domains in the long term. While the original submission was lacking clarity and significant technical details, the authors engaged with the reviewers and resolved lingering concerns. Reviewers are unanimous that this a strong contribution.",0.7443271279335022,0.12805367136994997
a. This paper tries to explain the success of magnitude based and gradient based methods in compressing neural network models. b. This paper tries to explain the success of magnitude based and gradient based methods in compressing neural network models. c. This paper tries to explain the success of magnitude based and gradient based methods in compressing neural network models.,"The paper used the Koopman operator theory to explain and guide the DNN pruning. All the reviewers deemed that such a viewpoint is novel (but at different levels). However, the paper still had some issues, including unclear technical details, vague/overselling statements, being computation and memory expensive, etc. The paper finally got 4 ""marginally above threshold"" (one being of low confidence), making it on the borderline. The AC read through the paper and agreed that the Koopman operator theory brings new perspective to DNN pruning, with potential for other analysis of DNNs. Although the paper is imperfect and not strong, it does not have severe problems either and the issues pointed out by the reviewers could be easily fixed (except the scalability issue, which can be left as future work). In order to encourage new ideas, the AC recommended acceptance.",0.8272615671157837,0.311958852800585
"the paper contributes to an underexamind area, theoretical analysis of expressivity of equivariant neural networks. The result is simple and easy to compute. the paper is reproducible (see attached code) Weaknesses: Presentation of the main idea is logical and useful.","The authors' provide a discussion of Cover's Theorem in the setting of equivariance. The reviewers consider the work well explained and interesting, especially after the revisions, and so I will vote to accept.",0.8509606719017029,0.34053322921196616
a remark about the variance between runs in Fig 5 looks small. a remark about the variance between runs in Fig 5 is not clear. a remark about the variance between runs in Fig 5 is not clear. a remark about the variance between runs in Fig 5 is not clear.,"This paper presents a tensor diagram view of the multi-headed self-attention (MHSA) mechanism used in Transformer architectures, and by modifying the tensor diagram, introduces a strict generalization of MHSA called the Tucker-head self attention (THSA) mechanism. While there is some concern regarding the incremental nature of the proposition, the identification of where to usefully add the additional parameter that converts from MHSA to THSA was nontrivial, and the experimental results on the performance benefits across multiple tasks is convincing.",0.8084983825683594,0.24903281405568123
Weaknesses: -- Lack of novelty. The paper is very interesting. The authors conduct experiments with different supervision levels and show that CL-InfoNCE can better bridge the gap with the supervised learned representations by using auxiliary information. WEAKNESSES: -- Lack of novelty. The paper is very interesting. The authors prove that CL-InfoNCE maximization learns to include the clustering information.,"The paper proposes a weakly supervised contrastive learning, using auxiliary cluster information, for representation learning. Their method generates similar representations for the intra-cluster samples and dissimilar representations for inter-cluster samples via a clustering InfoNCE objective. Their approach is evaluated thoroughly on three image classification task. The reviewers agree that the paper is well written, presenting interesting theoretical analysis (Reviewer h3zd, a8kw) and solid experimetal results (Reviewer RhYi, 1ziy). The core idea of the paper is relatively simple and well motivated (Reviewer h3zd). While the focus is using the clustering with auxiliary labels, the method can be applied without auxiliary labels with K-means. There were some concerns from the reviewers: the overlap with a concurrent work [1]. The authors have provided detailed discussions on conceptual (concurrent work focuses on unsupervised cases where this work focuses on weakly-supervised setting) and emprical comparisons. Accordingly, reviewer a8kw and 1ziy had some issues with the novelty of the paper, as it can be interpreted as slight modification from previously explored idea (vanilla InfoNCE loss). Despite some overlap with existing approaches, the paper presents an interesting and well conducted study of integrating clustering information for learning representation, so I vote for acceptance. [1] Weakly Supervised Contrastive Learning. ICCV 2021.",0.8301106095314026,0.4451661376903454
"Appendix A shows the runtime reduction using ImageNet or WMT. (At least, Appendix A shows the runtime reduction using the pruned model on real data). Third, the figures are somewhat cluttered and hard to understand. I. The authors choose to treat their problem in isolation, which means that the paper sometimes loses sight of why the method was proposed.","### Summary The key idea behind this approach is a new technique to map irregular sparsity to a regular, compressed pattern. The results can, in principle, therefore overcome several standard limitations with irregular data storage formats. The results improve over existing (though related) techniques. ### Discussion #### Strenghts - An interesting and timely topic to study - Results show non-compute improvements #### Weakness The primary weakness noted among the reviewers was the lack of study on actual decoding performance. As I note below, this is a serious oversight that given the already existing theoretical work in the area warrants study as the community should begin to turn towards mapping that theory to practice. ### Recommendation I recommend Accept (poster). This is a strong piece of theoretical work. However, I would like to note that while I believe this work meets the current evaluation standards set in the area, it is time for follow on work to take the additional step to validate the practicality of the approach through a performance evaluation (either in simulation or FPGA/ASIC work).",0.8230875730514526,0.2685614459216595
paper provides a principled and interesting approach to an impactful problem. it seems that the paper is trying to solve a problem that it is already solved at the beginning. the authors choose to compute O instead of a fully defined pi* to avoid sampling unnecessary states.,"In this paper, the authors introduce an exploration method for RL according to experimental design perspective via designing an acquisition function, which quantifies how much information a state-action pair would provide about the optimal solution to a Markov decision process, and the state-action that maximizes such acquisition function will be used for sampling for policy update. The empirical evidences show the proposed method is promising. Since most of the reviewers support the paper, I recommend acceptance of this submission. However, besides the questions raised by the reviewers, e. g., computation cost and planning quality from CEM, there is a major issue need to be clarified in the paper: >The algorithm designed for RL with generative model, which makes the state-action reset can be conducted (this is sometimes impossible in practice where the agent must start from initial state). This is different from the common RL setting, and thus reduce the complexity of RL. This should be emphasized in the paper. Meanwhile, for a fair comparison, this should be explicitly specified in experiment setting.",0.8287122845649719,0.3428977023189267
"the proposed BAM has a relatively high time complexity. the proposed BAM is not very efficient. The paper tackles an important problem of Bayesian online learning in a non-stationary environment. PMLR, 2017....","The article introduces a Bayesian approach for online learning in non-stationary environments. The approach, which bears similarities with weighted likelihood estimation methods, associate a binary weight to each past observation, indicating if this observation should be including or not to compute the posterior. The weights are estimated via maximum a posteriori. The paper is well written, the approach is novel and its usefulness demonstrated on a number of different experiments. The original submission missed some relevant references that have been added in the revision. The approach has some limitations, highlighted by the reviewers: * it requires to solve a binary optimisation problem whose complexity scales exponentially with the size of the dataset; although the greedy procedure proposed by the authors seems to work fine on the examples shown, the approach may not be applicable to larger datasets * it requires to store all the data * it requires the traceability of the marginal likelihood Despite these limitations, there was a general agreement that this paper offers a novel and useful contribution, and I recommend acceptance. As noted by reviewer o4TK, I also think that the title is not very accurate. Bayesian methods naturally allow recursive updates of one's beliefs, and therefore have ""memory"". Maybe change the title for ""Bayes with augmented selective/adaptive memory""?",0.8422083854675293,0.34178199660446906
'Reparameterized full-waveform inversion using deep neural networks' ''Reparameterized full-waveform inversion using deep neural networks''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''',"The paper presents an unsupervised method for learning Full-Waveform Inversion in geophysics, by combining a differentiable physics simulation with a CNN based inversion network. The reviewers agreed that the paper was well written and described an important advance but were concerned about limited novelty and a potential sim2real gap. The authors responded to their critique with significant new experiments and clarified the novelty of their method relative to prior work. Based on the author responses, I recommend acceptance.",0.7841103672981262,0.3758570831269026
- Authors rely on side information which they assume is a part of the training data. - Authors develop a kernel and use it as similarity function. - Authors rely on z_i (please see Step – 1 Problem Setup) and develop a kernel. - Authors rely on side information which they assume is a part of the training data. - Authors rely on side information which they assume is a part of the training data.,"The reviewers all acknowledge the importance of the paper as it addressed the challenge of the insufficient data problem in conditional contrastive learning, feeling that the idea was novel, the experiments verified the effectiveness of the model well, and the paper is well written. Reviewers also raised some good questions, such as the computational complexity, comparison with Fair_InfoNCE in the experiments, and kernel ablations. These questions are well addressed in the rebuttal and the revised version. One reviewer raised the issue of similarity to [1]. After taking a close look at this paper and [1], the AC felt that the motivation and focus of this paper are quite different from [1]. The authors should incorporate all the rebuttal info into the final version. [1] Jean-Francois Ton et al. 2021.",0.8196886777877808,0.2979751888662577
The feature masking module has too many losses making it difficult to reproduce. Cons The algorithm 1 is clean and easy to understand. Weakness The proposed method is simple and makes sense. The proposed method consists of three stages for training. The proposed method is simple and makes sense.,"Summary: Paper addresses the cross-domain few-shot learning scenario, where meta-learning data is unavailable, and approaches are evaluated directly on novel settings. Authors propose a 3-step approach: 1) self-supervised pretraining, 2) feature selection, 3) fine-tuning, and demonstrate gains over state-of-art. Pros: - Approach is novel for this setting - Paper is clear and easy to understand - Performance beats several prior methods - Experiments are thorough - Fundamental problem is worthwhile of investigation Cons: - Some concerns among multiple reviewers on how hyperparameters are selected. Authors have provided more information and tables in the paper. - Training process is multi-step and not unified. Authors provided additional information about unified training results, which yielded poorer results, likely due to overfitting from training many parameters at once. Overall recommendation based on the consensus of reviewers and AC expertise: accept.",0.8424416780471802,0.3681131043604442
False positives are not found in the GNN model. authors frame the entire work as a novel generalization of Granger causality. ii There is no enrichment analysis of TF binding. a. The authors frame the entire work as a novel generalization of Granger causality.,"The AC and reviewers all agree that the paper proposes a very interesting framework to extend Granger Causality to DAG structured dynamical systems with important applications. The submission was the object of extensive discussion, and the AC and reviewers all agree that the author feedback satisfactorily addresses the vast majority of their concerns. We strongly urge the authors to incorporate all the points and revisions mentioned in their feedback. We certainly hope that the author will pursue this line of work and consider scaling their approach to tackle larger applications such as those related to social networks.",0.8276286721229553,0.3297726972028613
equivariant message passing for the prediction of tensorial properties and molecular spectra. epub 2019 Apr 19. arXiv:2102.03150.. The proposed method seems to be simple and easy to implement. The The proposed method seems simple and easy to implement.,"All reviewers except one agreed that this paper should be accepted because of the strong author response during the rebuttal phase. Specifically the reviewers appreciated the new ablation study showing that improvements are not due to minor architectural changes, the new experiment on the number of time steps required for experiments, the agreement to change language around ""neural energy minimization"", the improvements to the related work, the novelty of the unrolled optimization approach, and the nice experimental results. Given this, I vote to accept. Authors: please carefully revise the manuscript based on the suggestions by the reviewers: they made many careful suggestions to improve the work and stressed that the paper should only be accepted once these changes are implemented. Once these are done the paper will be a nice addition to the conference!",0.8070407509803772,0.20056094080209733
The Proposition 1 provides more information on the power of the DropEdge method and motivates the derivation of C-DropEdge (1). ICML 2020 [2]. [3]. Strengths: The idea of using GNTK to understand the over-smoothing issue is interesting. The experiment results aligned with the theoretical claims and insights very well and showed that the messages in the paper are sound.,"In this paper, the authors established interesting theoretical results regarding the behavior Graph Neural Tangent Kernel (GNTK). They also provide sufficient evidence (some of which during rebuttal) that their approach is valid. We have had many discussions and I suggest that the authors apply reviewers' comments to the final version of their paper.",0.8514124155044556,0.2880985431373119
a worm missing? Could authors show some population trajectories? Could authors show some population trajectories? Could authors show some population trajectories? • How does warm prediction look like? Could authors show some population trajectories?,"The authors build an encoding model of whole-brain brain activity by integrating incomplete functional data with anatomical/connectomics data. This work is significant from a computational neuroscience perspective because it constitutes a proof of concept regarding how whole brain calcium imaging data can be used to constrain the missing parameters of a connectome-constrained, biophysically detailed model of the C. elegans nervous system. There were issues related to clarity in the initial submission which all appeared to have been addressed in the final revision. This paper received 3 accepts (including one marginal accept) and 1 reject. The paper was discussed and the reviewers (including the negative reviewer) were unanimous that the current submission should be accepted.",0.8037874698638916,0.15638947486877441
"the paper proposes a truly innovative technique, which seems to be effective and versatile enough to be applied on several tasks and different biomedical signals (EEG, EMG,...) The technique has been thoroughly evaluated and the results seem convincing. the technique has been thoroughly evaluated and the results seem convincing.","This paper introduces a tree-structured wavelet deep neural network to effectively extract more discriminative and expressive feature representations in time series signals. Based on a frequency spectrum energy analysis, the approach decomposes input signals into multiple subbands and builds a tree structure with data-driven wavelet transforms the bases of which are learned using invertible neural networks. In the end, the scattering subband features are fused using a self-attention-like mechanism. The effectiveness of the proposed approach is verified extensively on a variety of datasets from different domains including follow-up experiments in the rebuttal. Overall, the work is technically novel and provides an interesting way of extracting adaptive finer-grained features to deal with time series signals. The authors' rebuttal is solid which has cleared most of the concerns raised by the reviewers with additional supportive experimental evidence. I would recommend accept.",0.8367477059364319,0.42929469687598093
a: TRANS-ENCODER is the first completely unsupervised cross-encoder for sentence similarity. a: it is the first completely unsupervised cross-encoder for sentence similarity. b: it is still unclear why the iterative and coupled fine-tuning of bi- and cross-encoders results in performance improvement.,"For pairs of pieces of text, the central idea of this paper is to combine the approaches of using bi-encoders (where a vector is formed from each text then compared), which are easily trained in an unsupervised manner, with cross-encoders (where the two texts are related at the token level), which are normally trained in a supervised manner. The chief contribution of this paper is to train a combined model (as a ""trans-encoder"") by doing co-training of both model types in an alternating cyclic self-distillation framework. The paper suggests that this allows unsupervised training of a cross-encoder. This claim met some pushback from the reviewers, since the method does require good quality aligned text pairs (much like a traditional MT system does), and so the result is a task-specific sentence-pair modeling approach rather than a generic unsupervised learning approach. In the discussion, downsides included the claims of ""unsupervised"" being overstated, the genuine remaining need for related sentence pairs, the lack of a more theoretical understanding of why this works, and the feeling that the paper is not yet fully mature. Upsides include solid work building from existing models, big performance improvements over SimCSE, novelty in combining previous ideas in a new way for a new problem, and good experiments. To my mind, while the requirement of related sentence pairs does mean the model is task-specific and less than fully unsupervised, this is still a common and useful scenario, the performance of the model is strong, and, while the proposed model is built from existing components and ideas, they are combined in an interesting new way to achieve an intriguing and strong new way of training models, and the discussion here (and now in Appendix A.2) of what the authors had to do to get the model to work in terms of choosing different losses, etc., convincingly demonstrated that the authors had thought significantly and deeply about the nature of their proposal and how to get it to work well. Moreover the authors were able to work expeditiously during the reviewing period to address other weaknesses, such as now providing results with other methods than SimCSE (DeCLUTR and Contrastive Tension) and on other language models (RoBERTa). As such, although this paper is clearly somewhat borderline rather than an unambiguous accept, I find myself quite convinced by the novelty, thoroughness, and intriguing nature of this work, and so my vote is to accept it.",0.827606737613678,0.3956024174888929
"a new deformed sampling module that explicitly downscales the image into target resolution. the proposed method is end-to-end trainable, and can be plugged into different backbone networks. but when I read the experiment results, I find that it is not the module itself that brings the most difference.","Overall, this paper receives positive reviews. The reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. The authors' rebuttal helps address some issues. The area chair agrees with the reviewers and recommend it be accepted at this conference.",0.8323901891708374,0.2219676108409961
"Cons: The paper is very well-written, I could clearly follow the ideas and the experimental setting (modulo a few minor points mentioned at the end of this review). The motivation of this work is not fully clear to me. There is growing interest in (neural) cellular automata, and this work is valuable addition to the topic.","Meta Review for Variational Neural Cellular Automata This paper proposes a generative model, a VAE whose decoder is implemented via neural cellular automata (NCA). The authors show that this model performs well for reconstruction, but they also show that the architecture has some robustness properties against damage during generation. Experiments were conducted on 3 datasets: MNIST, Noto Emoji, and CelebA, and while experimental results were great on MNIST, the method was less performant so on the other two datasets, although there is clear evidence that the model can learn to generate meaningful images. For the robustness experiments, the authors show that VNCA is robust to perturbations (occlusions) and show that the model has a reasonable degree of robustness even without ever seeing any perturbations at training time. All authors agree that this model is an improvement over neural cellular automata, and that the approach is interesting and the results are sound (and even useful). Initially, there were concerns that NCA's were simply convolutional neural networks (the connection is already known, and not the point of the paper), and issues with comparison with baselines for damage reconstruction tasks, but these were addressed by the authors (which the reviewers have acknowledged, and have improved their scores). The authors have also responded to the concerns of reviewer cp9d, and due to the lack of response from cp9d, I assessed the authors' response myself and find that they do address the concerns (in particular, they removed claims of super-resolution, and improved the clarity of the work). With that in mind, the score of 5 is viewed as a score of 6 from my perspective (giving this work effectively an average score of 6). After my assessment of the paper and reviews, I agree with reviewer kwgv, as they have summarized the work in their original review: - The authors propose a variational neural cellular automaton, which learns to generate images by iterating the transition rule. - The paper is interesting, with good results, and a good fit for ICLR. - The paper solves an interesting problem on the topic of neural cellular automata. - There are some doubts/limitations that I have asked the authors to address (mainly concerning the architecture of the model). - There are some missing references and details that would help the readers to get a better sense of the subject. Crucially, kwgv have acknowledged that the *authors have improved the paper significantly after the reviews, and they have addressed all questions and comments that [they] raised* (especially with regards to the last 2 points), and kwgv has subsequently championed the work with a score of 8. With the increased scores from kwgv and AnwX in mind, and also with what I view as an increased score of 6 from cp9d (in the lack of response from the reviewer, the authors have addressed the concerns in my judgement), my conclusion is that this is a nice work that bridges NCAs with generative models, and I think the work will be a useful addition to the growing literature in this space. I will recommend it for acceptance at ICLR 2022 as a poster.",0.8261507749557495,0.3801714824512601
a novel goal-conditioned RL method is presented in this paper. the method is well presented and easy to understand. the results are also well analyzed and HindRL shows strong performance. weaknesses include the lack of diversity in the empircal evaluations.,"This paper proposes a method to improve the sample efficiency of the HER algorithm by sampling goals from a distribution that is learned from human demonstrations. Empirical results on a simulated robotic insertion task show that the proposed method enjoys a better sample efficiency compared to HER. The reviewers find the paper well-written overall and the proposed idea reasonable. However, there are concerns regarding the limited novelty of the proposed method, which seems incremental. Also, the empirical evaluation suffers from a lack of diversity. The considered tasks are virtually all equivalent to an insertion task. The paper would benefit from further empirical evaluations that include tasks such as those considered in the original HER paper.",0.8526293039321899,0.394818012203489
Truest: Loss function that minimizes correlation and increases sparsity provides a unique way to do feature selection for multi-modal cases. Truest: Loss function that minimizes correlation and increases sparsity provides a unique way to do feature selection for multi-modal cases.,"Canonical correlation analysis is a method for studying associations between two sets of variables. However these methods lose their effectiveness when the number of variables is larger than the number of samples. This paper proposes a method, based on stochastic gating, for solving a ℓ0 -CCA problem where the goal is to learn correlated representations based on sparse subsets of variables. Essentially, this paper combines ideas from Yamada et al. and Suo et al. who introduced Gaussian-based relaxations of Bernoulli random variables, and sparse CCA respectively. They also extend their methods to work with nonlinear functions by integrating deep neural networks into the ℓ0 -CCA model. They gave experimental results on various synthetic and real examples, including to feature selection on biological data. The author response addressed a number of the reviewers' concerns, including by providing additional experiments and analyzing the genes selected by their model on the METABRIC dataset. Overall this is a solid contribution both from a theoretical and experimental standpoint.",0.8213298916816711,0.2522630229126662
"a. The proposed algorithm is quite novel and interesting. However, in experiments, it seems that in most cases, using LBGM will degrade the performance a lot. a. The proposed algorithm is quite novel and interesting. a. The proposed algorithm is quite novel and interesting.","The paper shows that most variance of gradients used in FL and distributed learning in general is in very low rank subspaces, an observation also made in Konecny et al 2016 and some other related works in deep learning, though sometimes for a different purpose. The paper then proposes lightweight updates combining a fresh gradient with old updates. Experiments and a theoretical convergence guarantee complement the results, which are mostly convincing. The experiments compare against ATOMO but strangely not against the more common PowerSGD, which would also work with partial client participation. Overall, reviewers all agreed that the paper is interesting, well-motivated and deserves acceptance. We hope the authors will incorporate the open points as mentioned by the reviewers.",0.8412346839904785,0.2648247323841566
Weakness: The authors provide some valuable arguments about the performance of GCN on heterophilous graphs and verify the claims with some empirical results. Weakness: The authors provide some valuable arguments about the performance of GCN on heterophilous graphs and verify the claims with some empirical results. Weakness: The authors provide some valuable arguments about the performance of GCN on heterophilous graphs and verify the claims with some empirical results. Weakness: The authors provide some valuable arguments about the performance of GCN on heterophil,"Heterophily is known to degrade the performance of graph neural networks. This paper explores whether, for graph convolutional networks (GCNs), this is a general phenomenon, or if there are some circumstances under which a GCN can still perform well in a heterophilous setting. This paper characterizes one such setting under a contextual stochastic block model (CSBM) distribution with two classes (generalized in the appendix to multiple classes). The main takeaway is that there are indeed scenarios where a GCN can be expected to perform well, even under heterophilic neighborhoods. There are limitations, and the reviewers have been fairly thorough in pointing these out: the analysis is specific to GCNs under CSBM, and there are a number of assumptions on the node label/feature/neighborhood distributions. The non-linear operations in the GCN have also been dropped. Even still, the reviewers were generally satisfied that the experiments backed up the claims in this specific scenario. There is still quite a bit more to do in order to make this a more general result. Essentially, this paper shows that heterophily is not always a problem. One reviewer has stated that it is not always considered a problem anyway, but at least this paper outlines a specific scenario in which this is theoretically true. However, there is still a large space of “bad” heterophily, and this paper leaves open what these are, and how to deal with them. It is also possible that there are other “good” scenarios as well that are unexplored. Still, in the narrow scope under which the analysis lies, the paper is clear and accomplishes what it sets out to do. I would encourage the authors to ensure that the paper incorporates the suggestions of the reviewers, particularly with regard to scope, to ensure that the paper is properly grounded in its claims. All reviewers leaned towards the side of acceptance, except one who did not engage in post-review discussion. After reading over their review, and the subsequent response, I am satisfied that their concerns have been adequately addressed.",0.8111604452133179,0.4129838370718062
"The paper proposes a new explainable framework that decomposes the information into target and background portion. The main concern of this paper is the decomposition assumption. It would be better if authors can compare with them. Strengths: S1. In contrast of previously used approximation-based, perturbation-based, additive feature attribution methods, the authors claim that the newly proposed method has the advantages of higher fidelity and node-level explainability.","This paper proposes a decomposition-based explanation method for graph neural networks. The motivation of this paper is that existing works based on approximation and perturbation suffer from various drawbacks. To address the challenges of existing works, the authors directly decompose the influence of node groups in the forward pass. The decomposition rules are designed for GCN and GAT. Further, to efficiently select subgraph groups from all possible combinations, the authors propose a greedy approach to search for maximally influential node sets. Experiments on synthetic and real-world datasets verify the improvements over existing works. During their initial responses, reviewers suggested that the authors experiment with more baselines and also clarify some of the technical details. The authors revised their manuscript to address several of these comments. So, I am tentatively assigning an accept to this paper.",0.8579949736595154,0.3899241965678003
"the paper is well written, the motivation of the authors is clear, and the framework presented is easy to understand. I don't understand how the theory can help with DVS datasets, in which the ISI are highly variable. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g.","The authors provide a theory for training feed-forward spiking neural networks (SNNs) on input-to-output spike train mappings. They utilise for this heterogenous neurone and skip connections. The resulting method is tested on DVS Gesture, N-Caltech 101 and sequential MNIST. It achieved very good performance. The reviewers agreed that the results are interesting and significant. In the initial reviews, the reviewers pointed out some doubts about the theory and clarity of writing. These doubts and objections were addressed in the revision and the reviewers were quite satisfied with that. In conclusion, the manuscript presents interesting results for SNNs with a solid theory and very good experimental results. All reviewers vote for acceptance.",0.8364020586013794,0.22579560680809804
"The greedy step is not clearly explained and needs more discussion. The greedy step is not clearly explained and needs more discussion.. The algorithm only utilizes ""updates from the clients that participated in the previous round"". The algorithm only utilizes ""updates from the clients that participated in the previous round"". The algorithm only utilizes ""updates from the clients that participated in the previous round"". The algorithm only utilizes ""updates from the clients that participated in the previous round""","The paper proposes a novel method for (diverse) client selection at each round of a federated learning procedure with the aim of improving performance in terms of convergence, learning efficiency and fairness. The main idea is to introduce a facility location objective to quantify how representative/informative is the gradient information of a given set of clients is, and then choose a subset that maximizes this objective. Given the monotonicity and submodularity of the proposed facility location objective, the authors have been able to provide theoretical guarantees. Experimental results on two data sets (FEMNIST and CelebA) show the effectiveness to the proposed approach and algorithm. The reviewers had a number of concerns most of which were addressed in the authors response. The reviewers believe that the theoretical results of the paper are incremental given the prior work (see the reviews for more details); however, the reviewers (as well as myself) agree that the proposed method is novel and can provide significant practical advantage. Utilizing sub modular objectives for diverse selection is a well-known (and effective approach), but I am seeing it in the context of federated learning for the first time. My suggestion to the authors: (i) Improve the experimental section by adding a few more common data sets (such as CIFAR when data is distributed in a heterogeneous manner). CelebA and FEMNIST are not really the best data sets to try in FL (although they are commonly used). (ii) One of the reviewers had several critical comments about the theoretical results, please address those in the updated version. (iii) Please clarify in more detail how the theoretical and algorithmic contributions of there paper go beyond the recent work of (mirzasoleiman et el. 2020); (iv) iIt seems to me that the paper is missing some references on client selection in federated learning. Please revise the related work accordingly.",0.7985785603523254,0.2759350069249288
"Cons: 1. This paper identifies and tries to tackle an important problem in recommender systems - insufficient overlapping. 3. Generally speaking, the paper is well-written and well-presented. The motivation for the particular approach is not extremely clear. The proposed method tries to optimize recommendation algorithms via a minimax game.","This paper presented a domain transportation perspective on optimizing recommender systems. The basic motivation is to view recommendation as applying some form of intervention, implying a distributional shift after the recommendation/intervention. Distribution shift brings tremendous difficulty to traditional causal inference or missing data theory perspective of recommender systems as it violates the distributional overlapping assumption: in simple terms, if the model recommends radically different set of items, there isn't much you can say about its generalization ability; on the other hand, if the model only recommends items that it already observed during training (no distribution shift at all), it would inherent all the biases which already exist in the data. To that end, this paper proposed a domain transportation perspective by introducing a Wasserstein distance constrained risk minimization to find interventions that can best transport the patterns it learns from the observed domain to the post-intervention domain. The paper received overall borderline scores. All the reviewers acknowledged that the proposed perspective is novel and has the potential to spark a new direction for future work. The reviewers raised concerns, ranging from the bounds in the paper, sensitivity of the optimization w. r. t. the hyperparameter, to some relevant but missing baselines. The authors provided very detailed response and revised the paper quite substantially to address most of the feedback. I also read the paper myself given the borderline scores, and I think the authors did a reasonably good job improving the paper and I agree this paper provides an interesting and novel perspective on viewing recommendation, though I also agree with one reviewer that the idea of ""partially extrapolation"" can be further explored. My major complaint is around experimental evaluation. It seems to me that only the semi-synthetic experiment actually makes sense in this context (where the measure is based on the unobserved relevance as opposed to observed click), as the traditional random-split-on-clicks evaluation would inevitably favor models with little distributional shift (the training and test data essentially come from the same distribution, maybe not so with a sequential setting but still close). Furthermore, the inclusion of Yahoo R3 and Coat dataset is even more confusing, as the associated test set implies random exposure which is certainly not what this paper aims to address, unless I am missing something in which case more clarification would be nice. My overall assessment of the paper is still leaning towards positive but I also wouldn't be too upset if this paper doesn't end up making it. However, if accepted, I do want the authors to carefully revise the presentation of the experimental results for the final version.",0.8279992341995239,0.39954936197575397
"-I think that exchanging ideas between neuroscience and machine learning is a great way to develop novel generative models with new capabilities such as VPR. -The analytical experiments are well-thought-out and validate the role that the various components of the VPR play. -The authors may consider to compare with VTA only on the Moving Ball dataset, which in my view, is not enough to validate the superiority of the proposed model.","Thanks for your submission to ICLR. This paper considers a variational inference hierarchical model called Variational Predictive Routing. Prior to discussion, several reviewers were on the fence about the paper, most notably having concerns about some of the experimental results as well as various clarity issues throughout the paper. However, the authors did a really nice job addressing many of these concerns. Ultimately, several of the reviewers updated their scores, leading to a clear consensus view that this paper is ready for publication. We really appreciate your effort in providing additional details and results. Please do keep in mind the concerns of the reviewers when preparing a final version of the manuscript.",0.8412663340568542,0.29811716399022514
The results show that the SCRFD indeed enhances the detection performance on small faces. Strength The proposed methods are effective for detection of objects with wide range of scales. Weakness The proposed methods are rather straightforward and not interesting. Weakness The proposed methods are effective for detection of objects with wide range of scales.,"This paper received 4 quality reviews, with the final rating of 8 by 2 reviewers, and 6 by the other 2 reviewers. All reviews recognize the contributions of this work, especially its superior performance. The AC concurs with these contributions and recommends acceptance.",0.8249658346176147,0.2301224134862423
(1) Some important details are missing. (2) The motivation is clear and the proposed sound adversarial audio-visual navigation is interesting to me. Weaknesses: Some important details are missing. The motivation is clear and the proposed sound adversarial audio-visual navigation is interesting...,"This paper addresses audio-visual navigation tasks where a reinforcement learning agent perceives visual RGB and binaural audio inputs, rendered in a first-person perspective 3D environment, and is tasked to navigate to the audio source. The authors propose to make the RL navigation policy robust, by training the agent with additional adversarial audio perturbations. These perturbations consist of an adversarial ""ghost"" agent (attacker) that emits noise perturbations volume, position and category determined by policies that are trained to maximise the negative rewards for the navigation agent in zero-sum game. The agent is then evaluated on the simulated Replica and MatterPort3D environments and compared to a few baselines. The authors conduct a large number of ablation experiments. The three reviewers were globally positive about the paper, regarding the motivation, joint training of the agent and attacker, and experimental evaluation. Reviewer TLMn had questions about specific results and ablations of existing baselines, whereas reviewer i5Vv had questions about random noise ablations - the authors provided responses for these questions. Outstanding requests were about proofreading. After rebuttal and discussion, the scores for this paper are 6, 8 and a weak 8 (or 7), i. e., an average of 7, and thus I believe that the paper meets the conference acceptance bar.",0.813184916973114,0.2839948568493128
"g, Sec. The paper is clear. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec. g, Sec.","The paper studies how to build predictive models that are robust to nuisance-induced spurious correlations present in the data. It introduces nuisance-randomized distillation (NuRD), constructed by reweighting the observed data, to break the nuisance-label dependence and find the most informative representation to predict the label. Experiments on several datasets show that by using a classifier learned on this representation, NuRD is able to improve the classification performance by limiting the impact of nuisance variables. The main concerns were about the presentation and organization of the paper, which was heavily focused on the theoretical justifications but fell short in explaining the intuitions and implementation details. The revision and rebuttal have addressed some of these concerns and improved the overall exposition of the paper, based on which two reviewers raised their scores to 8. While there is still room to further improve the paper by providing more detailed discussions about the proposed algorithms, the AC considers the paper ready for publication under its current form.",0.786848247051239,0.2198585296507206
"True : a black-box setting is somehow contrived since it only poses challenges on computing the gradient (x,yt).?? I strongly suggest providing a concrete example of this scenario.....","This work proposed to detect backdoor in a black-box manner, where only the model output is accessible. Most reviewers think it is a valuable task, and this work provides a novel perspective of using adversarial perturbation to diagnosis the backdoor. Some theoretical analysis for linear models and kernel models are provided. There is still huge gap to analyze the DNN model. But on the other side, it provides some insight to understand the proposed method and could inspire further studies. Besides, since there have been many advanced backdoor attack methods, and many more are coming out, I am not sure that the proposed detection criteria is well generalizable, considering only some typical attack methods are tested. However, I think the studied problem is valuable, and the presented analysis is inspired for future works. Thus, I recommend for accept.",0.8265975117683411,0.3584013283252716
"is not a problem. a phenomenon that is not a problem. I think the paper tackles an important question, namely convergence of optimization methods under non-iid samples. Strengths, weaknesses, and comments.. I think","This paper studies online learning using SGD with momentum for nonstationary data. For the specific setting of linear regression with Gaussian noise and oscillatory covariate shift, a linear oscillator ODE is derived that describes the dynamics of the learned parameters. This then allows analysis of convergence/divergence of learning for different settings of the learning rate and momentum. The theoretical results are validated empirically, and are shown to generalize to other settings such as those with other optimizers (Adam) or other models (neural nets). The reviewers praise the clear writing and the rigorous and systematic analysis. 3 out of 4 reviewers recommend accepting the paper. The negative reviewer does not find the main contribution interesting and significant enough for acceptance. Although I think this is a reasonable objection, it is not shared by the other 3 reviewers. Since the negative reviewer does not point out any critical flaws in the paper, I think the positive opinions should outweight the negative one in this case. I therefore recommend accepting the paper.",0.8333534002304077,0.3323720425367356
"a binning method is trivially applied using the 1-vs-rest method. the results are also more mixed wrt these metrics. the paper tackles an interesting area, tackles an interesting area. it is a novel calibration, and demonstrates the utility in empirical experiments.","The paper studies the problem of multi-class calibration, proposing new notion of ""top-label calibration"", and presenting and comparing new algorithms for multi-class calibration. Reviewers generally found the paper to be well-written, and tackling a foundational problem. There were some questions regarding the experiments: (1) _Lack of explanation of why unnormalised beats normalised. _ One of the paper's main empirical findings is that using an unnormalised predictor with histogram binning (CW-HB) can significantly outperform a normalised one (N-HB). There is however limited discussion prior to this of why such behaviour is expected. (2) _Lack of comparison to isotonic regression. _ One can apply isotonic regression in conjunction with one of the M2B algorithms in Sec 3. It is of interest whether isotonic regression + a suitable M2B wrapper compares to HB + an M2B wrapper. (3) _Comparison to OVA calibration_. One reviewer raised the concern that the new algorithms proposed in this work are not too surprising; e. g., using a binary calibrator of one label vs everyone else appears natural. (4) _Overloaded appendices_. One reviewer pointed out that some material in the Appendix is not referenced in the body, thus making the work not self-contained. For point (1), the response indicates that the unnormalised model can obtain distribution-free guarantees. The lack of such a guarantee for the normalised model does not suggest that such a guarantee is impossible, however. Certainly the present work need not solve this issue in entirety, but if this is intended to be a main takeaway of the experiments, a little more discussion in the body seems advisable. On this note, from my reading, the experiments seek to demonstrate that suitable M2B reductions can dramatically improve the performance of HB. However, with the use of multiple evaluation metrics (Top-ECE, Top-MCE, Classwise-ECE) it is not clear if the authors intend to promote one specific M2B reduction as generally favourable; further, the text in Sec 3.2 suggests that N-HB is the method considered in prior works, which then seems to do better on top-label ECE than prior works. Which suggests that for this particular metric, one does not gain much from other M2B reductions? For point (2), the response argued that ""the main message of the paper is not about HB vs other binary calibrators, but proposing a single agnostic framework for achieving multiple notions of multiclass calibration using any binary calibrator"". From my reading of the paper, I think this claim is accurate, and agree that the conceptual advance is the generic M2B framework itself For point (3), the authors responded to suggest that while Algorithm 2 performs a natural one-versus-all calibration, this is different from Algorithm 1. Further, the latter is shown to be useful in Table 2 (bottom panel). For point (4), the revision involved referencing relevant material in the Appendix. These seem better, though I would prefer if theorem statements (e. g., Theorem 1) appear in full or as sketches in the body. Further to the above, I have a couple of minor suggestions: - consider removing most hline's from Tables 1 -- 3 - keep the ordering of Algorithms 1 -- 4 the same as that in which methods are presented in Table 1. **Summary**. The paper considers a foundational problem. It makes one simple yet interesting contribution in its definition of top-label calibration. Detailing the various multi-class calibrators in Section 3 is another contribution: albeit simple, it does illuminate the subtle issue of the role of normalisation in post-hoc calibration, which empirically is shown to have non-trivial impact. There are certainly avenues where the paper could be further strengthened, but overall I do see it as potentially being of broad interest to the community, and inspiring future work. My recommendation is thus for the authors to further incorporate the reviewers' detailed suggestions and the comments above, which can broaden the clarity, scope and impact of the work.",0.8156760931015015,0.3899804974996275
"aaron carroll: paper is good, but proof techniques are almost the same as previous work. he says paper is not a proof of identity covariance, but it is a theoretical paper. carroll: paper is written clearly with enough technical details to understand the arguments. carroll: paper is a good contribution to ICLR, but it is not a good one.","This paper extends recent and very active literature on analyzing learning algorithms in the simplified setting of Gaussian data and model weights, with the main generalization being to allow for non-isotropic covariance matrices. The main technical results seem to be correct and slightly novel, though reviewers feel they are not innovative or unexpected enough to stand on their own. However, the main contributions of the paper are then to interpret these results to give phenomenological results (regarding double descent, etc.), and reviewers were unanimously happy with these. In the end, all reviewers were positive about the paper. The largest reviewer criticisms of the paper were technical issues (ot31) and lack of context of recent literature (3RfG). Both of these concerns were mostly addressed by the revision/rebuttal. The reviewers still had specific comments on improvement, but found no major faults.",0.8389363288879395,0.35446878575852936
"the paper is well organized and written, with only some minor issues that I list below. Strong points: Well motivated. The authors present clearly the backfill problem and its effects on the predictions thus enabling the reader understand the importance of the suggested solution. Generalizability. Weak points: At parts the writing feels aesthetically dense.","This paper introduces Back2Future, a deep learning approach for refining predictions when backfill dynamics are present. All reviewers agree on that the authors successfully motivate their work and introduce a topic of great interest, i. e. that of dealing with the effect of revising previously recorded data and its effect timeseries predictions. The reviewers also underline the strong and thorough experimental section. Among the reviews is also underlined the potential impact of the work for the research domain. Many thanks to the authors for replying to the minor concerns raised. I concur with the reviews and find this submission very interesting, convincing and thus recommend for accept. Thank you for submitting the paper to ICLR.",0.8497753143310547,0.33236461298333275
"a lot of discussion is provided to parse the approximation results. a lot of the decompositions (1D or 2D discrete Fourier transforms) would gain at being worked out in more detail (e. The paper is very well written, with detailed and interesting discussions, which makes it a pleasant read.","The paper addresses hierarchical kernels and provides an analysis of their RKHS along with generalization bounds and cases where improved generalization can be obtained. The reviewers appreciated the analysis and its implications. There were multiple concerns regarding presentation clarity, which the authors should address in the camera ready version.",0.8522942662239075,0.3297011454900106
a metric for success rate and reward should be defined. a metric for cumulative reward should also be defined. a metric for success rate and reward should be defined. a metric for success rate and reward should be defined. a metric for success rate and reward should be defined.,"This paper is good but at a borderline. One reviewer increased the score during the discussions. However, no reviewer was in strong favor. So that this paper is still a borderline one, and it is up to the SAC to decide.",0.8349972367286682,0.14184056874364614
(-) This is not really a concern but the paper did not have an ethics statement. (-) This is not really a concern but the paper did not have an ethics statement. (-) This is not really a concern but the paper did not have an ethics statement. (-) The paper is pretty well-written (some minor typos at the end of this section) Most of my complaints are more from the more practical aspects of the results in this paper.,"Dear Authors, The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted: - The new result about query complexity of regression problem that the authors have added. Along with the result on for (noisy) Vandemonde matrix, these make the paper lie above the accept bar. - The authors have providing satisfying clarifications during the rebuttal that convinced reviewers to increase further their scores. The current consensus is that the paper deserves publication. Best AC",0.8259282112121582,0.4189347457140684
"Unlike other neural program synthesis methods, RobustFill, the method cannot handle any mistakes made in the input-output specification. Weaknesses The paper contains useful experiments that control for various aspects of the proposed method. The empirical results are quite strong and show clear improvements on prior work.","This paper addresses the problem of program synthesis given input/output examples and a domain-specific language using a bottom-up approach. The paper proposes the use of a neural architecture that exploits the search context (all the programs considered so far and their execution results) to decide which program to evaluate next. The model is trained on-policy using beam-aware training and the method is evaluated on string manipulation and inductive logic programming benchmarks. The results show that the proposed method outperforms previous work in terms of the number of programs evaluated and accuracy. Overall, the reviewers found the paper to be well-written and the idea proposed to be significantly novel and interesting to be presented at the conference and I agree. Several limitations were pointed out by the reviewers in terms of (i) actual run-time performance, (ii) the incompleteness of the search algorithm and the (iii) reproducibility of the approach. I believe the authors have addressed these points satisfactorily in their comments.",0.8471143841743469,0.45607468272958485
"a method to generate synthetic data with privacy preservation is proposed. the paper contains quite a lot of inconsistent/non-precise descriptions. the paper contains quite a lot of inconsistent/non-precise descriptions. it is a good paper, but the main weakness is the lack of thorough benchmarking.",The paper presents a new framework of synthesizing differential private data using deep generative models. Reviewers liked the significance of the problem. They raised some concerns which was appropriately addressed in the rebuttal. We hope the authors will take feedback into account and prepare a stronger camera ready version.,0.8529137372970581,0.34919220954179764
"References: Tian, Yonglong, Dilip Krishnan, and Phillip Isola. Springer International Publishing, 2020. (2018). Generalisation in humans and deep neural networks. Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm. IS THE Norm.","This paper explores addition of a version of divisive normalization to AlexNets and compares performance and other measures of these networks to those with more commmonly used normalization schemes (batch, group, and layer norm). Various tests are performed to explore the effect of their divisive normalization. Scores were initially mixed but after clarifications for design and experiment decisions, and experiments run in response to comments by the reviewers the paper improved significantly. While reviewers still had several suggestions for further improvements, after the authors' revisions reviewers were in favor of acceptance which I support.",0.7876063585281372,0.24745945406979636
"aaron carroll: paper is excellent, but there are three major methodological concerns. he says paper is lacking test ppl/bpc for origin model and student model. carroll: paper is a great example of how to improve the LM paradigm. carroll: paper is a great example of how to improve the LM paradigm.","This paper presents a very interesting study of using an artificial language (generated using a specific algorithm via a transformer model) and training SOTA transformer and LSTM language models on that language; the authors show that these LMs underestimate the probability of sequences from this language and overestimate the probability of ill formed sentences, among other observations. This is a very interesting study that captures the behavior of recent LMs. All reviewers are supportive of accepting this paper and it is good to see the engagement between reviewers and authors of this paper.",0.831876814365387,0.4035308100283146
the paper proposes a sampling method that aims at providing uniform samples from the latent space for deep generative models. it proves the proposed method for a mild assmption that DGN only comprises continuous piecewise affine (CPA) non-linearities.,"The paper proposes a simple method for uniform sampling from generative manifold using change of variables formula. The method works by first sampling a much larger number of samples (N) from uniform distribution in the latent space and then does sampling by replacement (using probability proportional to change in volume) to generate a smaller number of final samples (k << N) that are seen as approximately sampled from a uniform distribution from the generative manifold. Reviewers had some questions/concerns about the confusing language in the abstract and introduction around the use of the term ""uniform"" which the authors have addressed satisfactorily. Authors have also provided results on quality (FID metric) of the generated samples as asked by the reviewers. While the proposed method is rather simple, has high computational cost, and novelty is marginal (as noted by two of the reviewers), reviewers agree it is above the acceptance bar.",0.8402177095413208,0.5145880311727524
"a UCB-type algorithm is proposed for contextual bandits. the paper is well written, limitations are clearly stated. the paper is well written, limitations are clearly stated. the paper is well written, limitations are clearly stated. a UCB-type algorithm is not particularly novel.","This paper tackles the neural contextual bandit problem, for which existing approaches consists rely on bandit algorithms based on deep neural networks to learn reward functions. In these existing strategies, exploration takes place over the entire network parameter space, which can be inefficient for the large-size networks typically used in NTK-based approaches. In this work, the authors address this by building on an existing technique of shallow exploration, which consists in exploring over the final layer of the network only, allowing to decouple the deep neural network feature representation learning from most of the exploration of the network parameters. More specifically, they propose a simple and effective UCB-based strategy using this shallow exploration scheme, for which they provide a theoretical analysis. The proposed approach builds on several ideas for previous works, including borrowing proof techniques and theoretical arguments. Although this limits the novelty of the work, connecting these ideas together is not obvious and constitutes a significant contribution. Moreover, the proposed approach fixes an important known issue due to the matrix inversion in LinUCB, which could have a strong impact on the bandit community.",0.830636739730835,0.34293302851063867
"u_theta and l_xi are written as functions of x in Section 3.2. the proposed method makes the assumption that noise is homoscedastic, which is often not the case in deep learning applications. the method makes the assumption that noise is homoscedastic, which is often not the case in deep learning applications.","The paper proposes a novel method, PI3NN, for estimating prediction intervals (PIs) for quantifying the uncertainty of neural network predictions. The method is based on independently training three neural networks with different loss functions which are then combined via a linear combination where the coefficients for a given confidence level can be found by the root-finding algorithm. A specific initialization scheme allows to employ the method to OOD detection. Reviewers agreed on the importance of the problem of producing reliable confidence estimates. The proposed method addressed some of the limitations of the existing approaches, and reviewers valued that a theoretical as well as an empirical analysis is provided. On of the main criticisms was that the theoretical derivation of the method is based on the assumption of the noise being homoscedastic. This however is a common issue with other methods in this area, which are nevertheless all applied (and seem to work) on heteroscedastic data as well and are outperformed by the proposed method. Another main point that was criticized was that the empirical analysis was limited. In turn the authors added another experiments on another dataset and with another network architecture (a LSTM) to their analysis. Moreover, the authors adequately addressed a lot of the concerns and questions of the reviewers in their answers and the revised manuscript. The final mean scores are exactly borderline (5.5) but with a higher confidence of reviewers voting for acceptance. Based on the listed points, the paper should be accepted. I would encourage to improve the discussion around the dependence on x in Section 3.2, which could still be made clearer, in the final version of the manuscript, and to add the discussion about the limitations of the theoretical analyses (i. e. the applicability only to the homoscedastic settings) to the conclusion.",0.8316065073013306,0.41955767671267186
"Moreover, the authors should explain how the empirical loss in Eq. is expected. The authors conduct a complete and detailed theoretical analysis. the detailed derivative process should be listed in the appendix...................................","The authors provide a framework for unsupervised clarification based on minimizing a between-cluster discriminative similarity. It is more flexible than existing methods whose kernel similarity implicitly assumes uniform weights, and the authors connect to ideas such as max-margin and weighted kernel approaches. This yields a clustering algorithm naturally that alternates between updating class labels and similarity weights. Moreover the reviewers (and I) appreciate the analysis of generalization error through Rademacher complexity arguments and detailed author responses. I might add while the paper draws connections to weighted kernel methods and have since added references to sparse subspace clustering etc, there is recent interest in using similar arguments to derive error bounds and uniform concentration results for center-based methods that might be included in the survey of related work, for instance recent work from Swagatam Das and collaborators. The authors have importantly added details on the optimization using SMO, and the revision should include these details in a clear exposition together with the computational complexity discussion mentioned in their response.",0.8345872759819031,0.29946306751420104
(1) there are Nash equilibriums of this game where Alice and Bob are identical. (2) there are Nash equilibria of this game where Alice and Bob are identical. (3) there exist Nash equilibria of this game where Alice and Bob are identical. (4) the two goal generators are identical because they all have the same updates. (5) the method is not a good curriculum generation.,"While one reviewer remained concerned about the possibility of convergence to bad equilibria and felt that the proposed method appears to be four minor changes from prior work (PAIRED), the authors demonstrate empirically that the proposed changes make a significant difference in their evaluation. Other reviewers were positive about this work and all others rated this work as an accept. Post rebuttal the most positive reviewer increased their score to an 8 and felt did a good job answering their concerns. They wanted to see an analysis of systems with larger numbers of agents, but felt that the current manuscript was more than sufficient to warrant acceptance, and fell into the category of a good paper with the additional ablations provided during the rebuttal. The AC recommends accepting this paper.",0.8183485865592957,0.24588743597269058
"l. a number of methods have never been described before, nor are they explained. l. a number of traces are not certified, and the certification is not discussed. l. a number of traces are not certified, and the certification is not discussed.","The authors propose a framework for for the certification of reinforcement learning agents against adversarial observation/state perturbations based on randomized smoothing. They develop the theory of the framework, demonstrating that the framework can be used to certify lower bounds on the worst-case cumulative reward of an agent. They validate their theoretical bounds experimentally. The paper is well written and reviewers were mostly in agreement that the contributions are worthy of acceptance. The technical concerns from reviewer zGtv were addressed during the discussion phase, but I strongly encourage the authors to revise the manuscript to address the points raised in the discussion.",0.824081301689148,0.23829145580530167
"Weaknesses: The attributed network datasets concided is a bit limited. Strengths: Building the latent subgraph from embeddings is an interesting idea. The final features extracted are mainly specified by human. Though effective, it leaves an open question that whether thses sorts of features can be leared adaptively?","This paper proposes a new link prediction algorithm based on a pooling scheme called WalkPool. The main idea is to jointly encode node representations and graph topology information into node features and conduct the learning end-to-end. The paper shows the superiority of the method against the baselines. Strength * The paper is generally clearly written. * A new method is proposed, which is technically sound. * Many experiments are conducted to verify the effectiveness of the proposed method. Weakness * The novelty of the work might not be so significant. There is a similarity with the SEAL algorithm. The authors have addressed most of the problems pointed out by the reviewers. They have also conducted additional experiments.",0.8406952619552612,0.277565511316061
"the paper does not introduce the literature thoroughly, includes multiple typos, and has mistakes in the terminology. g., bounding the perturbation radius), most results follow from the specific set of assumptions that work for this case but cannot be easily extended. e., the upper bound on the error radius decays if we require a stronger result, not the other way around.","Verifying robustness of neural networks is an important application in machine learning. The submission takes on this challenge via the interval bound propagation (IBP) framework and provides a theoretical analysis on the training procedure. They establish, in the large network with case, that the certification via IBP reflects the robustness of the neural network. Despite the tensions between the changing architecture and the required accuracy, the results are insightful. The AC recommends the authors to revise the paper, correcting the significant amounts of typos and improve the presentation for its final version.",0.8300299644470215,0.3400608708461126
ICLR 2022 Questions: 1- Figure 1: the generator with more layers. 1- Figure 2: the generator with more layers. 3- Figure 3: the generator with more layers. 4- Figure 5: the generator with more layers. 4- Figure 5: the generator with more layers.,"This paper received six reviews, consisting of three 8s two 6s and one 3. The reviewers generally felt that the proposed Electra-like pretraining provided fairly significant downstream improvements. Additional ablations were provided to during the author response period and other author responses were sufficient to cause scores to rise during the discussion period. The vast majority of reviewers recommended accepting this paper and the AC also recommends acceptance.",0.8045985102653503,0.19800024293363094
"aaron carroll: paper is well structured and well-explained. not sure how reproducible this work is without code. he says it should be compared with other distributed training methods, including data and model parallelism. carroll: paper does not put graphs in context with other approaches for parallel training.","The reviewers were split about this paper: on one hand they appreciated the clarity and the experimental improvments in the paper, on the other they were concerned about the novelty of the work. After going through it and the discussion I have decided to vote to accept this paper for the following reasons: (a) the potential impact of the work, (b) the simplicity of the idea, and (c) promise of release of open source code. I think these things make the paper a strong contribution to ICLR. The only thing I would like to see added, apart from the suggestions detailed by the reviewers, is a small discussion on the carbon footprint of training such largescale graph networks. The authors motivated the work by saying it could have a beneficial impact for modelling energy which is needed to combat climate change. However, we know from recent results that such large scale models also have a non-trivial emission footprint. So I'd like to see the authors specifically calculate the carbon footprints of the models they trained. There are tools to help with this such as: https://mlco2. github. io/impact/ With this addition I think this paper will not only make a large impact on graph network training but also start a discussion of how to responsibly decide training, taking environmental impact into account.",0.823398768901825,0.29255409725010395
"a recursive expression (43) is also wrong. (no effect on results I guess) 3. 4. In the reinforcement learning literature, numerous algorithms exist to learn the value function. chief among these algorithms are (linear) temporal difference learning and the residual gradient.","This paper presents a study of the over parametrization of linear representations in the context of recursive value estimation. The reviewers could not reach a consensus over the quality of the paper, with a fairly wide range of scores even after the rebuttal. After considering the paper, the rebuttal, and the discussion, I lean towards accepting the paper. Despite the concerns voiced by some of the reviewers, the topic and analysis of the manuscript are novel and interesting, and it is my expectation that this manuscript will prove a valuable source of inspiration for future work. I invite the authors to carefully consider the feedback received by all the reviewers (and in particular Reviewers xq3y and gT5o and) and to revisit the manuscript accordingly.",0.8262961506843567,0.27861225605010986
"ICLR reader with no background in theoretical computer science should read this paper. paper suggests three possible improvements on NTK analysis: Faster rates, with 1/sqrt(n) dependence on the number of examples instead of 1/sqrt(n) as in e. g. A better complexity measure of the data/target, that allows the rate to be adaptive to some data-dependent complexity.","This paper goes beyond the NTK setting in analyzing optimization and generalization in ReLU networks. It nicely generalizes NTK by showing that generalization depends on a family of kernels rather than the single NTK. The reviewers appreciated the results. One thing that is missing is a clear separation between NTK results and the ones proposed here. Although it is ok to defer this to future work, a discussion of this point in the paper would be helpful.",0.8386309742927551,0.36761439573019744
a) The paper is relatively well written with justifications and reasons for the choices. b) The paper presents a simple idea to use reinforcement learning: imitation learning in particular to better maintain the content between the two styles. c) The paper is relatively well written with justifications and reasons for the choices.,"The paper proposes a new method for unsupervised text style transfer by assuming there exist some pseudo-parallal sentences pairs in the data. The method thus first mines and constructs a synthetic parallel corpus with certain similarity metrics, and then trains the model via imitation learning. Reviewers have found the method is sound and the empiricial results are decent. The assumption on pseudo-parallal pairs would limited the application of the methods in other settings where the source/target text distributions are very different. The authors have added discussion on this limitation during rebuttal.",0.8299950361251831,0.3688918620347977
a) It seems that the transformer-based architecture is essential in order to achieve such good results due to the self-attention mechanism. 2021. 2021.. The paper is well written and has almost no typos...,"This paper regards video understanding as an image classification task, and reports promising performance against state of the arts on several standard benchmarks. Though the method is quite simple, it achieves good results. The visualization in this paper also provides good insight. All reviewers give positive recommendations for this paper.",0.8552074432373047,0.2770428629592061
ICSGLD is not apparent. ICSGLD is not apparent.?........ The choice of the piecewise continuous approximation is not apparent... the choice of the function H is not apparent.. .? ?? ?....,"This paper proposes a new variant of a stochastic gradient Langevin dynamics sampler that relies on two key ideas: approximation of the target density with a simpler function (as in [Deng, 2020]) and the parallel simulation of many chains. The authors also prove that their approach can be theoretically more efficient than a single-chain algorithm. The reviewers see the contribution as significant although they did raise some concerns regarding the clarity of the paper. Since these concerns do not appear to be major, I recommend acceptance but I advise the authors to address the comments of the reviewers to maximize the impact of the paper.",0.8153092861175537,0.218866154178977
"The authors explain the results clearly. Minor comments The results are interesting. NeuPL is a good RL algorithm. NeuPL is interesting, and it also converges to an N-step best response. It could enable skill transfer and speed up the learning................",The authors propose a new framework of population learning that optimizes a single conditional model to learn and represent multiple diverse policies in real-world games. All reviewers agree the ideas are interesting and the empirical results are strong. The meta reviewer agrees and recommends acceptance.,0.8659954071044922,0.3220325201749802
The main strengths of the paper include: (1) Can use more experiments to show a more convincing picture. (2) Can use more experiments to show a more convincing picture. i. The main weaknesses of the paper include: (1) Can use more experiments to show a more convincing picture.,"The paper was seen positively by all reviewers. The strength of the paper are: - Intuitive and interesting combination of Koopman Operators and Optimal Control for Reinforcement Learning - Convincing experiments on challenging benchmark tasks - All of the issues of the reviewers (advantages to SAC, gaps in the theory and missing references) have been properly addressed in the rebuttal. I therefore recommend acceptance of the paper.",0.8388644456863403,0.3894394878298044
a tropical rational mapping is a mathematically solid analysis of neural networks. the paper presents a mathematically solid analysis of neural networks. Strengths include a bound for the approximation error of the pruned network. a comparison between the presented algorithm and the one from Smyrnis & Maragos would be beneficial.,"The submission introduces an algorithm for structured pruning of fully connected ReLU layers using ideas from tropical geometry. The paper begins with a very accessible overview of key concepts from tropical geometry, and shows how ReLU networks can be thought of as tropical polynomials. It gives an efficient K-means-based algorithm for pruning units in a way that approximately minimizes the Hausdorff distance between certain polytopes. Experiments show that the method outperforms other methods based on tropical geometry and is competitive with SOTA methods from a few years ago. I think the reviewers, authors and I all agree on the following points: tropical geometry is a mathematical topic not commonly used in our field and for which it is difficult to find expert reviewers (notice that most of the citations aren't from ML venues). The paper is well-written, and the authors have taken pains to present the required concepts in an accessible way. Nobody has raised any concerns about correctness. While this isn't the first pruning method that uses tropical geometry, the algorithm is novel and interesting. It's expensive, but not unreasonably so. The experiments are a proof-of-concept: they use small networks by today's standards, and the baselines aren't the current SOTA. The average scores are slightly below the usual cutoff. The reviewers are concerned about whether this method is useful, given that is based on different principles from current methods and can't quite compete with current SOTA. But my own sense is that this is a paper that we'd like to have at ICLR. It gives a clear, accessible introduction to tropical geometry and demonstrates its usefulness for practical deep learning. It demonstrates competitiveness with fairly strong baselines, which is all we should expect from methods that haven't benefited from years of hill-climbing on the same handful of ideas. I recommend acceptance.",0.8144891262054443,0.3762507683131844
g. It's slightly awkward that different losses are used for different datasets. cRT re-trains classifiers on 'balanced' datasets. LM-softmax loss is not by softplus but by normalization of features and classifier weights.,"This work presents a principled objective function for large margin learning. Specifically, it introduces class margin and sample margin, both of which it aims to promote. It also derives a generalized margin softmax loss which to draw general conclusions on the existing margin-based losses. The effectiveness of the proposed theory is empirically verified in visual classification, imbalanced classification, person re-identification, and face verification. The reviewers initially raised some concerns, but most of them were well addressed in the rebuttal and convinced the reviewers. Specifically, pU1u was satisfied by authors' reply on Theorem 3.2 and the practical methods. pGzf appreciated clarifications around the evaluation metric used on IJB-C and believes this work can improve our understanding of margin-based face recognition. Finally, 3YiD had some reservations about number of parameters which got clarified by the authors. In sum, all post rebuttal ratings fall in the accept zone, and the reviewers find the paper interesting and insightful. In concordance with them, I recommend this paper for publication. Please make sure to include suggestions made by reviewers in the camera ready version.",0.8273851275444031,0.303581506352533
attacj framework is a new approach to attacj to improve the robustness of different ViT varinsts. weaknesses This paper doesn't consider the robust training for ViT to improve the robustness. e. Patch-Fool does not have an epsilon factor like traditional robust training seems to have.,"This paper provides an interesting study on the adversarial robustness comparisons between ViTs and CNNs, and successfully challenges the previous belief that ViTs are always more robust than CNNs on defending against adversarial attacks. Specifically, as revealed in this paper, when the attacker considers the attention mechanisms, the resulting patch attack can hurt ViTs more. Overall, all the reviewers enjoy reading this paper and appreciate the comprehensive robustness comparisons between ViTs and CNNs. The reviewers were concerned about the missing experiments about adversarial training, vague statements about the inspiration for future defenses, visualization of adversarial examples, etc. All these concerns are well addressed during the discussion period, and all reviewers reach a consensus on accepting this paper. The final version should include the experiments, visualizations, and clarifications provided in the rebuttal. In addition, please release the code as promised.",0.8385943174362183,0.36294830510658876
g. Minor concerns that do not affect my score g. Minor concerns that do not affect my score g. Minor concerns that do not affect my score. g. Minor concerns that do not affect my score. g. Minor concerns. g.,"Thanks for your submission to ICLR! This paper presents a novel way to combine domain adaptation with semi-supervised learning. The reviewers were, on the whole, quite happy with the paper. On the positive side, the results are very extensive and impressive, it's a clever way to combine domain adaptation and semi-supervised learning, and it's a fairly general approach in that it works in several settings (e. g., unsupervised vs semi-supervised domain adaptation). On the negative side, the approach itself is somewhat limited technically. After discussion, the one somewhat negative reviewer agreed that the paper has sufficient merit and should be accepted; thus, everyone was ultimately in agreement. I also read this paper carefully and personally find it very interesting and promising, so I am happy to recommend acceptance. It seems to give state of the art performance in several cases, and could possibly lead to more research down the road on methods to combine adaptation techniques with SSL.",0.8073070049285889,0.14050348498858511
"p3: ""we derive linear upper and lower bounds of the form zi >= Az(i-1) + c for each..."" p4: ""the encoding of the verification problem using Lagrange multipliers looks sound"" p4: ""the encoding of the verification problem using Lagrange multipliers looks sound"" p4: ""the encoding of the verification problem using Lagrange multipliers looks sound""","The authors improve upon existing algorithms for complete neural network verification by combining recent advances in bounding algorithms (better bounding algorithms under branching constraints and relaxations involving multiple neurons) and developing novel branching heuristics. They show the efficacy of their method on a number of rigorous experiments, outperforming SOTA solvers for neural network verification on several benchmark datasets. All reviewers agree that the paper makes valuable contributions and minor concerns were addressed adequately during the rebuttal phase. Hence I recommend that the paper be accepted.",0.8151429891586304,0.3142539942637086
"False summing: GGPD is a good sampler without DDSS. summing: GGPD is a good sampler, but DDSS is not a good one. summing: DDSS is a good sampler, but GGDP isn't a good one.","The paper tackles a very interesting problem in the context of diffusion-based generative models and provides empirical improvements. Pre-rebuttal, reviewers' main concerns lie in the motivation and clarification of the method, while after rebuttal, all reviewers satisfied the response and gave positive scores. The authors should include the additional results to well address the reviewers' concerns in the final version.",0.8325778245925903,0.2368388151129087
"I also appreciated the small extensions like regularization and attention. I also appreciated the small extensions like regularization and attention. Overall, I enjoyed the paper.........................................","The paper proposes two new generalized additive models (GAM) based on neural networks and referred to as NODE-GAM and NODE-GA2M. An empirical analysis shows that the proposed and carefully designed architectures perform comparably to several baselines on medium-sized datasets while outperforming them on larger datasets. Moreover, it is shown that the differentiability of the proposed models allows them to benefit from self-supervised learning. Reviewers agreed on the technical significance and novelty of the proposed models and valued the clever design of the new architectures. Most concerns and open questions could be answered in the rebuttal and by changes in the revised manuscript. Based one the suggestions of one reviewer new experiments comparing the proposed models to NAM were added, which improved the paper further. The paper should be accepted.",0.8221073746681213,0.37249430269002914
I am curious about the implication of these claims. is much more compelling than the previous ones.... I am in favor of acceptance..........,"This work studies the approximation and estimation errors of using neural networks (NNs) to fit functions on infinite-dimensional inputs that admit smoothness constraints. By considering a certain notion of anisotropic smoothness, the authors show that convolutional neural networks avoid the curse of dimensionality. Reviewers all agreed that this is a strong submission, tackling a core question in the mathematics of DL, namely developing functional spaces that are compatible with efficient learning in high-dimensional structured data. The AC thus recommends acceptance.",0.8260672688484192,0.26890882911781466
"a minor issue is the structure of this paper. the paper contributes a lot to the research area of 1-Lipschitz CNNs. the authors present the three methods in the order: HH, LLN, CR. r.. a minor issue is the structure of this paper.","The paper provides a procedure for certifying L2 robustness in image classification. The paper shows that the technique indeed works in practice by demonstrating it's accuracy on CIFAR-10 and CIFAR-100 datasets. The reviewers are positive about the paper. Please do incorporate feedback, especially around experimental setup to ensure that the work compares various methods fairly and provides a clear picture to the reader.",0.837920606136322,0.3363087147474289
"The key finding of this manuscript is ""[...] linearity linearity of neural networks.... the paper is well written...........","The authors provide in this manuscript a theoretical analysis to explain why deep neural networks become linear in the neighbourhood of the initial optimisation point as their width tends to infinity. They approach this question by viewing the network as a multi-level assembly model. All reviewers agree that this is an interesting, novel, and relevant study. The paper is very well-written. Initially, a weak point raised by a reviewer was that an empirical evaluation of the theory was missing. The authors addressed this issue in a satisfactory manner in their response. In conclusion, this is a strong contribution worth publication.",0.8550207614898682,0.6216399360980307
"Pros ""our method needs no fixed-size memory"" - This might make sense to compare with existing approaches on computation, training time, etc. Cons ""our method needs no fixed-size memory"" - This might make sense since the model still needs to store the network from previous tasks (or the teacher network).","This paper presents a zero-shot incremental learning approach that does not store past samples for experience replay. The idea is novel and well motivated, and the paper is well written. Reviewers' comments were mainly about missing baselines, missing ablation studies, and clarifications about the proposed method. In the revised paper, the authors provided more justifications and added new experimental results on large benchmark datasets as well as ablation studies. After discussion, all the reviewers are positive about this submission. Thus, I recommend to accept this paper. I encourage the authors to take the review feedback into account in the final version.",0.8309844732284546,0.263475026935339
I think it is better to say ‘predict mentions when not knowing the corresponding entities’. Can you elaborate? I think it is better to say ‘predict mentions when not knowing the corresponding entities’. Can you elaborate?? The authors used dense retriever in the first phase of their algorithm (retrieving top K entities) and then trained reader mouse for ranking and extracting entity mentions from the output of the retriever.,"This paper casts entity linking in a retrieve-then-read framework by first retrieving entity candidates and then finding their mentions via reading comprehension. All reviewers agree that the proposed approach is novel, well-motivated, and simple yet performant. The authors have done a good job of addressing all the concerns raised, and the reviewers are unanimous in their recommendation for accepting the paper. I hope the authors will also incorporate the feedback and their responses in the final version.",0.837917149066925,0.30319254721204436
"The . The paper is not a comprehensive work, but rather a generalization of previous work.. The paper is confusing and hard to follow. The paper is confusing and hard to follow.. .. The   ........ g.....","The paper considers the saddle point problem of finding non-convex/non-concave minimax solutions. Building onEG+ of Diakonikolas et al., 2021 that works under weak MVI conditions, the work presents a new algorithm CurvatureEG+ that works for a larger range of weak MVI condition compared to previous work and also works for the constrained and composite cases. The authors show cases where this algorithm converges while the previous algorithms can be shown to reach limit cycles. Overall, this theoretical work seems strong. Most reviewers seem to agree that the contribution is good enough for publication. Compared to EG+ the additional contribution is to expand the range of weak MVI condition. While this seems like a slight improvement, looking beyond just the final convergence rate, the paper has some nice insights that provides a unifying view that captures past algorithms (like EG+ as special case). I recommend acceptance.",0.8103825449943542,0.2898285078949162
g. The experiment shows an improvement in almost all cases. They point out that the rigid mapping between query-key and value pairs can lead to the learning of redundant query-key matrices and value matrices. Strengths: The paper presents a comprehensive analysis of the limit of multi-head attention.,"This paper identifies a limitation with current attention in transformers where they scoring with query-key pairs is strongly tied to retrieving the value and proposes a more flexible configuration that subsumes the previous setup but provides more flexibility. The authors shows this leads to improvements in various settings. Overall, all reviewers seem to agree there is interesting insight and results in this paper and it merits publication. Also the discussion helped stress important points regarding weight sharing and more. One concern is that the model was not evaluated on standard NLP/vision datasets (I assume alluding to GLUE/SuperGlue/SQuAD, etc.), and authors seem to hint that pre-training this is an issue for them computationally. This leaves open whether this indeed can and should replace the standard attention mechanism across the board, but is still very worthy of publication.",0.8438791036605835,0.39535347558557987
"positive spokesman - negative - """"Very good"" ""stunning"" - ""stunning"" - ""stunning"" - ""stunning"" - ""stunning"" - ""stunning""","All the reviewers liked the paper. The proposed method contains novel ideas of learning feature representation to maixmize the mutral informatio nbetween the latent code and its corresponding observation for fine-grained class clustering. The model seems to successfully avoid mode collapse while training generators and able to generate various object (foregrounds) with varying backgrounds. The foreground and background control ability is an outstanding feature of the paper. Please incorporate the comments of the reviewers in the final version. BTW, the real score of this paper should be 7.0 as Reviewer 5wFE commented that he/she would raise the score from 5 to 6 but at the time of this meta review, ths core was not raised. So the final score of the paper should be 8/8/6/6.",0.782423198223114,0.18669385409780911
The authors do not explain why MEME shows a better trend than MVAE or MMVAE in the relatedness experiments. Weakness: The paper is very well organized and easy to read. Strengths: The paper is very well organized and easy to read. Weakness: The authors do not explain why MEME shows a better trend than MVAE or MMVAE in the relatedness experiments.,"PAPER: This paper introduces a new method to learn joint representations from multimodal data, with potentially missing data. The primary novelty builds from the idea of semi-supervised VAE, introducing the concept of bi-directional information flow, which is termed “mutual supervision”. This approach brings the same advantages of semi-supervised VAE to the multimodal setting, allowing the cross-modal interactions to be modeled in the latent space. DISCUSSION: The discussion brought many important issues, addressed by both reviewers and authors. In general, it seems that most reviewers appreciate the technical novelty of the paper, related to the mutual supervision. While some concerns were expressed about the similarity with semi-supervised VAE (Joy et al., 2021), I would agree with other reviewers and the authors that the extension is not straightforward. Bi-directional information flow is a worthwhile novelty in itself. One reviewer also mentioned a concern about previous work on multimodal generative models; previous work on the same topic should not preclude new papers, as long new technical ideas are proposed. The final observation is about modeling more than 3 modalities. This is effectively a challenge with the proposed idea and should be acknowledged in the paper, but it is also an issue for many other approaches. New research will be needed to study 3+ modalities, but it should be seen as a future work direction. SUMMARY: Based on the reviews, discussion and personal reading of the paper, I lean towards acceptance of this paper. The paper introduces a new technical idea (bi-directional information flow, aka mutual supervision) which enables multimodal representation learning with missing data. The authors should revise their paper to acknowledge potential limitations of the approach (e. g., complexity challenges with 3+ modalities), but the idea is very interesting and worth publication.",0.8140509128570557,0.38870678891738253
paper is not very focused and the takeaways are unclear. it also proposes novel methods to start to address this area. it is not clear if there is that much more improvement to be had by exploring at a finer granularity. the authors managed to transmit a moment of reflection and reflect on the expansion of thoughts.,"Exploration can happen at various levels of granularity and at different times during an episode, and this work performs a study of the problem of exploration (when to explore/when to switch between exploring and exploitation, at what time-scale to do so, and what signals would be good triggers to switch). The study is performed on atari games. Strenghts: ------------ The study is well motivated and the manuscript is overall well written Studies a new problem area, and proposes an initial novel method for this problem extensive study on atari problems Weaknesses -------------- some clarity issues as pointed out by the reviewers no illustrative task is given to give a more intuitive exposition of the ""when to explore"" problem comparison to some extra baselines like GoExplore would have been insightful Rebuttal: ---------- Most clarity issues have been addressed satisfactorily. It has been explained why some requests for extra baselines would be challenging/or not relevant enough. While the authors agree that GoExplore would be an interesting baseline, they seem to have not added it. An illustrative task was not provided. Summary: ------------ All reviewers agree that this manuscript opens up and tackles a novel direction in exploration, and provides an extensive empirical study on atari games (a standard benchmark for such problem settings). While I agree with the reviewers that point out that this paper could have been made stronger by adding an illustrative task and additional baselines like GoExplore, there is a general consensus that the provided empirical study on this novel problem setting is a good contribution in itself. Because of this I recommend accept.",0.8198145031929016,0.29747152369883323
a. The method to obtain true dynamics error should be given in discussing Figure.1. b. The aim of section 5.3 is to study the effect of rollout horizon. c. The v2 dataset is much larger than the v0 dataset.,"This paper empirically studies various design choices in offline model-based RL algorithms, with a focus on MOPO (Model-based Offline Policy Optimization). Among the key design choices is the uncertainty measure used in MOPO that provides an (approximate) lower bound on the performance, the horizon rollout length, and the number of model used in ensemble. The reviewers are positive about the paper, found the experiments thorough, and the results filling a gap in the current literature. They have raised several issues in their reviews, many of which are addressed in the rebuttal and the revised paper. I would like to recommend acceptance of the paper. Also since the results of this work might be of interest to many researchers working on model-based RL, I also recommend a spotlight presentation for this work. I have some additional comments: (1) The paper studies the correlation of uncertainty measures with the next-state MSE, with the aim of showing which one has a higher correlation. The underlying assumption is that the next-state MSE is the gold standard that we should aim for. If we go back to the MOPO paper, we see that to define an uncertainty-penalized reward, we need an upper bound on the absolute value of G(s, a), which is the difference between the expected value of the value function at the next-state according to the true model and the estimated model. If we assume that the value function belongs to the Lipschitz function class w. r. t. a metric d, this upper bound is proportional to the 1-Wasserstein distance between the true next-state distributions and the model's distribution. If the dynamics is deterministic, 1-Wasserstein distance becomes the d(T(s,a),T^(s,a)) . If the distance d is the Euclidean distance, this becomes the squared error. Therefore, the squared error makes sense for deterministic dynamics, and it only provides an upper bound of |G(s,a)| . If the environment is not deterministic, the squared error may not be a reasonable gold standard anymore to compare the correlation of various uncertainty measures with. The paper introduces a generic MDP framework, but does not mention anything about its focus on MBRL for deterministic environments until the last sentence of its conclusion. Please clarify this in your camera ready paper. (2) The experiments are conducted using 3 or 4 seeds. Although this is the common practice in the deep RL community, it is too small. Standard deviations in Tables 1, 2, ... are computed with 3 seeds, which would be cringeworthy to statisticians and empirical scientists. I encourage the authors to increase the number of independent random experiments to make their results more powerful.",0.8033914566040039,0.4202456988083819
... 3. Model the extraction attack as a sequence-to-sequence problem. 4. Comprehensive empirical results to show the effectiveness of the components. Cons: 1. 2. Model the extraction attack as a sequence-to-sequence problem. 3. Comprehensive empirical results to show the effectiveness of the components.,All reviewers agree on acceptance and I agree with them. I recommend a spotlight.,0.8362159132957458,0.19777199402451515
"the Constraints: they propose to optimize the state-action stationary distribution directly, which avoids the instability caused by triple optimization problems for the actor, the critic and the cost Lagrange multiplier with three different objective functions. Constraints: they propose to optimize the state-action stationary distribution directly, which avoids the instability caused by triple optimization problems for the actor, the critic and the cost Lagrange multiplier with three different objective functions. Suggestions In tabular CMDPs and continuous tasks,","This paper presents a new technique for constrained offline RL. The proposed method is based on reducing a nested constrained optimization problem to a single unconstrained optimization problem that can be efficiently represented with a neural network. The proposed algorithm is tested against several baselines on both random grid-worlds and continuous environments. Results clearly show that the proposed algorithm outperforms baselines while keep the provided constraints satisfied. The reviewers agree that the paper is well-written, the proposed algorithm is novel and technically sound, and the empirical evaluation clearly supports the claims of the paper. There were some concerns regarding the novelty of this idea, but these concerns were properly addressed by the authors in the discussion.",0.8225896954536438,0.4017011635005474
'weakness': not enough baselines to compare with. 'the number of time steps where collision happens over the course of an episode' states the explanation seems a bit overly complicated. 'the number of time steps where collision happens over the course of an episode' states the explanation seems a bit overly complicated. 'the number of time steps where collision happens between the robot and obstacles over the course of an episode' states the explanation seems a bit overly complicated.,"The paper addresses vision-based and proprioception-based policies for learning quadrupedal locomotion, using simulation and real-robot experiments with the A1 robot dog. The reviewers agree on the significance of the algorithmic, simulation, and real-world results. Given that there are also real-robot evaluations, and an interesting sim-to-real transfer, the paper appears to be an important acceptance to ICLR.",0.8200467824935913,0.20654757507145405
"the paper is well organized and written, and easy to follow. Cons: The architecture of the proposed method is quite clean and simple. Several useful techniques and tricks are proposed with detailed experiments.. The paper mainly focus on finding regularization techniques and minor changes, which works for ViT.","The paper proposes a GAN architecture with a ViT-based discriminator and a ViT-based generator. The paper initially received a mixed rating with two ""slightly above the acceptance threshold"" ratings and ""three slightly below the acceptance threshold"" ratings. Several concerns were raised in the reviews, including whether there are advantages of using a ViT-based GAN architecture over the CNN-based GAN and whether the proposed method can be extended to high-resolution image synthesis. These concerns are well-addressed in the rebuttal with most of the reviewers increasing their ratings to be above the bar. The meta-reviewer agrees with the reviewers' assessments and would like to recommend acceptance of the paper.",0.848392903804779,0.33239142745733263
a) The paper is well written and the problem well motivated. b) The key contribution seems to be extension of soft decision trees to the recurrent setting which is a nice and clinically useful contribution. c) The algorithm used to train is certainly a heuristic but seems reasonable.,"This paper proposes a tree-based method for interpretable policy learning, for fully-offline and partially-observable clinical decision environments. The models are trained incrementally, as patient information becomes available. The method was overall deemed novel by the reviewers, and the interpretability of the model well validated by clinicians. Numerous points of clarification were brought up by reviewers, related to the notation, learning process and result reporting. All of the concerns were responded to by the authors in great detail and the manuscript was appropriately revised. All the reviewers have raised their scores as a result of the updates. Thus, the paper is ready for acceptance.",0.8392010927200317,0.37751739010924384
a few points that the paper can improve upon: comparing wall-clocks of TRGP/GP. Strengths: The paper addresses an important problem with a good solution is theoretically sound. Weaknesses: I don't see any major weaknesses in this paper.,"The submission addresses the problem of whether or not to update weights for a previous task in continual learning. The approach is to specify a trust region based on task similarity and update weights only in the direction of the tasks that are similar enough to the current one. The paper was on the balance well received (3/4 reviewers recommended acceptance, 2 with scores of 8) and complemented for its simple but effective approach, and good discussion of related literature. The submission attracted a reasonable amount of engagement and discussion between reviewers and authors, which should be taken into account in the final version of the paper.",0.8286193609237671,0.3771249453226725
"g. The paper does not present any empirical results or experimental analysis. g., the definition of Gid after eq. g., the extension to the unknown mechanism and stochastic mechanisms. g., the definition of Gid after eq.","The paper provides new insights about how to identify latent variable distributions, making explicit assumptions about invariances. A lot of this is studied in the literature of non-linear ICA, although the emphasis here is on dropping the ""I"". I think more could be said about how allowing for dependencies among latents truly change the nature of the problem since any distribution can be built out of independent latents, by some more explicit contrast against the recent references given by the reviewers. In any case, the role of allowing for dependencies in the context of the invariances adopted is discussed, and despite no experimentation, the theoretical results are of general interest to the ICLR community and a worthwhile contribution to be discussed among researchers in this field.",0.824874222278595,0.27880528531968596
False spokesman: a sufficient survey should be conducted on layer fusion of BERT. False spokesman: a sufficient survey should be conducted on layer fusion of BERT. True spokesman: a sufficient survey should be conducted on layer fusion of BERT.,"This paper has a deep analysis of the over-smoothing phenomenon in BERT from the perspective of graph. Over-smoothing refers to token uniformity problem in BERT, different input patches mapping to similar latent representation in ViT and the problem of shallower representation better than deeper (overthinking). The authors build a relationship between Transformer blocks and graphs. Namely, self-attention matrix can be regarded as a normalized adjacency matrix of a weighted graph. They prove that if the standard deviation in layer normalization is sufficiently large, the outputs of the transformer stack will converge to a low-rank subspace, resulting in over-smoothing. In this paper, they also provide theoretical proof why higher layers can lead to over-smoothing. Empirically , they investigate the effects of the magnitude of the two standard deviations between two consecutive layers on possible over-smoothing in diverse tasks. In order to overcome over-smoothing, they propose a series of hierarchical fusion strategy that adaptively fuses presentation from different layers, including concatenation fusion, max fusion and self-gate fusion into post-normalization. These strategies reduce similarities between tokens and outperforms BERT baseline on a few datasets (GLUE, SWAG and SQuAD). Overall I agree with reviewers that this is a good contribution.",0.806721568107605,0.2824073093632857
Strengths: Overall the paper is well written in my opinion. Overall the paper is well written in my opinion. Strengths I think this paper would benefit from a more detailed discussion of Theorem 2. Strengths....,"*Summary:* Low-rank bias in nonlinear architectures. *Strengths:* - Significant theoretical contribution. - Well written; detailed sketch of proofs. *Weaknesses:* - More intuitions desired. - Restrictive assumptions. *Discussion:* Authors made efforts to improve the discussion in response to 6P7z. Authors agree with eeoo about Assumption 2 being relatively restrictive but point out that main results do not need it. They discuss Assumption 1 and revised it formulation. Reviewer eeoo was satisfied with this. Following the discussion udhX raised their score (after authors acknowledged an early problems and improved them) and found the paper well written with novel and significant results. *Conclusion:* Three reviewers consider this a good paper that should be accepted. A fourth reviewer rated it marginally above the acceptance threshold but following the discussion period explicitly recommended acceptance. I find the topic interesting, timely, relevant. In view of unanimously favorable feedback from four reviewers I am recommending accept.",0.8429248332977295,0.4705674754721778
Cons: Paper is very well written and flows well. Cons: Paper is very well written and flows well. Cons: Paper is very well written and flows well. The paper is very well written and flows well.......,"Description of paper content: The paper addresses the problem of credit assignment for delayed reward problems. Their method, Randomized Return Decomposition, learns a reward function that provides immediate reward. The algorithm works by randomly subsampling trajectories and predicting the empirical return by regression using a sum of rewards on the included states. The method is compared to a variety of existing methods on Mujoco problems in “episodic reward” settings, where the reward is zero except for the final step of the episode, where it is the sum of rewards from the original task. Theoretical argument suggests the method is an interpolation of return decomposition (regress based on all states, not a subsample) and uniform reward distribution (send episodic reward to all states equally). By regressing with a subset of states, the method reduces compute for longer problems and is suggested to be more scalable. Summary of paper discussion: The reviewers largely commended the simplicity of the method, the simplicity of the presentation, the novelty of the algorithm, and the quality of the empirical results. The negative reviewer maintained their initial review’s score on account of a bias introduced by the algorithm.",0.7909153699874878,0.2365605963859707
Cons: This mathematical framework seems appropriate for analyzing the implicit regularization of SGD in the manifold of zero training loss. g. (2019) [Sect. ] [Sect. ] [Sect. ] [Sect.,All the reviewers agree that this paper made a solid contribution of understanding the algorithmic regularization of SGD noise (in particular the label noise for regression) after reaching zero loss. The framework is novel and has the potential to extend to other settings.,0.8625799417495728,0.3063663858920336
False positives: the paper makes unrealistic assumptions. the authors provide an ablation study to demonstrate that all of the components of the algorithm contribute to the improved performance. the experiments are the weaker portion of the paper. a positives: the paper provides a useful insight into anomalies.,"The paper tackles the problem of detecting anomalies in multiple time-series. All the reviewers agreed that the methodology is novel, sound and very interesting. Initially, there were some concerns regarding the experimental evaluation, however, the rebuttal and subsequent discussion cleared up these concerns to some extent and all reviewers are eventually supporting or strongly supporting acceptance.",0.854030430316925,0.4190530454119047
the proposed objective functions (quantile loss and continuous ranked probability score) appear well-motivated and have nice theoretical properties. the authors may also want to add an algorithm box into the main paper.. the paper introduces complex topics well and introduces complex topics well.,"The paper proposes a framework for training autoregressive flows based on proper scoring rules. The proposed framework is shown to be a computationally appealing alternative to maximum-likelihood training, and is empirically validated in a wide variety of applications. All three reviewers are positive about the paper and recommend acceptance (one weak, two strong). The reviewers describe the paper as well written and well motivated, and recognize the paper's contribution as significant. Overall, this is a nice and promising methodological exploration of flow-model training that is worth communicating to the ICLR community.",0.8479064106941223,0.3271957119305928
I have to ask Fig.1. I can see some inconsistencies. g................,"This work starts from the observation that maximum likelihood estimation, while consistent, has a bias on a finite sample which is likely to hurt for small sample sizes. From this, they apply Firth bias reduction to the few-shot learning setting and demonstrate its empirical benefits, notably relatively to L2 regularization or label smoothing alternatives. After some discussion with the authors, all reviewers are supportive of this work being accepted. Two are also suggesting this work be featured as a spotlight. The proposed method is simple, well motivated, and appears to be effective. Therefore, I'm happy to recommend this work be accepted and receive a spotlight presentation.",0.8294105529785156,0.14393324839572114
"Its theoretical contribution on the first finite time convergence result (to the stationary point) is important. (Zhang et al., 2018). it is important...?.??... (2018). ( 2018)... (e. ) in Algorithm 2. (e. ..... .",This paper provides actor-critic method for fully decentralized MARL. The results remove some of the restrictions from existing results and have also obtained a sample bound that matches with the bound in single agent RL. The authors also give detailed responses to the reviewers' concerns. The overall opinions from the reviewers are positive.,0.8227063417434692,0.13844440123066307
alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = alpha = al,"This paper presents an interesting analysis of mixup, discussing when it works and when it fails. The theory is further illustrated with small but intuitive examples, which facilitates understanding the underlying phenomena and verifies correctness of the predictions made by the theory. The submission has received three reviews with high variance ranging from 3 to 8: mn55 favoring rejection while eGEK recommending accept. I read all the reviews and authors' response. Unfortunately, mn55 did not follow up to express how convinced they are with author's reply, but I do find the responses to mn55 very solid and convincing. In concordance with eGEK, I do find the provided analysis important and helpful, and the presentation of the theory through concrete examples very compelling.",0.7277692556381226,0.14422058717658123
SEFS (no SS) isn't consistent from table 1. Language should be clarified / copy-edited prior to publication.............,"This paper proposes a feature selection method to identify features for downstream supervised tasks, focused on addressing challenges with sample scarcity and feature correlations. The proposed approach is highly motivating in biological and medical applications. Reviewers pointed out various strengths including potential high impacts in biomedical applications, technical novelty and significance, and comprehensive and illustrative experiments. The authors adequately addressed major concerns raised by reviewers.",0.8200052976608276,0.16764408955350518
a. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s,"The paper develops a diffusion-process based generative model that perturbs the data using a critically damped Langevin diffusion. The diffusion is set up through an auxiliary velocity term like in Hamiltonian dynamics. The idea is that picking a process that diffuses faster will lead to better results. The paper then constructs a new score matching objective adapted to this diffusion, along with a sampling scheme for critically damped Langevin score based generative models. The idea of a faster diffusion to make generative models is a good one. The paper is a solid accept. Reviewer tK3A was lukewarm as evidenced by their original 2 for empirical novelty that moved to a 3. From my look, it felt like a straightforward application of ideas in one domain, sampling, to another, generative modeling. It's a good paper, but it does not stand out relative to other accepts.",0.794416069984436,0.14642947928282862
"The paper is very simple, but principled and novel. the results for fairness are somewhat uninteresting. g. The control style transfer problem seems somewhat contrived. g. The results for fairness are somewhat uninteresting. g. The results for style transfer are somewhat uninteresting. g. The results for fairness are somewhat uninteresting.","This paper introduces the concept of classifier orthogonalization. This is a generalization of orthogonality of linear classifiers (linear classifiers with orthogonal weights) to the non-linear setting. It introduces the notion of a full and principal classifier, where the full classifier is one that minimizes the empirical risk, and the principal classifier is one that uses only partial information. The orthogonalization procedure assumes that the input domain, X can be divided into two sets of latent random variables Z1 and Z2 via a bijective mapping. The random variables Z1 are the principal random variables, and Z2 contains all other information. Z1 and Z2 are assumed to be conditionally independent given the target label. The paper outlines two approaches to construct orthogonal classifiers that operate only on Z2. The approach is highlighted in three applications: controlled style transfer, domain adaptation, and fair classification. The reviewers all found the proposed method to be principled and compelling. Beyond clarification questions and some discussion on related work, the reviewers raised a few issues that were subsequently addressed: 1) Additional baselines for domain adaptation and fairness. 2) Controlled style transfer being a new task with no established baselines, and 3) The feasibility of training a proper “full classifier” that minimizes the empirical risk, and its necessity in the approach. The authors addressed these concerns and updated the paper, to the satisfaction of the reviewers. All of them unanimously recommend acceptance.",0.8132020831108093,0.28290227078474484
False Angaben: RL is not a scalable model. False Angaben: RL is a scalable model. False Angaben: RL is a scalable model. False Angaben: RL is a scalable model.,"A novel method is described that uses RL to search for a rule set which predicts multiple relations at once for KBC-like problems. The rules can include latent predicates, which reduces the complexity of individual rules, similar to Cropper & Muggleton's (2015) meta-interpretive learning framework, which is usual for rule-learning systems. Another novel aspect is use of a cache memory for rules. Pros - the idea of using RL instead of carefully-designed discrete search for symbolic learning systems is a very nice novel idea - the experimental results are strong Cons - the benchmarks are synthetic (although GraphLog does at least include noise) - although the Cropper and Muggleton work is cited the relationship between the search spaces of the two systems is not discussed - this should be corrected in the final version.",0.8149818181991577,0.3415126968175173
"the paper is well-written, and the technical part of this paper is easy-to-follow. on the plus side, - the REP-UCB algorithm is conceptually very simple. - The sample complexity in this paper is measured by the number of episodes N.","In this paper, the authors extend the FLAMBE to the infinite-horizon MDP and largely improved the sample complexity of the representation learning in FLAMBE. Meanwhile, the authors also consider the offline representation learning with the same framework. Although there is still some computational issue in MLE for the linear MDP, the paper completes a solid step towards making linear MDP for practice. The paper could be impactful for the RL community. As the reviewers suggested, there are still several minors to be addressed: - The extension of the proposed algorithm for finite-horizon MDP should be added. - The directly comparison between the sample complexity of FLAMBE and the proposed algorithm in infinite-horizon MDP is not appropriate. The authors should clarify the difference here. - The organization of the proof is not clear. As reviewer suggested, the one-step back trick should be emphasized for better significance of the submission.",0.8428032994270325,0.32431283758746254
paper aims to highlight the interesting concept of PCs. but the methodological contribution was not clear enough. the paper's main issues were that the methodological contribution was relatively difficult to understand. the paper's authors claim that the complexity of PC is O(log D |p|) where |p is number of neural network units.,"The paper revisits lossless compression using deep architecture. In contrast to main stream approaches, it suggests to make use of probabilistic circuits, introducing a novel class of tractable lossless compression models. Overall, the reviews agree that this is an interesting direction and a novel approach. I fully agree. Actually, I like that the paper is not just saying well, we could use a probabilistic circuit for ensure tractability but also shows that there is still a benefit of different variable orderings for encoding and decoding. In any case, adding probabilistic circuits to the ""compression family"" is valuable and also paves the way to novel hybrid approaches, combining neural networks and probabilistic circuits. I have enjoyed reading the paper, reviews, and discussion.",0.8300796747207642,0.28235647601208513
I think proofs are necessary to get most of the paper. g. The main limitation of the paper is in assuming no fine-tuning with real-world samples. g. The main limitation of the paper is in assuming no fine-tuning with real-world samples.,"This manuscript introduces a theoretical framework to analyze the sim2real transfer gap of policies learned via domain randomization algorithms. This work focusses on understanding the success of existing domain randomization algorithms through providing a theoretical analysis. The theoretical sim2real gap analysis requires two critical components: *uniform sampling* and *use of memory* **Strengths** All reviewers agree that this manuscript provides a strong theoretical analysis for an important problem (understanding sim2real gap) well written manuscript, and well motivated Intuitive understanding for theoretical analysis is provided **Weaknesses** analysis is limited to sim2real transfer without fine-tuning in the real world the manuscript doesn't provide a novel experimental evaluation lack of take-aways **Rebuttal** The authors acknowledge the limitation of not addressing fine-tuning, but also point out that several papers have performed sim2real transfer without fine-tuning. The authors address the lack of novel experimental evaluation by arguing that the theoretical analysis can be directly linked to existing algorithms for which empirical evaluations have already been performed. I agree with the authors that in that context it seems of little value to redo those experiments. However, I also believe that those links could be made even clearer in the manuscript and I would encourage the authors to do so. Furthermore, while the authors do provide intuitive take-aways for domain randomization algorithms, it would be helpful if those take-aways were more clearly linked to existing algorithms as well (given that there is no experimental evaluation of this). **Summary** This manuscript provides a theoretical framework for analyzing the sim2real gap and using that framework provides bounds on the sim2real gap. All reviewers agree this is a strong theoretical analysis. Some take-aways on what makes domain randomization algorithms successful are provided by the provided sim2real-gap analysis (memory use, uniform sampling). Thus I recommend accept.",0.8181354403495789,0.32288169034502723
Strength: Sound theory of group-invariant MDP.: The description sometimes is hard to follow. Strength: The paper pursues an interesting research problem. The paper is well written. The paper is well written. the description sometimes is hard to follow.,"The paper investigates the use of equivariant neural network architectures for model-free reinforcement learning in the context of visuomotor robot manipulation tasks, exploiting rotational symmetries in an effort to improve sample efficiency. The paper first provides a formal definition and theoretical evaluation of a class of MDPs for which the reward and transition are invariant to group elements (""group-invariant MDPs""). It goes on to describe equivariant versions DQN, SAC, and learning from demonstration (LfD). Experiments on a set of different manipulation tasks reveal that the proposed architectures outperform contemporary baselines in terms of sample complexity and generalizability, while ablations demonstrate the contribution of the different model components. The idea of structuring a neural architecture to exploit symmetry present in a domain as a means of improving sample complexity is compelling and principled. The contributions of the paper are two-fold. First, while the idea of exploiting symmetry in the context of deep RL is not new, the paper describes a variation of equivariant DQN that is effective for visual control domains (visuomotor control) that are more challenging and realistic than those considered previously. Second, the paper proposes novel equivariant versions of SAC and LfD and validates their effectiveness through extensive experiments. Following a detailed author response to the initial reviews together with the inclusion of additional experiments and other updates to the paper, the reviewers largely agree on the significance of these contributions and value of the paper as a whole.",0.8145716786384583,0.40790227684709757
a negative sample doesn't have much impact on the results. it would be interesting to see a ablation showing the performance dip as we decrease the #negative samples. Strengths: (1) The motivation of this paper is clear and the problem they studied is valuable. the method for generating ‘augmentations’ for contrastive learning via random replacement from the marginal feature distribution makes sense and is novel to the best of my knowledge.,The paper explores self-supervised learning on tabular data and proposes a novel augmentation method via corrupting a random subset of features. The idea is simple but effective. Experiments include 69 datasets and compare with a number of methods. The result shows its superiority. It would be inspiring more work for SSL on the tabular domain.,0.8508908748626709,0.33664673417806623
ii) The authors use bit-wise accuracy to evaluate the fingerprint detection performance. iii) The authors use bit-wise accuracy to evaluate the fingerprint detection performance. iii) The authors use bit-wise accuracy to evaluate the fingerprint detection performance.,"The paper proposes and studies a method for the responsible disclosure of a fingerprint along with samples generated by a generative model, which has important applications in identifying ""deep fakes"". The authors establish both the detectability of their fingerprint-without significant loss of fidelity-as well as the robustness to perturbations. The reviewers found the problem and contributions to be important and significant, well substantiated by an extensive experimental study.",0.8318623900413513,0.5955260147651036
"despite this theoretical inconsistency, the experimental section shows that the proposed method works better than [GWG]. (b) Is there a typo in equation 10? Should it be p instead of n in the summation? (c) Is there a typo in equation 10? (d) Is there a typo in equation 10? (e) Is there a typo in equation 10? (f) Is there a typo in equation 10? (f) Is there","This paper provides a novel path auxiliary algorithm for more efficiently exploring discrete state spaces within a Metropolis-Hastings sampler for energy based models. In particular, it essentially replaces the ""single site update"" by instead proposing an entire path using local information, thus enabling the chain to take larger steps, which can improve acceptance/mixing significantly as they demonstrate. The work is a timely contribution that improves upon exciting recent work. After much discussion among several knowledgeable reviewers and clarifications regarding some details of the main theorem from the authors, there is consensus that the contributions are correct, novel, and likely of impact to the machine learning community. Since the revision period, the empirical evaluations have also been improved and the contributions have methodological novelty as well as promising practical performance.",0.8116912245750427,0.26441832184791564
"the paper addresses an important problem and contributes real progress. Strengths: I like the idea of learning affordance and using them to prune impossible subtasks during exploration. Weaknesses: Not many, none serious, but some serious. the paper presents experiments that demonstrate the effectiveness of the proposed method and provides extensive ablation studies.","This paper proposes a hierarchical reinforcement learning approach that exploits affordances to better explore/prune the subtasks, and thus making the overall learning more efficient. The idea of the paper is novel and interesting. After the rebuttal, all the reviewers agree that the paper is a solid contribution. Therefore, I recommend acceptance of this paper.",0.8717201948165894,0.41253793984651566
The paper is not a comprehensive paper.: The paper is not a comprehensive paper. The paper is a novel unsupervised technique that is competitive or outperforms the state of the art...........,"A multi-scale hierarchical variational autoencoder based technique is developed for unsupervised image denoising and artefact removal. The method is shown to achieve state of the art performance on several datasets. Further, the multi-scale latent representation leads to an interpretable visualization of the denoising process. The reviewers unanimously recommend acceptance.",0.844569206237793,0.2790776351466775
"e. Did you experiment with different configurations of,,?. e. Did you experiment with different configurations of,,?. e. Did you experiment with different configurations of,,?. e. Did you experiment with different configurations of,,?. e. Did you experiment with different configurations of,,?. e. Did you experiment with different","Thank you for your submission to ICLR. The reviewers and I are in agreement that the paper presents a substantial contribution to the field at the intersection of differentiable simulation and ML methods. In particular, the half-inverse method is compelling, non-obvious, and hints of a nice path forward towards the goal of practical differentiable simulations within models. Overall I'm happy to recommend the paper be accepted.",0.8012282848358154,0.17187786164383095
The regret bound is tighter than NeuralUCB and NeuralTS. 2020] or Thompson Sampling [Zhang et al. 2021......................................,"Summary: This paper studies the neural contextual bandit problem, and proposes a neural-based bandit approach with a novel exploration strategy, called EE-Net. Besides utilizing a neural network (Exploitation network) to learn the reward function, EE-Net also uses another neural network (Exploration network) to adaptively learn potential gains compared to currently estimated reward. Discussions: The reviewers appreciated the novelty and the quality of the ideas and results in this paper. Most questions were about details in algorithm design choices and in the analysis. The authors have addressed these questions and updated their draft. The reviewers have now reached a consensus and recommend accepting this paper. Recommendation: Accept.",0.7994594573974609,0.20910857085670742
:: I think the proposed method is very practical and the ideas of this paper are organized logically. CONS: I think the proposed method is very practical and the ideas of this paper are organized logically. DIS: I think the proposed method is very practical and the ideas of this paper are organized logically.,"The authors propose a rank coding scheme for recurrent neural networks (RNNs) - inspired by spiking neural networks - in order to improve inference times at the classification of sequential data. The basic idea is to train the RNN to classify the sequence early - even before the full sequence has been observed. They also introduce a regularisation term that allows for a speed-accuracy trade-off. The method is tested on two toy-tasks as well as on temporal MNIST and Google Speech Commands. The results are very good, typically improving inference time with very little loss in accuracy. Furthermore, the idea seems novel and the paper is well written. An initial criticism was that experiments with spiking neural networks (SNNs) were missing. The authors added a proof of concept for SNNs, which satisfied the reviewer. The authors also added some control experiments in response to the initial reviews, which improved the manuscript. In summary, the manuscript presents a valuable novel idea with good experimental verification and interesting aspects both for ANNs and SNNs. The reviewers consistently vote for acceptance.",0.8205996155738831,0.3766625140878287
"it would be useful to show the certified accuracy(CA) of a whitebox algorithm like (Cohen et al., Madry et al.) as comparison. 1. While the autoencoder reduces the variance of the gradient estimates, the fact that 2q forward passes need to be made for every gradient computation, should indicate that turning this method into a ZO-method increases the computational burden.","All reviewers have converged to an unanimous rating of the paper, highlighting, in the paper or during the discussion, many strengths, including a compelling approach clearly relevant to applications and its solid range of experiments. A clear accept, and I would encourage the authors to push in the final version the experiments and discussions following the threads with reviewers (in particular, Vo8C and ULvk). Thanks also to authors and reviewers for a thorough discussion which helped to strengthen further the paper's content. AC.",0.8126921653747559,0.27110018767416477
the authors propose a relax loss to defend privacy leakage. they find that membership privacy risks can be reduced by narrowing the gap between the loss distributions. negative aspects: - The paper lacks intuition behind some assumptions and choices made in designing the algorithm (more below),The paper proposes an approach and specific training algorithm to defend against membership inference attacks (MIA) in machine learning models. Existing MIA attacks are relatively simple and rely on the test loss distribution at the query point and therefore the proposed algorithm sets a positive target mean training loss value and applies gradient ascent if the average loss of current training batch is smaller than it (in addition to the standard gradient descent step). The submission gives extensive experimental results demonstrating advantage over existing defense methods on several benchmarks. The primary limitation of the work is that it defends only against rather naive existing attacks which do not examine the model (but rely only on the loss functions).,0.8423569202423096,0.3570155861477057
"'6: marginally above acceptance threshold', but would be willing to raise my score. paper is clear and easy to follow, but lacks a formalization. 'it is a good paper and i am in favour of its acceptance', says john.","This contribution investigates and takes a step back on an important problem in recent ML, namely the impact of the noise distribution in density estimation using Noise Contrastive Estimation. The work offers both theoretical insights and convincing experiments. For these reasons, this work should be endorsed for publication at ICLR 2022.",0.8408664464950562,0.25802475959062576
The paper is not clear on how the runtime of the approaches differs. W1... I think. S1...... S1 The paper is in general well written and the proposed method produces quite interesting results.,The reviewers are unanimous that this is a strong submission that deserves to be accepted.,0.8650791049003601,0.18051514849066735
"I have 2 concerns with this proposal: It requires an automated unit test generation suite to be available for at least one of the languages translated. This, to me, indicates an issue with the utilized test set where gold implementations might not be correct translations. Other concerns/Suggestions: The authors propose to utilize an automated unit test generation suite to build this parallel data from the model translations itself.","This paper is about unsupervised translation between programming languages. The main positive is that it introduces the idea of using a form of unit test generation and execution behavior within a programming language back-translation setup, and it puts together together a number of pieces in an interesting way: text-to-text transformers, unit test generation, execution and code coverage. Results show a substantial improvement. The main weaknesses are that there are some caveats that need to be made, such as the (heuristic, not learned) way that test cases are translated across languages is not fully general, and that limits the applicability. There are also some cases where I find that the authors are stretching claims a bit beyond what experiments support, e. g., in the response to zd7L about applicability to COBOL. All-in-all, though, it's a good implementation of an idea that should have a lasting place in this line of work, so it's worth accepting.",0.8372690081596375,0.40498550342661993
"a rejection-sampling-based method is definitely the main focus of this paper. authors conducted comprehensive experiments to show that in practice, we can easily allow efficient sampling. a rejection-sampling-based method is not strictly sub-linear-time in general.","This is an exciting paper that provide the efficient algorithms for exact sampling from NDPPs along with theoretical results that are very pertinent in and out themselves. The AC agree with the reviewers that the authors satisfactorily addressed the concerns raised in the reviews, and is convinced that the revised version will be greatly appreciated by the community. We very much encourage the authors to pursue this line of work and in particular to overcome the practical restriction to the ONDPP subclass.",0.8478034734725952,0.38914284110069275
e. coli synthesis trees are a special case of DAGs. e.coli synthesis trees are a special case of DAGs. e.coli synthesis trees are a special case of DAGs. e.coli synthesis trees are a special case of DAGs.,"After much back and forth about prior work, 3 reviewers score this paper as an 8 and one scores it as a 3. Other reviewers have written to the 3 and told them they believe that their review is now too harsh, in light of clarifications w. r. t. related work. I tend to agree, though I must admit that I am not an expert on this topic. Given that there is almost unanimous support for accepting and it's possible that the one hold-out has not seen some of the extra information, I recommend acceptance. Given the praise from the other three reviewers, I moreover recommend a spotlight.",0.8081371188163757,0.2931071173399687
"a novel methodology and extensive empirical evaluation. a unique study of antibody loop generation with both novel methodology and extensive empirical evaluation. Strengths: The use of a predictor to evaluate neutralization is justified based on very recent work. The experimental evaluation may be problematic, it is not convincing to use machine learning methods to predict neutralization ability based on CDR H3.","This paper proposes use of a novel generative modelling approach, over both sequences and structure of proteins, to co-design the CDR region of antibodies so achieve good binding/neutralization. The reviewers are in agreement that the problem is one of importance, and that the technical and empirical contributions are strong. There are concerns over the relevance of evaluating the method by using a predictive model as ground truth. Still, the overall contributions remain.",0.86630779504776,0.38404166232794523
"I encourage authors to make additional improvements, especially for the experiments described in Section 5. I. The paper's main strength is that the authors' long proof of the performance guarantee (not sure how tight the bound is), and the key implementation codes are provided in the Appendix. I. The paper's main strength is that the authors' lengthy proof of the performance guarantee (not sure how tight the bound is), and the key implementation codes are provided in the Appendix. I. The paper introduces this technique as a simpler solution for","The paper introduces a procedure to control the churn (i. e. differences in the predictive model due o retraining) using distillation. This is a strong paper, with novel technique which is clearly presented, and is backed by sound theory. The experimental results were also deemed extremely convincing by reviewers TJ4g and pZBb. Reviewer nqfu raised a question about the similarities between churn reduction and domain adaptation. The authors have addressed this by pointing out similarities to their work but also noting that, in the settings mentioned by the reviewer, alternative approaches such as completely retraining the model might be more appropriate. This part of the rebuttal is convincing. Reviewer TJ4g has pointed out several points of improvement, to which the authors have responded adequately. All in all, this paper is ready for and deserving of acceptance.",0.8275368213653564,0.4532711957182203
"g. as in Xu et al. The paper presents a method that is novel in terms of the estimand it proposes, the density ratio, in order to learn the structural function. the authors present error analyses for the density ratio and the causal function, as well as experiment results comparing their work to recent algorithms in the field.","This paper proposes a method that uses conditional moment restriction methods to estimate causal parameters in non-parametric instrumental variable settings. This is done by converting to an unconditional moment restriction setting common in the econometrics causal inference literature. The paper was reviewed quite favorably by reviewers, and the authors updated the manuscript to address specific issues raised by reviewers.",0.8584075570106506,0.3231870442007979
"despite the above strengths, there are some vague parts in the current version. the paper excels at fixing issues with (Lorraine, 2019) the paper excels at fixing issues with (Lorraine, 2019) the paper excels at fixing issues with (Lorraine, 2019)","The paper provides a method for with tuning continuous hyperparameters (HPs). It is closely related to a previous work (Lorraine, 2019) that was limited to certain HPs, and in particular could not be applied to HPs controlling the learning such as learning rate, momentum, and are known to be influential to the convergence and overall performance (for non-convex objectives). The reviews indicate a uniform opinion that the paper tackles an important problem, that its methods provide a non-trivial improvement over previous techniques and in particular those of (Lorrain, 2019), and that the provided experiments are extensive and convincing. The initial reviews had several concerns about technical details in the paper such as the analysis or how the meta-hyperparameters are tuned. However, in the discussions the authors provided adequate responses, resolving these concerns. I believe that with minor edits that are possible to get done by the camera-ready deadline the authors can incorporate their responses into the paper making it a welcome addition to ICLR.",0.8236873149871826,0.387617826461792
"False characterization of the results of the study is not a sufficient basis for acceptance. e.g., down-weighting by variance and using probabilistic ensembles are well-known to the community. e.g., inverse variance weights, and aleatoric uncertainty quantification through the Gaussian NLL loss.",The reviewers unanimously appreciated the clarity of the work as well as the framing of the proposed method. Congratulations.,0.8404334187507629,0.2207687422633171
The State of Sparsity in Deep Neural Networks by Gale et al. a? Strengths a? Strengths......  .,"This work considers one-shot pruning in deep neural networks. The main departure from previous work is to consider stochastic Frank-Wolfe. The reported results are convincing although a number of baselines were missing from the initial submission. The authors provide a balanced account of the strengths and weaknesses of the proposed approach. The authors adequately addressed the concerns of the reviewers. For instance they ran additional experiments to compare to missing pruning baselines. I would encourage the authors to revise the manuscript by including the missing related work, the additional clarification discussions (e. g., motivation for K-sparse constraints, follow-up analysis, and cost per iteration) and to include the additional experiments that were conducted (e. g., pruning with training).",0.7970463633537292,0.24321621821986303
"The method is novel, extending prior work The paper is well-written and easy to follow. a paper on this topic pp. 1-2., the paper is well-written and easy to follow. The method is novel The method is novel for all systems, but not all. The The","This work propose to learn hierarchical skill representations that, as opposed to prior work, consist of both discrete and continuous latent variables. Specifically, this work proposes to learn 3 level hierarchy via a hierarchical mixture latent variable model from offline data. For test time usage and adaptation on down-stream tasks, the manuscript proposes two ways of utilizing the learned hierarchy in RL settings. **Strengths** A novel method to learn hierarchical representations with mixed (discrete/continuous) latent variables is proposed Detailed experimental evaluation, and baseline comparisons, show promising results **Weaknesses** There were various clarity issues as pointed out by the reviewers (fixed in rebuttal phase) The related work was missing relevant work, and the proposed framework was not connected well to existing work (fixed during rebuttal) **Rebuttal** The authors significantly updated the manuscript based on the feedback of the reviewers, and improved both clarity of the manuscript (method+experiments) as well as the exposition of the proposed framework with respect to related work. **Summary** After the rebuttal, all reviewers agree that this is a good paper that should be accepted. Thus my recommendation is accept.",0.8171765208244324,0.32374539847175277
"The authors present an optimization method for the new loss that has similar rate of convergence as standard SGD Experiments are presented using a large number of natural and medical image public datasets. I am confused about the DDSM+ dataset, where the authors state they have combined the DDSM with CBIS-DDSM datasets.",I recommend this paper to be accepted. All reviewers are in agreement that this paper is above the bar.,0.8504236936569214,0.09135008603334427
The paper is well organized. Weakness: The paper is well organized. Strength The paper is well organized. Weakness The paper is well organized. Strength The paper is well organized. Weakness The paper is well organized. Weakness The paper is well organized. Weakness The paper is well organized.,"In this paper, the authors generalize the univariate Shapley method to bivariate Shapley method. The authors first build a directly graph based on the asymmetric bivariate Shapley value (adding feature j to all sets contained feature i). Then several graph algorithms are applied to analyze the directly graph to derive (1) univariate feature importance available in univariate approach and (2) relations like mutually redundancy only available in bivariate approaches. Experiments on several datasets with comparison to existing methods demonstrated the superiority of the proposed method. All reviews are positive.",0.797885000705719,0.20114190876483917
"a) The paper is an impressive empirical study covering a vast range of variations, baselines and ablations. b) The paper is an impressive empirical study covering a vast range of variations, baselines and results. c) The paper is an impressive empirical study covering a vast range of variations, baselines and results.","This paper studies the Lottery Ticket hypothesis in reinforcement learning for identifying good sparse representations for low-dimensional tasks. The paper received initial reviews tended towards acceptance. However, the reviewers had some clarification questions and concerns. The authors provided a thoughtful rebuttal. The paper was discussed and most reviewers updated their reviews in the post-rebuttal phase. Reviewers generally agree that the paper should be accepted but still have good feedback. AC agrees with the reviewers and suggests acceptance. However, the authors are urged to look at reviewers' feedback and incorporate their comments in the camera-ready.",0.8187661170959473,0.3305468101364871
"Theorem 1: Weaknesses: Theorem 1 could have more explanation. g., iNaturalist 2018) are not considered. g., iNaturalist 2018) are not considered. g., iNaturalist 2018) are not considered. g., iNaturalist 2018) are not considered. g., iNaturalist 2018) are not considered. g., iNaturalist 2018) are not considered.","This paper is proposed to investigate the robustness of self-supervised learning (SSL) and supervised learning (SL) in both balanced (in domain) and imbalanced (out of domain) settings. It can be concluded that SL can regularly learn better representations than SSL, and representations are better from balanced than from imbalanced datasets. The SSL is more robust than SL in the imbalanced settings, which is the crucial of this paper. Expect the experimental results, the authors also provided theoretical analysis to support their claims. The authors also extend a well-established method SAM into the Reweighted SAM as the technical contribution to better address the imbalanced setting. The paper is well written with clear logic to follow.",0.7765727639198303,0.21139373169058845
it is possible to train other methods on very large datasets like OC20. the.. The paper is well written and the terminology is difficult to follow for someone without chemistry.......,"This paper builds on the success of the FermiNet neural wave function framework by pairing it with a graph neural network which predicts the parameters of neural wave function from the geometry. The resulting PESNet trains significantly faster, with no loss of accuracy. This method constitutes an important advance in ML-powered quantum mechanical calculations. The reviewers unanimously recommend acceptance.",0.8386420607566833,0.24011784543593723
Weakness W1. The theoretical results are strong and interesting. Strength S1. The theoretical results are strong and interesting. Strength W1. The theoretical results are strong and interesting. Strength W2. The authors should clarify the implementation details. LOWER BOUND The paper provides a lower bound.,"This paper addresses the reward-free exploration problem with function approximation under linear mixture MDP assumption. The analysis shows that the proposed algorithm is (nearly) minimax optimal. The proposed approach can work with any planning solver to provide an ( ϵ+ϵopt )-optimal policy for any reward function. After reading the authors' feedback and discussing their concerns, the reviewers agree that the contributions in this paper are valuable and that this paper deserves publication. I encourage the authors to follow the reviewers' suggestions as they will prepare the camera-ready version.",0.8291871547698975,0.24214161642723614
Cons: The proposed method can improve the performance with large margins. The connection between (D) and (D) is not very clear. Pros: The proposed method can improve the performance with large margins. Cons: The proposed method can improve the performance with large margins. Pros: The proposed method can improve the performance with large margins.,"All reviewers believe that this paper is valuable, and the authors have made a significant, careful contribution. Some suggestions from the area chair: - ""in causality"" is not a standard technical term and also not non-technical idiomatic English, so it should be explained the first time it is used. - The authors should briefly cite and discuss research on so-called positive and unlabeled (PU) learning. This seems like the special case where there is exactly one known class and one novel class. The distinction between sampling in causality and labeling in causality appears in the PU literature, though not under this name. - The authors could also mention the obvious but surprising point that if data are generated by two clusters, then a classifier can be learned using exactly one labeled example--not even one from each class. - I have read the reference EJ A’Court Smith. Discovery of remains of plants and insects. _Nature_, 1874 and I fail to see its relevance. It is only one paragraph. Work from the 1800s should not be cited merely to suggest a veneer of scholarliness. - The writing uses italics for emphasis much too often.",0.8099455237388611,0.2184403294386963
"a couple of points: your decision to use SWAG, is interesting, especially from a computational perspective. a naive baseline here would be to do gradient descent-ascent/first order Lagrangian method; a fancier one would be SQP/interior point method.",The paper describes a new model-based RL technique for constrained MDPs based on Bayesian world models. It improves sample efficiency and safety. The reviewers are unanimous in their recommendation for acceptance. This represents an important advance in RL. Great work!,0.8365991115570068,0.2827450349926949
"Theorem 1 is novel, but empirical novelty seems limited theorem 1 is novel. consistency is not possible., the ELBO degrades the decoder.....................",Wide agreement from the reviewers. Interesting theorems. Empirical work illustrates the theory. Claim and insight: failure of VAEs is caused by the inherent limitations of ELBO learning with inflexible encoder distribution. Good discussion pointed out related work and insights from the experiments.,0.8475335240364075,0.304210065305233
"a number of existing techniques have been consolidated into one umbrella of ""hindsight information matching"" algorithms. the paper offers a version of the Categorical DT that includes an unsupervised learning component. the paper also offers a version of the Categorical DT that includes an unsupervised learning component.","The paper describes a framework that unifies several previous lines under hindsight information matching. Within that framework, the paper also describes variants of the decision transformer (DT) called categorical DT and unsupervised DT. The rebuttal was quite effective and the reviewers confirmed that their concerns are addressed. The revised version of the paper is significantly improved and consists of an important contribution that should interested many researchers. Well done!",0.8650628328323364,0.4400132478525241
"aaron carroll: the paper provides an excellent review of both the LFI and black-box sequence design literatures. carroll: ""this indicates that composite methods’ way of using parameterized models is not the optimal solution for small-scale tasks"" he says the paper provides an excellent review of both the LFI and black-box sequence design literatures. carroll: ""this paper provides an excellent review of both the LFI and black-box sequence design literatures""","The paper investigates various approaches, and a unifying framework, for sequence design. There were a variety of opinions about the paper. It was felt, after discussion, that the paper would benefit from a sharper focus, and somewhat suffers from being overwhelmed by various approaches, lacking a clear narrative. But overall all reviewers had a positive sentiment, and the paper makes a nice contribution to the growing body of work on protein design.",0.8408862948417664,0.4456542208790779
a. ''why does the expansion module need periodic blocks?''. ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''',"The paper proposed a novel deep learning model specifically designed for periodic time series forecasting problems. The approach includes lay-by-layer expansion, residual learning, and periodic parametrization. The model outperforms state-of-the-art baselines on several time series forecasting benchmarks. The reviewers appreciate the extensive experimental results, but also suggested improvement on writing and comparison regarding the parameter efficiency of the model.",0.7606196403503418,0.19212779061247903
the paper is not a new paper. the paper is not a new paper. the paper is not a new paper. The problem the authors focus on is significant in graph generative modeling community.......,"This paper provides an overview of evaluating graph generative models (GGMs). It systematically evaluates one of the more popular metrics, maximum mean discrepancy (MMD). It highlights some challenges and pitfalls for practitioners and suggests some ways to mitigate them. The reviewers found the paper practically relevant and several reviewers upgraded their scores through the discussion process. The authors acknowledged there are still some remaining issues regarding (i) considering other metrics & descriptor functions; ii) evaluating node/edge attributes and iii) addressing molecule generation. I am satisfied that these areas are beyond the scope of the current work and that the clarification improvements in the paper are adequate. It stands well enough on its own to accept in its present form.",0.8430607914924622,0.33848243632486885
the paper is well written and the figures are (mostly) of high quality. Could the authors clarify? a) I think the role of the normalizing flow is underexplained. b) I think the normalizing flow is underexplained. c) I think the normalizing flow is underexplained. d) I think the normalizing flow is underexplained. d) I think the normalizing flow is underexplained. e) I think the normal,"This paper presents a method for producing higher quality uncertainty estimates by mapping the predictions from an arbitrary (e. g. deep learning) model to an exponential family distribution. This is achieved by using the model to map from the inputs to a low-dimensional latent space and then using a normalizing flow to map to the parameters of the distribution. The authors show empirically that this improves over a variety of baselines on a number of OOD and uncertainty quantification tasks. This paper received 5 reviews who all agreed that the paper should be accepted (6, 6, 8, 8, 8). The reviewers in general found the method novel compared to existing literature, compelling and the results strong. Multiple reviewers asked for experiments with higher dimensional output distributions (e. g. CIFAR 100) and had concerns regarding the ""entropy regularization"" term (akin to the beta term in a beta VAE, this is a constant applied to the entropy term). The reviewers seemed satisfied with the author response, however, and the concensus decision is to accept.",0.7979469299316406,0.321257794541972
"ave baseline is not used for comparison, but it seems to assume leader can solve task. ave baseline is not used for comparison, but it is used for a MARL scenario. ave baseline is used for comparison, but it is not used for comparison.","This work proposed a method for encouraging an agent showing altruistic behaviour towards another agent (leader) without having access to the leader's reward function. The basic idea is based on the hypothesis that having the ability to reach many future states (i. e., called choice) is useful for the leader agent, no matter what it reward function is. The altruistic agent learns a policy that maximizes the choice of the leader agent. The paper defines three notions of choice, and evaluates them on four environments. The reviewers believe that this work attempts to solve an important problem, proposes a novel approach, and performs reasonably good experiments. The reviewers are all on the positive side at the end of the discussion phase. Therefore, I recommend acceptance of the paper. I also suggest a spotlight presentation for this work because of the novelty of the problem, which might be of interest to other researchers. The authors have already done some revisions to their paper (including adding a new environment). I encourage them to consider any remaining comments from reviewers in their final version.",0.8147164583206177,0.19126342433137875
The current work provides a novel alternative for the problem of coordination graph learning....... Weaknesses..........,"All reviewers found that the paper offers interesting contributions for multi-agent RL and favour acceptance of the paper. The strengths of the paper are summarized below: - Good algorithmic contribution - Offers a new set of benchmark tasks for coordination in MARL settings - Exhaustive experiments on complex tasks with a reasonable number of agents - All the issues raised by the reviewers (missing references, missing discussion of limitations...) have been satisfactorliy addressed. I therefore join the reviewers in the recommendation to accept the paper.",0.847816526889801,0.2596919387578964
"a good theoretical insight into one of the first types of DNN structures for sequence to sequence translation. I am not following this line of research closely. 5.1. E,D,O, are not defined. 5.2. E,D,O, are not defined. 5.1. E,D,O, are not defined. 5.2. E,D,O, are not defined. 5.2. E,","This paper presents a theoretical analysis of the approximation properties of linear recurrent encoder-decoder architectures, obtaining universal approximation results and subsequently showing approximation rates of targets for RNN encoder-decoders. It introduces a notion of ""temporal products,"" which helps to characterize the types of temporal relationships that can be efficiently learned in this setting. Overall, the reviewers and I all agree that this paper makes important theoretical contributions to the important problem of the approximation capabilities of encoder-decoder architectures. The main weaknesses involve the rather simplified linear problem setup, but this limitation is easily forgiven in this first-of-its-kind rigorous analysis. I recommend acceptance.",0.8235920071601868,0.2716558450212081
the results are not clear to me. the results are quite extensive and validate the claims made in the paper...............,"This is an intriguing work that introduces a novel sparse training technique. The core insight is a novel reparametrization or sparsity pattern based on the so-called butterfly matrices that enables fast training and good generalization. The theory is solid and useful. Most importantly, the method is novel and is likely to become impactful. Understanding better what contributes to the excellent performance is an interesting question for future work. In agreement with all the reviewers, it is my pleasure to accept the work.",0.8379124402999878,0.25607745349407196
I hope the authors address the additional comments in the final version.&noteId=PWzRo82xR07 and hopefully authors address the additional comments in the final version.....................................,"This paper proposes Adam and Momentum optimizers, where the optimizer state variables are quantized to 8bit using block dynamics quantization. These modifications significantly improve the memory requirements of training models with many parameters (mainly, NLP models). These are useful contributions which will enable training even larger models than possible today. All reviewers were positive.",0.8086148500442505,0.25745910964906216
"all would share the same null-space with the same metamers. a null-space. the paper is interesting, but I feel that it has a number of points that should be clarified....... presented. The presented. [   [ [  (see section 2.2) and well presented. analyzed. well presented. presented. presented. interpreted.","This paper shows that images synthesized to match adversarially robust representations are similar to original images to humans when viewed peripherally. This was not true for adversarially non-robust representations. Additionally the adversarially robust representations were similar to the texform model image from a model of human peripheral vision. Reviewers increased their score a lot during the rebuttal period as the authors provided more details on the experiments and agreed to tone down some of the claims (especially the strong claim that the robust representations capture peripheral computation similar to current SOA texture peripheral vision models). As well stated by reviewer s6dV, two representations with the same null-space are not necessarily the same. With reviewer scores of 8 across the board, reviewers agree that this is interesting work that should be presented at the conference. I agree.",0.8264066576957703,0.2913162720861373
"'i think the topic of the work is an interesting and valuable one,' says author. 'i think the method is quite efficient without too much overhead,' says author. 'i think the improvement brought by ODConv decreases as the model size increases,' says author.","This paper presents ODConv, a convolution pattern which uses attention in the convolutions across all dimensions of the weight tensor. The paper is well motivated and well explained, easy to follow. This work is built on top of previous work, but reviewers all agree that the contributions of this paper are significant. The experimental section is comprehensive, with several benchmarks, and show clear improvements. The reviewers suggested a few additional remarks, and discussions to add to the paper, which the authors have addressed in the rebuttal. Reviewers seem in general happy with the authors answers to their concerns. This seems like a sound and meaningful paper. I am fully in favour of acceptance, and I recommend this paper to be presented as a spotlight.",0.846831202507019,0.34635524824261665
"fusion strategy is arguably difficult to combine with multi-scale transformers, e. g. The proposed method accelerates recent ViT models by increasing throughput and decreasing MAC. the token reorganization seems to fuse discarded tokens into one to supplement attentive tokens.","The paper presents an approach to select visual tokens in images and reorganize them for the object classification, within Transformers. All four reviewers find the paper interesting and novel, and they are also very positive about the experimental results. The authors also addressed minor concerns of the reviewers successfully through the discussion phase, clarifying details and adding experiments. We recommend accepting the paper.",0.8336450457572937,0.24753748439252377
I wonder if it is due to a bad estimation of the initial state???... The paper is well-motivated and easy to follow. The paper is well-organized and easy to follow. The.,"This paper introduces a new technique for discovering closed-form functional forms (ordinary differential equations) that explain noisy observed trajectories x(t) where the ""label"" x'(t) = f(x(t), t) is not observed, but without trying to approximate it. The method first tries to approximate a smoother trajector x^hat(t), then relies on a variational formulation using a loss function over functionals {C_j}_j, defined in terms of an orthonormal basis {g_1, …, g_S} of sampling functions such that the sum of squares of all the C_j approximates the theoretical distance between f(x) and the solution f*(x). These sampling functions are typically chosen to be a basis of sine functions. The method is evaluated on several canonical ODEs (growth model, glycolitic oscillator, Lorenz chaotic attractor) and compared to gaussian processes-based differentiation, to spline-based differentiation, regularised differentiation, and applied to model the temporal effect of chemotherapy on tumor volume. Reviewers found that the paper was well-motivated and easy to follow (EBvJ), well evaluated (EBvJ), offering new perspectives to symbolic regression (79Ft). Reviewer vaG3 had their concerns addressed. Reviewer ZddY had concerns about the running time (a misunderstanding that was clarified) and the lack of comparison to a simple baseline consisting in double optimisation over f and x^hat(0) using Neural ODEs (the authors have added a Neural ODE baseline but were in disagreement with ZddY and 79Ft about their limitations). Reviewers engaged in a discussion with the authors, and the scores are 6, 6, 8, 8. I believe that the paper definitely meets the conference acceptance bar and would advocate for its inclusion as a spotlight in the conference.",0.8093737363815308,0.33570365814699066
The model is quite novel.. The model is quite novel. The model is quite general and can be applied to other graph structures. Strengths The model is quite novel. Strengths validity. Strengths.. ..,"This paper proposes a spanning tree-based graph generation framework for molecular graph generation, which is an interesting problem. The tree-based approach is efficient and relatively effective in molecular graph generation tasks, and the empirical results are convincing. There were some concerns during the initial reviews, but all of them have been addressed during the discussion phase. Thus, I recommend this work be accepted.",0.8328940868377686,0.2966996811862503
the paper proposes a series of principled improvements to state-of-the-art MCTS planning algorithms. the proposed improvements are particularly effective in low simulation budget settings. the paper is well written and I enjoyed reading it. primary Strengths: Several interesting adjustments of how the MCTS operates in AlphaZero/MuZero.,"The paper presents improvements to AlphaZero and MuZero for settings where one is restricted in the number of rollouts. The initial response from reviewers was generally favorable but the reviewers wanted more details and clarifications of multiple parts of the paper, and further intuition about the Gumbel distribution. The authors’ responses were detailed and convinced or maintained strong positive support of most reviewers. The authors also stated that they plan to provide a release of the code and also provided a policy improvement proof. Overall this is an interesting approach that is likely to be of significant interest to many.",0.8618319034576416,0.4032450534403324
"The paper concludes that ""Bel or ConfTr do not necessarily recover the accuracy of the baseline.......... Strengths. The paper. Weaknesses The paper addresses an important problem and the associated technical area has seen a surge in publications including a workshop at ICML 2021.","In this paper, a new learning scheme for minimizing the confidence set by conformal prediction is proposed. Most of the reviewers agree that the idea is interesting and novel. This is an important contribution to trustworthy ML, with theoretically sound considerations and thorough experimental validation.",0.8452792763710022,0.3021810588737329
"the paper is question answering. and the paper is question answering. the paper is a conceptually simple and clean paper with extensive results. The tasks considered are extensive, covering many areas of NLP tasks such as QA, NLI, sentiment classification, summarization etc................","This paper studies constructing text2text transformer models that are good at zero-shot task generalization via multi-task learning over a diverse set of NLP tasks. One main contribution of the work is to create prompt templates for various NLP tasks (that are of different task formats) such that all tasks can be framed into text2text learning format and that is ""natural"" to the pretrained T6 model. The paper conducts extensive experiments to demonstrate the promising zero-shot generalization ability of such multi-task learner. Strength: - Important problem setup that has broad applications - Extensive experiments to validate the claims - Useful resources are developed for the problem Weakness: - Good to study the effect of using different combination of training tasks on the downstream zero-shot generalization, which can shed some light on the usefulness of upstream tasks - Justification of ""true zero shot learning"" capability would require further experiments on analyzing the data overlap between MTL datasets (and also T5 pertaining task data) and the unseen task data. - Some more discussion on the task split and categorization will be helpful.",0.8252618312835693,0.4170609779655933
"i. Pros: Interesting use of forward-mode differentiation and structure of the gradients. i. Cons: IMO, the claims/explanations for continuous-time and forward-mode differentiation should be separated. i. Cons: i. The main motivation for continuous-time seems to be being able to learn T.","This paper addresses a continuous-time formulation of gradient-based meta-learning (COMLN) where the adaptation is the solution of a differential equations. In general, outer loop optimization requires backpropagating over trajectories involving gradient updates in the inner loop optimization. It is claimed that one of main advantages of COMLN is able to compute the exact meta-gradients in a memory-efficient way, regardless of the length of adaptation trajectory. To this end, the forward-mode differentiation is used, with exploiting the Jacobian matrix decomposition. All the reviewers agree that the derivation of memory-efficient forward-mode differentiation is a significant contribution in the few-shot learning. The paper is well written and has interesting contributions. Authors did a good job in responding to reviewers’ comments during the discussion period. What is missing in this paper is the discussion of some limitations of the proposed method. This can be improved in the final version. All reviewers agree to champion this paper. Congratulations on a nice work.",0.839129626750946,0.3087218457034656
"Observations: f is symmetric. g., x2 ) is a ""distorted"" version of the other (e. g., x1 ). Weaknesses: For Observation 2: Dp(x1,x2)x2x12 is symmetric. g., x2 ) is a ""distorted"" version of the other (e. g., x1 ).","All reviewers suggest acceptance of this paper, which reports the relationship between perceptual distances, data distributions, and contemporary unsupervised machine learning methods. I believe this paper will be of broad interest to different communities at ICLR.",0.8020754456520081,0.18022480172415573
"g., Vidal et. paper introduces a new algorithm for a significant problem. if this is accurate, it will have meaningful impact on many other problems in signal processing. in its current state the paper is not yet ready for acceptance. g., Vidal et. paper introduces a new algorithm for a significant problem.","The paper looks at subspace recovery in the presence of outliers, of which there have been many formulations. They study a recent formulation, DPCP, but relax the requirement that the dimension of the subspace is known -- obviously very important in practice. The approach is quite clever: they exploit the fact that for this non-convex problem, starting a simple algorithm at a randomly chosen starting point will converge to a local minimizer, and they can run an ensemble of these algorithms (each with different starting points) and be guaranteed the solutions will span an appropriate subspace. This idea alone is a nice contribution. The paper has theory and experiments. Most reviewers were positive about the paper. The most critical review, by 1qf1, still acknowledged that this paper has a lot of potential, but in their opinion the paper was not in a state ready for acceptance, especially regarding the formulation of the main result, Theorem 7. The other reviewers were OK with the state of the paper, and the authors made changes in the rebuttal. Hence, while acknowledging the paper could possibly still be improved (what paper couldn't be!), I think the paper is in a good enough state to accept it for ICLR. I don't think there would be enough benefit to the community (authors, readers and reviewers) to ask for this to go through one more round of submission/revision.",0.8332006335258484,0.37253168150782584
a. d. r. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s. s,This paper studies the memorization power of Relu Neural networks and obtains sharp bounds in terms of parameters. The writing is very clear and the results very interesting.,0.8089308738708496,0.11147998106714926
Several novel contributions. a. a. a. a. a. that the programmatic reinforcement learning is a promising method. Motivation a promising approach. that the a promising approach. a promising approach. Motivation Motivation that the proposed method yields better interpretability compared to deep RL policies. e. The paper is well written.....  that the proposed method is,"This paper presents an approach to synthesize programmatic policies, utilizing a continuous relaxation of program semantics and a parameterization of the full program derivation tree, to make it possible to learn both the program parameters and program structures jointly using policy gradient without the need to imitate an oracle. The parameterization of the full program derivation tree that can represent all programs up to a certain depth is interesting and novel. In its current form this won’t scale to large programs that require large tree depth, but is a promising first step in this direction. The learned programmatic policies are more structured and interpretable, and also demonstrated competitive performance against other commonly used RL algorithms. During the reviewing process the authors have actively engaged in the interaction with the reviewers and addressed all the concerns, and all reviewers unanimously recommend acceptance.",0.8243432641029358,0.28721361288002556
"eigenvalues of the Hessian of models blcoks and loss landscape. improves accuracy on clean and corrupted test set. the paper is simple and straightforward. the paper is simple and straightforward., , Ascoli et al., 2021), or.","### Description The paper demonstrates that efficient architectures such as transformers and MLP-mixers, which do not utilize translational equivariance in the design, when regularized with SAM (sharpness aware minimization) can achieve same or better performance as convolutional networks, in the vision problems where the convolutional networks were traditionally superior (with data augmentation or not, regularized or not). The paper demonstrates it very thoroughly through many experiments and analysis of the loss surfaces. ### Decision + Discussion I find the paper to be very timely in its context. It has a remarkable coverage of experimental studies and different use cases: SAM + augmentation, +contrastive, +adversarial, +transfer learning; as well as ablation studies such as keeping first layers convolutional. The reviewers have asked further questions, and the authors were able to conduct respective experiments within the discussion period fully addressing all concerns and making the findings of the paper even more comprehensive and convincing. After the rebuttal 3 reviewers were for ""accept"", one ""marginally above"" and one ""marginally below"". In the latter case the concern was that the paper is an experimental study of a known method, SAM. While I understand that many researchers are expecting theoretical and innovative results from ICLR papers, I find that it does not prevent acceptance. Indeed, the experimental findings in this paper are on a ""hot"" topic, could be of wide interest and could lead to a change of paradigm in designing models towards more generic ones. On the other hand, it could just indicate that CNNs are not fully exploiting their potential, e. g. not exploiting the context well enough in the hidden layers? To get more insight, I am still wondering, how the predictions behave if the input is shifted by a few pixels in CNN and Transformers? It seems counterintuitive that making the first layers in ViT just an MLP of image patches is a good design. Furthermore, fully convolutional models allow to take input of an arbitrary size and average the predictions on the output if it happened to be larger than 1x1. Since convolutions are also used for e. g. semantic segmentation and generative models, one should not (and the authors do not in the paper) discard them too fast. See also a recent work combining transformers and convolutional networks, Chen et al. (ICCV 2021) Visformer: The Vision-friendly Transformer.",0.7992910742759705,0.3267016439802117
hyperparameter optimization (HPO) tasks....................................................,"This paper presents a novel methodology for performing meta learning for gradient-based hyperparameter optimization. The approach overcomes limitations (scaling, e. g.) of previous methods through distilling the gradients of the hyperparameters. The paper received 4 reviews, of which all were positive (6, 6, 8, 8). The reviewers appreciated the technical clarity of the paper and found the proposed approach sensible, novel, technically sophisticated and effective. The main concerns were regarding the comprehensiveness of the experiments and technical presentation of the dataset distillation. It seems that the reviewers found the author response (lots of results were added) satisfactory regarding these points. Thus the recommendation is to accept.",0.8046466708183289,0.353632973972708
"the are of interest...?? -Proposition 2, should n be used instead of m? -Proposition 2, should n be used instead of m? -Proposition 2, should n be used instead of m? -Proposition 2, should n be used instead of m?",The authors extend the result of Ongie et al. (2019) and derive sparseneural network approximation bounds that refine previous results. The reuslts are quite ineteresting and relevant to ICLR. All the reviewers were positive about this paper.,0.794342041015625,0.13204412907361984
"cs. i have below minor concerns/comments related to experimental results. delta_t is critical hyper-parameter in discretization ODEs. if delta_t is too small, the value of partial gradient (i.e. vanishing/exploding gradient) might be slow.",The paper proposes a new recurrent architecture based on discretization of ODEs which allow for learning multi-scale representations and help with the vanishing gradient problem. The reviewers all agree this architecture is novel and provide substantial theoretical and empirical evidence. A strong accept.,0.8546046614646912,0.2576439658800761
"""A data-driven health monitoring method for satellite housekeeping data."" Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. ""A data-driven health monitoring method for satellite housekeeping data."" IEEE Transactions on Aerospace and Electronic Systems 53.3 (2017): 1384-1401. Strengths: What's the convergence property of the min-max strategy?","The paper proposed a novel approach that leverages the discrepancies between the (global) series association and the (local) prior association for detecting anomalies in time series. The authors provided detailed empirical support to motivate the above detection criterion, and introduced a two-branch attention architecture for modeling the discrepancies and establishing an anomaly score. All reviewers acknowledge the technical novelty of this work (including the key insight of modeling anomalousness with Transformer’s self-attention and concrete training mechanism via a minimax optimization process) as well as the comprehensiveness of the empirical study. Meanwhile, there were some concerns in the positioning of the work, in particular in the clarity in connection to related work, and some reviews concern the clarity of the presentation (e. g. missing some details in experimental results), and the clarity of the exposition of the training process. The authors provided effective feedback during the discussion phase, which helped clarify many of the above concerns. All reviewers agree that the revision makes a solid paper and unanimously recommend acceptance of this work. The authors are strongly encouraged to take into account the feedback from the discussion phase to further improve the clarity concerning the technical details as well as the reproducibility of the results.",0.7975632548332214,0.2204458974301815
Strengths Strengths Strengths the performance of the proposed method is impressive. the method is already sufficient for generating realistic images. The paper is mostly focused on the variance preserving diffusion process.....,"This paper presents a faster sampling method for diffusion based generative models which are usually slow in practice. The key idea is based a progressive distillation approach (e. g., how to distill a 4 step sampler into a 1 step sampler). The paper studies the various design choices for diffusion models which existing work hasn't looked at that deeply and sheds light on the effects of these choices. The paper also shows that DDIM can be seen as a numerical integrator for probability flow ODE. The experimental results are impressive. There were some concerns such as the effect of progressive distillation and the overhead of distilling the diffusion model but the authors provided a satisfactory response and backed it up with additional results. Overall, this is a nice paper on making diffusion based generative models generate faster samples and also provides novel insights into the behavior of these models under various design choices. Given the significant recent interest in these models which are pretty impressive in terms of generation quality but slow, the paper indeed makes a timely contribution which will fuel further interest in these models. All the reviewers have voted for acceptance. Based on my own reading, the reviewers' assessments, the discussions, and the authors' response, I would vote for acceptance.",0.825234591960907,0.3903245846882011
RR and SO can be n times faster than gradient descent as long as 2 is small and the loss functions are strongly convex. RR and SO can be n times faster than gradient descent as long as 2 is small and the loss functions are strongly convex.,"This paper studies the dependency of SGD convergence on order of examples. The main observation of the paper is: if the averages of consecutive stochastic gradients converge faster to the full gradient, then the SGD with the corresponding sampling strategy will have a faster convergence rate. For different sampling strategies, sampling with replacement has slower convergence in stochastic gradient, where sampling without replacement can converge faster. The paper also proposes two new algorithms that can improve convergence rates in some interesting settings. The reviewers find the analysis clean and the new algorithms are interesting. There is some concern on the dependency on n or d for the faster rate, which should be discussed more clearly in the final version of the paper. Overall this is a solid contribution to the example selection problem.",0.8156089782714844,0.4278601620878492
aaron ramirez: paper shows stronger results than prior work with more practical implications. ramirez: why and when SGD (or any given learning algorithm) is well-calibrated remains unclear. he says paper is well written and easy to follow.,"This article introduces an interesting variant of the work of Nakkiran & Bansal (2020). It shows empirically that the test error of deep models can be approximated from the disagreement on the unlabelled test data between two different trainings on the same data. The authors then show theoretically that a calibration property can explain such behaviour, and they report experiments showing that the relationship does exist in practical situations. All reviewers agree on the practical and theoretical value of the article, which is very well organised and written. The ideas developed here are likely to lead to further work in the future, and they clearly deserve to be published at ICLR. I agree with one of the reviewers that the title is somewhat misleading, as the reader expects an analysis based on SGD. The title could be shortened to ""Assessing Generalization via Disagreement"", and the experimental restriction to SGD could be mentioned in the abstract.",0.8305216431617737,0.2962960916615668
"The paper is well-written and easy to follow. The paper is well-written and easy to follow. The paper is well-written and easy to follow. The paper is well-written, the method section has some important details missing. g.......................","This paper proposes an alternative approach to epsilon-greedy exploration by instead generating multi-step plans from an RNN, and then stochastically determining whether to continue with the plan or re-plan. The reviewers agreed that this idea is novel and interesting, that the paper is well-written, and that the evaluations are convincing, showing large improvements over epsilon-greedy exploration and more consistently strong performance than other baselines. While the original reviews contained some questions around discussion of related work and the simplicity of the evaluation environments, the reviewers felt these concerns were adequately addressed by the rebuttal. I agree the paper explores a very interesting idea and convincingly demonstrates its potential, and should be of wide interest to the deep RL community especially as it touches on many different subfields of RL: MBRL/planning, exploration, HRL, navigation, etc. I recommend acceptance as a spotlight presentation.",0.833733081817627,0.34845433235168455
"g. The paper provides the theoretic interpretation of their uncertainty penalization. The paper doesn't present a result comparing the value of the learnt policy against the optimal policy under more assumptions (e. Lee et al., Batch Reinforcement Learning with Hyperparameter Gradients, ICML 2020 Pros and Cons Cons: The proposed method outperforms various offline RL algorithms including model-based algorithms.","This paper makes significant advances in offline reinforcement learning by proposing a new approach of being pessimistic to deal with uncertainties in the offline data. The proposed approach uses bootstrapped Q-functions to quantify the uncertainty, which by itself is not new, and introduces additional data based on the pseudo-target that is penalized by the uncertainty quantification. The use of such additional data is the first of a kind, and the paper provides theoretical support for the case of linear MDP and empirical support with the D4RL benchmark. The reviewers had originally raised concerns or confusions regarding theoretical analysis and experiments. The authors have well responded to them, and no major concerns remain.",0.8396298885345459,0.34545424915850165
"DS(S)-GNNs are a very powerful graph representation learning method. it is a very simple method that can be used for graphs. but the proposed method shows improvements, but the improvements are rather small. it is a bit difficult to compare the proposed method to the proposed one.","Improving the expressiveness of GNN is an important problem in the current graph learning community. Its key idea is to generate subgraphs from the original graph, then encode the subgraphs into the message passing process of GNN. The proposed method is proven to be strictly more powerful than 1-WL. The authors also quantize how design choices such as the subgraph selection policy and equivariant neural architecture can affect the architecture’s expressive power. After the rebuttal, all reviewers are glad to accept this submission. During the discussion, while reviewer B3oK has shown some concerns on the concurrent works in NeurIPS 2021, it should not affect the decision of the submission once the authors have discussed them in the main text. The authors have done this in their revision, thus an acceptance (spotlight) is suggested.",0.8514200448989868,0.4296139068901539
The paper is generally hard to read and understand. Cons: Paper is generally hard to read and understand. The organization of the paper isn't well organized. is generally hard to read and understand. The paper is very well reasoned and draws interesting insights. The paper proposes a new architecture in which the last conversation block is replaced with multi-head self-attention.,"The paper presents an empirical analysis of Vision Transformers - and in particular multi-headed self-attention - and ConvNets, with a focus on optimization-related properties (loss landscape, Hessian eigenvalues). The paper shows that both classes of models have their strengths and weaknesses and proposes a hybrid model that takes the best of both worlds and demonstrates good empirical performance. Reviewers are mostly very positive about the paper. Main pro is that analysis is important and this paper does a thorough job at it and draws some useful insights. There are several smaller issues with the presentation and the details of the content, but the authors did a good job addressing these in their responses. Overall, it's a good paper on an important topic and I recommend acceptance.",0.8496884107589722,0.48336024085680646
the paper is clear from a method-recipe perspective. the main weakness of the proposed approach is that it takes the form of combining multiple existing techniques into one coherent approach. The paper is clear from a method-recipe perspective. The paper is clear from a method-recipe perspective.,"The authors propose a well-presented approach to likelihood-free inference. The reviewers are all in alignment in recommending this paper for acceptance. There was a healthy discussion between authors and reviewers, where the authors have already incorporated many of their recommendations. The potential for this methodology to be applied to situations with expensive simulators should be intriguing to a broad audience. As a result, I recommend for this paper to be accepted as a spotlight.",0.8428907990455627,0.32307605594396593
"a trilemma. a non-saturating GAN............ a clean idea...... deterministic vs. deterministic vs.. on CIFAR-10,....","The paper modifies DPMs by replacing the denoising L2 losses with GANs to learn the iterative denoising process. This leads to excellent results using a small number of refinement steps. In some sense, this also takes away one of the key advantages of DPMs over GANs, which is DPMs minimize a well-defined objective function. Nevertheless, the results are convincing, but not spectacular. I am not convinced that we should continue to report training FID on CIFAR-10. I would have like to see class-conditional ImageNet results. Also, it is not clear whether the proposed technique provides additional gains on top of SoTA GANs. Overall, I recommend acceptance as a spotlight.",0.8267613053321838,0.2559125768020749
"paper aims to tackle the problem of modeling a continuous time dynamic system, using a category of neural networks. it introduces an alternative view to the neural ODE, by reducing the non-linear, vector-valued optimal control problem to a system of forward-facing initial value problems. despite the experimental section, it seems to be a good paper.","The authors presents an alternative view of Neural ODEs, offering a novel understanding of depth in neural networks. The reviewers were overall impressed by the novelty and potential for insights this work brings. There was some disappointment that the empirical results were not stronger (both in terms of pure performance and computational cost) and that it wasn't clear how the theoretical insights into depth actually translated into a practical insight. Nevertheless, I agree with the reviewers that this is a good submission and would I think make for an interesting addition to the conference programme.",0.8552754521369934,0.4002141940097014
"a gi simulation would be nice, but I'm not sure how it works. a gi simulation would be interesting, but I'm not sure how it works. a gi simulation would be useful, but I'm not sure how it works.","The paper analyzes the learning behavior of deep networks inside RL algorithms, and proposes an interesting hypothesis: that many of the observed difficulties in deep RL methods stem from _capacity loss_ of the trained network (that is, the network loses the ability to adapt quickly to fit new functions). As the paper points out, some of these difficulties have popularly been attributed to other causes (such as difficulties in exploration) or to less-specific causes (such as reward sparsity: the paper proposes that capacity loss mediates observed problems due to sparsity). The paper investigates its hypothesis two ways: first by attempting to measure how capacity varies over time during training of existing deep RL methods, and second by proposing a new regularizer to attempt to preserve capacity. These experiments are set up well, and their results are convincing &mdash; while there is likely no perfect way to measure or preserve capacity, the methods chosen here make sense. This is a strong paper: it proposes a creative, appealing, and interesting hypothesis about an important problem (difficulties in training deep RL methods), and conducts a well-designed evaluation. We expect and hope that it will inspire interesting follow-on work. We thank the authors for their thorough and helpful participation in the discussion period, including updates to improve the clarity of the paper.",0.8121177554130554,0.2437678687274456
.. The paper is very well-written and easy to follow. The paper is very well-written and easy to follow... The paper is very well-written and easy to follow.....,"The paper aims to solve the source-free domain adaptation, specifically on measurement shift. The proposed method lowers the domain gap via restoring the source feature distribution with a lightweight approximation. The effectiveness and performance are well validated by extensive experiments on various datasets compared with other recent methods, and the ablation analysis supports the claim of the paper well. The paper is well written with clear logic to follow.",0.8515118956565857,0.4643898862414062
kNN method proved effective in solving sentence similarity tasks. but the experiments are limited and the paper needs to be improved. the authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks. the paper could reach a much wider audience (which I think would be an improvement) if it provided more intuitive paraphrases of the main results in Section 2.3.,"This paper presents a novel framing of what's at stake when selecting/segmenting text for use in language model pretraining. Four reviewers with experience working with these models agreed that the conceptual and theoretical work here is insightful and worth sharing. The empirical work is fairly small-scale and does not yet support broad conclusions, but reviewers did not see such conclusions as necessary for the paper to be valuable.",0.8407890796661377,0.36347255731622374
I think the paper is a good contribution I think the paper is a good contribution I think the paper is a good contribution..... The paper is quite clear and well-written... Analysis of training tasks is interesting and important..... ( (i.  |C| (task complexity) and |C| (task complexity)..,This manuscript expands the range of recent work in reinforcement learning for language games to much larger and more realistic datasets. A timely and relevant contribution and one that is well evaluated. Further work in stabilizing RL approaches for such large-scale problems is likely to have other far-reaching consequences. Reviewers were unanimous that this is a strong submission after the author discussion period.,0.8386816382408142,0.32328150048851967
Pros: paper is clearly written and well motivated the approach is novel empirical results are convincing and very solid authors provide code and instructions to recreate datasets for reproducibility. Cons: Please provide bullet point list of contributions. org/pdf/1906.11892. pdf. Strengths of the paper include the following: (1) The problem addressed is an important and understudied one. (2) The paper does not provide any theoretical guarantees about worst-case subclass performance.,"This work presents an approach to learning good representations for few-shot learning when supervision is provided at the super-class level and is otherwise missing at the sub-class level. After some discussion with the authors, all reviewers are supportive of this work being accepted. Two reviewers were even supportive of this work being presented at least as a spotlight. The approach presented is well motivated, experiments demonstrate its value and include a nice application in the medical domain, making the work stand out relatively to most work in few-shot classification. Therefore, I'm happy to recommend this work be accepted and receive a spotlight presentation.",0.8258221745491028,0.29895275719463826
"the proposed method borrows ideas from L-BFGS and Broyden’s method, which appears natural choices. The authors establish various convergence results showing that the approximate hypergradients converge to the true hypergradients, under different sets of assumptions. Experimentally, the proposed method is comparable to or outperforms the Jacobian-Free method by Fung et al.","The paper considers the setting of bi-level optimization and proposes a quasi-Newton scheme to reduce the cost of Jacobian inversion, which is the main bottleneck of bi-level optimization methods. The paper proves that the proposed scheme correctly estimates the true implicit gradient. The theoretical results are supported by numerical experiments, which are encouraging and show that the proposed method is either competitive with or outperforms the Jacobian Free method recently proposed in the literature. Even though the reviews expressed some initial concerns regarding the empirical performance of the proposed method, the authors adequately addressed those concerns and provided additional experiments. Thus, a consensus was reached that the paper should be accepted.",0.8658915162086487,0.48142194946606953
"Weaknesses: Definition 2.1 could be improved by requiring K to be on the same order of magnitude as the lower bound. Strengths: The paper is well motivated and timely, it considers the problem of minimizing cycles of interaction of an agent with the environment which still learning near-optimal policies.","The authors’ present a precise definition of deployment efficient RL, where each new update of the policy may be costly, and theoretically analyze this for finite-horizon linear MDPs. The authors include an information-theoretic lower bound for the number of deployments required. The reviewers found this an important setting of interest and appreciated the theoretical contributions. The authors’ carefully addressed the raised points and also addressed questions about deployment complexity and sample complexity in their revised work. One weakness of the paper is that it does not provide empirical results and the linear MDP assumption, while quite popular in theoretical RL over the last few years, is quite restrictive. However,the paper still provides a very interesting theoretical contribution for an important topic and I recommend acceptance.",0.8394502997398376,0.3552119036515554
The paper is well-written and comprehensive. are interesting and give more practical insight on the performances of some models. I keep my score. I keep my score..........,"This paper provides generalization bounds for meta-learning based on a notion of task-relatedness. The result is natural and interesting--intuitively, when tasks are similar, then meta-learning algorithms should be able to utilize all data points across all tasks. The theoretical contribution is novel, and the results also provide more practical insight into the performances of some models.",0.8673996329307556,0.39272521839787566
"it does have a number of advantages over recent-published competitors. the results are not huge, either. the results are not enormous, either. the convergence issues are not just theoretical........................",This paper proposes a theoretically sound and practically effective method to compress quantized gradients and reduce communication in distributed optimization. The method is interesting and worth publication.,0.8403598666191101,0.23548197373747826
"The experiments show that the proposed tildeI(w;S) correlates with the generalization gap, and helps improving the performance. It would strengthen the paper a lot if the paper discuss and perform experiments to show if the assumptions are valid. The main weaknesses of this paper are language and clarity.","This paper revisits the information bottleneck principle, but in terms of the compression inherent in the weights of a neural network, rather than the representation. This gives the resulting IB principle a PAC-Bayes flavor. The key contribution is a generalization bound based on optimizing the objective dictated by this principle, which is then tractably approximated and experimentally verified. Reviews raise concerns about assumptions made to achieve the tractable version and a public discussion debates whether this is truly a PAC-Bayes bound. The authors address these adequately. Another concern is whether improvements in experiments can be ascribed to the new objective. Authors add new experiments in support of this. Additional concerns about the clarity of certain aspects of the paper were or were promised to be addressed by the authors. Overall, the perspective of this paper, its technical contributions, and experimental evaluations appear to be worthwhile to share with the community, as they advance the applicability of the information bottleneck principle.",0.8452033996582031,0.46371925042735207
centered clipping has certain robustness in the non-iid setting. resampling to reduce heterogeneity across workers could be interesting. -Are there more complicated examples of non-iid models that perform poorly?. -Are there more complicated examples of non-iid models under non-adversarial attacks that perform poorly?.,"This manuscript proposes and analyses a bucketing method for Byzantine-robustness in non-iid federated learning. The manuscript shows how existing Byzantine-robust methods suffer vulnerabilities when the devices are non-iid, and describe a simple coordinated attack that defeats many existing defenses. In response, the primary algorithmic contribution is a bucketing approach that aggregates subgroups of devices before robust aggregation. This approach is also easily composed with existing Byzantine-robust methods. The manuscript includes an analysis of the performance of the proposed approach, including an information-theoretic lower bound for certain settings. During the review, the main concerns are related to the clarity of the technical contributions, and unclear technical statements. The authors respond to these concerns and have satisfied the reviewers. After discussion, reviewers are generally strongly positive about the strength of the manuscript contributions. The authors are reminded to make the final changes agreed in the public discussion e. g., discussion of the reduction to SGD when δ=0",0.8281305432319641,0.25275160633027555
MHSA: a phonetic-linguistic division model. MHSA: a phonetic-linguistic division model. MHSA: a phonetic-linguistic division model. MHSA: a phonetic-linguistic division model. MHSA: a phonetic-linguistic division model. MHSA: a phonetic-linguistic division model. MHSA: a phonetic-linguistic division model. MHSA: a phonetic-linguistic division model.,"The paper conducted a thorough experimental analysis of the attention map in the Conformer models for CTC based speech recognition models and connected it with phonetic and linguistic information in the speech. Using these insights, the paper presented some computation improvement and marginal quality gains. The authors actively conducted additional experiments to further justify the claims. The paper is strong in terms of the systematic way of in-depth analysis and further development (i. e. sharing the attention map across layers for speedup). But as pointed out by the reviewers, it lacks some comparisons with other alternatives to justify the importance of sharing attention maps in reducing computations. Also it would be better if there's justifications on how the observations generalize to other types of models (such as LAS, RNN-T). The decision is mainly because of the thorough analysis conducted in the paper which can be a good contribution to the community.",0.8020169138908386,0.190059094896747
"the authors try to shed light on some design choices. the authors try to shed light on some design choices. the paper is generally well written but I suggest to revise the organization since some parts are introduced after they are effectively used. Especially, I am bothered by the comparison to the ""direct regression"" baseline. g. 2018 or Behera et al. g. 2018 or Behera et al.","The paper focused on deep regression problems and proposed a label encoding technique which can be thought as a sibling of the famous error-correcting output codes but designed for regression problems. The main idea is well illustrated in Figure 1 at the top of page 3, where the encoder and decoder are the main objects of the proposal (and a quantizer is also needed for using the encoder/decoder which is a uniform quantizer in the paper). The idea/proposal is supported by solid theoretical arguments and convincing empirical evidences (not only the paper but also the rebuttal). While there were some concerns in the beginning, the authors have successfully clarified all the concerns and then the average score has been increased from 5.5 to 7.5. As a result, the paper is clearly above the bar of acceptance. What is more, an advantage is that the proposed method is task-agnostic and can be combined with different task-specific feature extractors borrowed from very complex regression problems (e. g., head pose estimation, facial landmark detection, age estimation, and end-to-end autonomous driving), making its significance and potential impact high. Given these facts, I think the paper might be selected as a spotlight presentation at the conference.",0.8202935457229614,0.277801270596683
The model obtains excellent results on a variety of datasets showing its generality. The expose of the attention mechanism and update layer lacks detail. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g. g,"The paper proposes a rotationally equivariant transformer architecture for predicting molecular properties. The proposed architecture demonstrates good computational efficiency and good results on three benchmarks. All four reviewers recommend acceptance (two weak, two strong), citing the novelty of the architecture, the good computational efficiency of the model and the good empirical results as the main strengths of the paper. The reviewers expressed minor criticisms and recommendations for improvement, some of which were addressed by the authors during the reviewing process, which led to an increase in scores. Overall, this is a nice contribution of machine learning to science, and I'm happy to recommend acceptance to ICLR.",0.826605498790741,0.18565912612810218
"my overall assessment varies drastically along these two axes: 1. the paper is interesting, but the overall narrative is encumbered by vague and confusing statements. 1. the paper is misleading. For a>0, convergence to 0 is the desired behavior, so coloring that region as ""problematic"" is not appropriate. 2. the paper is a good read, but the paper is a bit too broad in its claims for the ICLR.","Overall, the paper provides interesting counter examples for the SGD with constant step-size (that relies on a relative noise model that diminishes at the critical points), which provide critical (counter) insights into what we consider as good convergence metrics, such as expected norm of the gradient. The initial submission took a controversial position between the mathematical statements and the presentation of the statements on the behavior of the SGD method in non-convex optimization problems. While the mathematical is sufficient for acceptance at ICLR, the presentation was inferring conclusions that could have been misread by the community. I am really happy to state that the review as well as the rebuttal processes helped improved the presentation of the results that I am excited to recommend acceptance.",0.8438618183135986,0.29403966585440294
I find the evaluation unfair to local SGD. I also find the convergence analysis a bit limiting (since the mixing matrix really depends on the setting). Strengths: The paper adds a new dimension to the federated learning setup by combining local and decentralized SGD approaches.,"The paper contributes to the literature on federated learning by introducing a hybrid local SGD (HL-SGD) method. HL-SGD is motivated by the setups where edge devices are grouped into clusters with fast connections within the cluster, but slower connection between the devices and the server. HL-SGD uses hybrid updates: decentralized updates within the clusters and federated averaging steps between the clusters and the server. Initially, the reviews expressed concerns regarding comparison to prior work, empirical results, and privacy of the proposed scheme. However, the authors adequately addressed all of the concerns, added relevant discussions and results to the paper, and a consensus was reached that the paper should be accepted.",0.8526008129119873,0.42258586088816324
The proposed approach is a good one. The proposed approach is not effective against attackers that have access to data from a similar distribution to the training one (e.. ). Strengths: Interesting idea of applying proof-of-work to machine learning as an entry barrier.,the paper proposed a novel idea of requiring users to complete a proof-of-work before they can read the model's prediction to prevent model extraction attacks. Reviewers were excited about the paper and ideas. Some misunderstanding raised by reviewers were sufficiently clarified by the authors in the rebuttal.,0.8589126467704773,0.24850630698104698
authors propose a method to extend the proposed method to higher-order bases. authors should provide an ablation study to address this. paper makes a connection between message passing neural networks and finite element methods. authors hope to use the proposed method in a future paper.,"This paper introduces a graph neural network (GNN) based on the finite element method (FEM) for learning partial differential equations from data. The proposed finite element network is based on a piecewise linear function approximation and a message passing GNN for dynamics' prediction. The authors also propose a method to incorporate inductive bias when learning the dynamical model, e. g. including a convection component. The paper received three clear accept and one weak accept recommendations. The reviewers discussed the possible extensions of the method, and also raise several concerns regarding experiments, e. g. the added value of a synthetic dataset, implementation tricks or hyper-parameter settings. The rebuttal did a good job in answering reviewers' concerns: after rebuttal, there was a consensus among reviewers to accept the paper. The AC's own readings confirmed the reviewers' recommendations. The paper is well written and introduces solid contribution at the frontier of GNNs and finite elements methods, especially a pioneer graph-based model for spatio-temporal forecasting derived from FEM. Therefore, the AC recommends acceptance.",0.8429532051086426,0.334437104085317
"The paper is a relatively straightforward extension of the prior work GCA to include latent embeddings and to use signed distance fields. Weaknesses: Lack of experiments on real-world datasets, which could potentially break the proposed method. cGCA is a relatively straightforward extension of the prior work GCA to include latent embeddings and to use signed distance fields.","This paper introduced a probabilistic extension to a pipeline for 3D scene geometry reconstruction from large-scale point clouds. All reviewers recognized the significance of the proposed approach and praised the simplicity of deriving a probabilistic version of Generative Cellular Automata that performs well in a number of reconstruction benchmarks. Authors were responsive during rebuttal and managed to clarify the concerns raised about the limited scope of the experiments and certain parameters involved, and also raise one reviewer's scores.",0.839031994342804,0.3951336642106374
"priacy.. The authors seem understand both FL and FR in depth, the proposed solution can meet the requirements of differential priacy. (1) The authors do not compare with any existing methods. (2) 'That prevents the FL approach from broadcasting the whole model amongclients and the central server.' The experimental validation is weak.","This paper received 4 quality reviews. The rebuttal and discussions were effective. All reviewers raised their ratings after the rebuttal. It finally received 3 ratings of 8, and 1 rating of 5. The AC concurs with the contributions made by this work and recommend acceptance.",0.8276663422584534,0.19669714458286763
authors present thorough analyses of standard feedforward networks. they quantitatively demonstrate that such networks can model the visual stream well. this is highly relevant to allowing for such networks to be more plausibly accepted by the neuroscience community. WC+CT predicts neural responses with more than 50% accuracy.,"This paper experiments with what is required for a deep neural network to be similar to the visual activity in the ventral stream (as judged by the brainscore benchmark). The authors have several interesting contributions, such as showing that a small number of supervised updates are required to predict most of the variance in the brain activity, or that models with randomized synaptic weights can also predict a significant portion of the variance. These different points serve to better connect deep learning to important questions in neuroscience and the presence of the paper at ICLR would create good questions. The discussion between authors and reviewers resulted in a unanimous vote for acceptance, and the authors already made clarifications to the paper. I recommend acceptance.",0.850770115852356,0.37896963134407996
"I believe that an example based on learned dynamics model would make the paper much better. Pros The paper clearly identifies an issue with the prior work (EPIC) and proposes an appropriate solution. Weaknesses: Compared to EPIC, DARD needs access to the transition model T when computing the reward function. As a result the distance between reward function become more aligned with the trained policies scores.","The paper proposes a new pseudometric, DARD, for comparing reward functions that avoid policy optimization. DARD builds on a recent work by Gleave et al. 2020 where the pseudometric EPIC was proposed. In contrast to EPIC, DARD operates on an approximate transition model and evaluates reward functions only on transitions close to their training distribution. Empirical experiments in different domains demonstrate the effectiveness of the proposed pseudometric. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers' questions and increased their overall assessment of the paper. At the end of the discussion phase, there was a clear consensus that the paper should be accepted. The reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper.",0.858595073223114,0.40295185810989803
I am not sure what the SG score adds to the discussion. Cons The discussion on previous stack RNNs was a strong segment of the paper as the authors connect the various approaches using common notation. Cons The discussion on previous stack RNNs was a strong segment of the paper as the authors connect the various approaches using common notation.,"This paper advances the long running thread of sequence modelling research focussed on differentiable instantiations of stack based models. In particular it builds upon recent work on the Nondeterministic Stack RNN (NS-RNN) by introducing three extensions. The first is to relax the need for a normalised distribution over the state and action distribution and allow unnormalised weights, this mostly serves to facilitate gradient flow and thus easier training. The second extension allows the RNN to condition on the top stack state as well as the symbol, improving expressiveness. The third improvement introduces a method for limiting the memory required to run the proposed model on long sequences, thus allowing its application to practical language modelling tasks. Each of these requires substantial algorithmic innovations. The reviewers all agree that this is a strong paper worthy of publication. The paper includes a useful review of previous differentiable stack models which nicely sets up the rest of the paper where the contributions are well motivated and clearly presented. The reviewers had a number of clarification questions, partly due to the author's use of overly concise citations for key algorithms rather than inline descriptions. This situation has been improved by updates made to the paper. The evaluation includes a series of synthetic experiments which are clear and provide a good elucidation of the various stack models properties. The practical evaluation on language modelling is more limited and serves mostly to demonstrate that the nondeterministic model can be scaled to a basic language modelling task. Overall this is a strong paper with a well motivated and clear hypothesis. It provides a substantial extension to the prior work on nondeterministic stack models and progresses this line of research toward practical applications.",0.8308641910552979,0.3625184077592123
Weaknesses: The writing can be improved. The paper is very complete and introduces all the different notions needed to understand the theory of the proposed methods. Strengths: The paper is very complete and introduces all the different notions needed to understand the theory of the proposed methods.,The paper proposes to extend mirror descent to sampling with stein operator when the density is defined on a constrained domain and non euclidean geometry. All reviewers agreed on the novelty and the merits of the paper. Accept,0.8365223407745361,0.30134827333192027
a concise description of the games and their sources of stochasticity is missing. a deterministic model of the game of Go could be used to train a model. a model of the game could be used to train a model of the game's deterministic behavior.,"The paper extends MuZero to stochastic (but observable) MDPs. To represent stochastic dynamics, it splits transitions into two parts: a deterministic transition to an afterstate (incorporating all observations and actions up to the current time), followed by a stochastic outcome (accounting for new randomness that follows the last action). The transition to an afterstate is similar in spirit to ordinary MuZero's dynamics model; the stochastic outcome is learned by a VQ-VAE. At planning time, MuZero retains the MCTS lookahead from ordinary MuZero. Stochastic MuZero achieves impressive results: e. g., it maintains the original MuZero's strong performance on the deterministic game of Go, while improving on MuZero significantly (and achieving superhuman performance) on the stochastic game of backgammon. This is a strong paper overall: it presents a convincing and successful extension of the already-influential MuZero work, along with large-scale computational experiments confirming the utility of the approach. There are nonetheless a few weaknesses: first, compared to the original AlphaZero and MuZero work, it is perhaps less surprising that the given approach is successful, since it is more closely related to prior work. Second, due to the large-scale computational infrastructure needed, it is only possible to run some of the experiments once. This is not in itself a problem, but care needs to be taken in interpreting the results of such single-run experiments: e. g., any figures that show results of single-run experiments should have a clear warning label, and any statements such as ""stochastic MuZero performs better than original MuZero"" should be tempered with a caveat about how reliable these conclusions are likely to be. Section 5.4 (which runs shorter experiments using three random seeds each) makes a start at evaluating reliability, but (a) the headline results in previous sections do not contain any caveats or pointers to 5.4, and (b) 5.4 should explicitly acknowledge that it cannot hope to detect even quite-common failure cases with so few seeds.",0.8217213749885559,0.3692270188281933
"the paper tackles an important problem and proposes an intuitive approach for alleviating the problem. the paper is well written and well structured. g., focused on gradient projection operations [Yu et al., 2020], which, while similar in motivation, may lack the flexibility of the proposed approach's learnable rotations.","The paper addresses the problem of inconsistent gradients in multi-task learning, proposing ways to handle both their magnitude nd direction. Gradient directions are aligned by introducing a rotation layer between the shared backbone and task-specific branches. Reviewers appreciated the technical approach, higlighting the novelty of the rotation layers in this context. The empirical evaluations are systematic fair and insightful, and the presentation is polished. Reviewers unanimously supported accepting the paper.",0.8510817289352417,0.38928414980570475
"a detailed analysis of the proposed strategy. ###Clarity### This paper enhances transferability of vision transformers (ViT) by introducing two strategies specific to the architecture of ViT models. Specifically, Self-Ensemble finds multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. The proposed Token refinement can potentially enhance the discriminative capacity at each block of ViT. The paper conducts extensive experiments on multiple datasets, settings, and vision","In this paper, the authors enhance the adversarial transferability of vision transformers by introducing two novel strategies specific to the architecture of ViT models: Self-Ensemble and Token Refinement method. Comprehensive experiments on various models (including CNN's and ViT's variants) and tasks (classification, detection, and segmentation) successfully verify the effectiveness of the proposed method. In general, the problem studied is relevant and important. The paper is well-written and well-motivated with empirical findings. The proposed two strategies are novel, simple to implement, and effective in practice. Following the author's response and discussion, the average score increases from 6 to 7.5, with most concerns well addressed. AC believes that the paper should be highlighted at the ICLR conference.",0.8679842352867126,0.4885481210691588
#################################################################################################################################################################################################################### Pros: The results seem,"The paper demonstrates that test error of image classification models can be accurately estimated using samples generated by a GAN. Surprisingly, this relatively simple proposed method outperforms existing approaches including ones from recent competitions. All reviewers agree this is a very interesting finding, even though theoretical analysis is lacking. Given the importance of the problem of predicting generalization, I recommend acceptance.",0.7472116947174072,0.2808269318193197
E. focuses on local attention to depth-wise convolution. I think the single above fact is valuable and enough for acceptance. I am curious about the performance of larger models with the ImageNet 22k dataset. E. The authors pointed out that this work focuses on local attention.,"All three reviewers recommend acceptance. The paper introduces an interesting study and insights on the connection between local attention and dynamic depth-wise convolution, in terms of sparse connectivity, weight sharing, and dynamic weight. The reviews included questions such as the novelty over [Cordonnier et al 2020] and the connection to Multi-scale vision longformer, which were adequately addressed by the authors. The findings in this paper should be interesting to the ICLR community.",0.8625956773757935,0.34393567964434624
"I would expect to understand how your theory differs from previous works and introduce fewer ""approximations"" Weaknesses: The major weakness of this work is while you clearly explain how your theory differs from previous works, you keep coming back to a line in your introduction, ""the limitation of these approximations is not well understood"".","All the reviewers think that the work is significant and new. Therefore, they support the paper to be published at ICLR 2022. Given the strong results and the “accept” consensus from the reviewers, I accept the paper as “spotlight”. The authors should implement all the reviewers’ suggestions into the final version.",0.833670973777771,0.2950971554964781
"a. Sharma, s., levine, s., &. Hausman, K. (2019). ""Exploration by Maximizing R'enyi Entropy for Reward-Free RL Framework"" Weaknesses: Discussion of limitations is only included in the supplementary material.","This paper tackles the problem of exploration using intrinsic rewards in RL in states that have never been encountered before. The authors derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement, which estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples. The intrinsic reward resulting from the so-called DISDAIN (discriminator disagreement intrinsic reward) exploration bonus is more tailored to the true objective compared to pseudocount-based methods. Reviewers agree that the paper is well-motivated and well-written, that the proposed DISDAIN exploration method is simple and practical, and that experiments are convincing. Experiments on continuous control tasks such as MuJoCo locomotion environments could have strengthened the paper further.",0.7988001704216003,0.2145804304629564
"Weakness: It needs a lot of background knowledge about PSR, MARL and graph-based approaches to POMDP. Strength: The mathematical derivation appears sound and theoretical guarantee is given. Weakness: It needs a lot of background knowledge about system models and graph-based approaches to POMDP.","This paper presents an extension of the Predictive State Representation (PSRs) to multi-agent systems, with a dynamic interaction graph represents each agent’s predictive state based on its “neighborhood” agents. Three types of agent networks are considered: static complete graphs (all agents affect all others experience); static non-complete graphs (only some agents affect one another); and dynamic non-complete graphs (agents affect one another in a time varying way). A number of theoretical results are presented, including PAC bounds for the approximations in the framework. The paper also contains a number of experiments that clearly show the advantages of the proposed technique over some related methods. The reviewers unanimously agree that this is a strong paper, with a solid theoretical and empirical analysis.",0.8294536471366882,0.34786322712898254
"Weakness: The paper does not include results on evaluating the support alignment explicitly. Binkowski 2019: Batch weight for domain adaptation with mass shift Strengths: Overall a great paper. Very easy to follow, the algorithm seem theoretically grounded and simple and very well motivated. Binkowski 2019: Batch weight for domain adaptation with mass shift Strengths: Overall a great paper.","Thanks for your submission to ICLR. Three of the four reviewers are ultimately (particularly after discussion) very enthusiastic about the paper, and feel that their concerns have been adequately addressed. The fourth reviewer has not updated his/her score but has indicated that their concerns were at least somewhat addressed. I took a look at their review and agree that the authors have addressed these concerns sufficiently. I am happy to recommend this paper for acceptance at this point. Note that I really appreciate the time and effort that the authors went into adding additional results and clarifications for the reviewers.",0.8210136294364929,0.3070157251010338
Reference: kbn.org/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/gnn/,"The paper presents a novel method of fusion of information from two modalities: text (context and question) and a Knowledge Base, for the task of question answering. The proposed method looks quite simple and clear, while the results show strong gains against baseline methods on 3 different datasets. Ablation studies show that the model achieves good performance on more complex questions. While the reviewers raise some concerns, e. g., on the sensitivity of the proposed method, the technical novelty against prior works, they see values in this paper in general. And the authors did a good job in their rebuttal. After several rounds of interactions, some reviewers were convinced to raise their scores by a little bit. As a result, we think the paper is in a good shape and ICLR audience should be interested in it.",0.7599871158599854,0.1516608966048807
the paper is well-written and well-organized. The paper is well-organized. The paper is well-organized. The paper is well-written and well-organized. The paper is well-organized. The paper is well-written and well-organized. The paper is well-organized.,"This paper addresses an important issue of AutoML systems, specifically their ability to ""cold start"" on a new problem. Some of the reviewers initially had concerns about the experimental validation and the theoretical foundations of the method, but during the discussion phase the authors addressed concerns extremely well. The authors already included most of the feedback of reviewers, further strengthening the paper.",0.8343566060066223,0.32222286079611095
"paper proposes a solid architecture by considering the requirements of the problem. paper's experiments and results are extensive, covering 3 different tasks. paper's proposed method is very fast compared to others. paper's main comment left on my side is 3) the limited novelty compared to prior work.",This paper studies the problem of motion prediction for multiple agents in a scene using transformer-based VAE like architecture. The paper received mixed reviews initially which generally tended towards borderline acceptance. All reviews appreciated extensive experiments but had some clarifications and requests for ablations. The authors provided a strong rebuttal that addressed many of the reviewers' concerns. The paper was discussed and all the reviewers updated their reviews in the post-rebuttal phase. Reviewers unanimously agree that the paper should be accepted. AC agrees with the reviewers and suggests strong acceptance. The authors are urged to incorporate reviewers' comments in the camera-ready.,0.8377814292907715,0.3497043754905462
False summing up of the paper's main arguments for the paper's theoretical analysis. False summing up of the paper's main arguments for the paper's theoretical analysis. summing up of the paper's main arguments for the paper's theoretical analysis.,"All reviewers agreed that this is a strong paper, that the methodological contributions are both relevant and significant, and that the experimental validation is convincing. I fully share this viewpoint!",0.841905951499939,0.34762089078625047
"aaron carroll: this paper tackles an interesting and useful question of introspecting and analyzing different inductive biases. carroll: this paper presents an important analysis which could have useful downstream applications. he says it is a good work, but it is not a good one for a general purpose. carroll: this paper presents an important analysis which could have useful downstream applications.","This paper examined physics-inspired inductive biases in neural networks, in particular Hamiltonian and Lagrangian dynamics. The work separated the benefits arising from incorporating energy conservation, the symplectic bias, the coordinate systems, and second-order dynamics. Through a set of experiments, the paper showed the most important factor for improved performance in the test domains was the second-order dynamics, and not the more common explanation of energy conservation or the other factors. The increased generality of this approach was demonstrated with better predictions on Mujoco tasks that did not conserve energy. All reviewers liked the insights provided by the paper. They agreed that the paper clearly laid out several hypotheses and systematically tested them. The reviewers found the experiments thoughtful and the results compelling. The reviewers also pointed out several aspects of the document that could be improved, including additional formalism clarifications (reviewer nLbj), baseline algorithms (reviewer wu5x), and domains (reviewers 7KKB,SW9u). The reviewers found the author's response satisfactory but were disappointed that a revised paper was not ready to read. The reviewers want the final paper to include the modifications that were promised in the author response. All four reviewers indicated to accept this paper which contributes novel insights that simplify and generalize physics-inspired neural networks. The paper is therefore accepted.",0.8220598101615906,0.3419354086120924
"k-NN module is densely triggered for each position, which seems quite heavy. Pros: There are some benchmarks that require models to be able to handle long inputs. k-NN module is not a powerful tool for LM, but it is a useful tool for LM.","This paper studies the problem of dealing with long contexts within a Transformer architecture. The key contribution is a kNN memory module that works in concert with a Transformer by integrating upper layers with additional retrieved context. The idea is simple but the execution is good. While the idea is reminiscent of other recent work on this topic, and novelty is somewhat borderline, it is practically useful. Overall, though ambivalent, my recommendation is that the paper should probably be accepted",0.8541943430900574,0.37573929131031036
SSAC is a well-written and motivated paper. it is well-written and motivated. but the paper is weaker than many previous papers. a strong predicator is needed to improve the clustering. the paper is well-written and motivated.,"One might assume that the k-means problem has already been beaten to death, but this paper shows there are still remaining questions. And rather interesting ones at that, with a novel angle of having additional help from a prediction algorithm of cluster memberships. This connects to learning-augmented algorithms research. The reviewers agreed that the problem is interesting and gives a novel angle, and the interestingness stems from novelty, and the ability to ""escape"" from NP-hardness. The reviewers and authors had nice discussions about details and conclusions, on how limiting is it that the authors focus on reasonably accurate predictors, for instance, and where could the predictors come from. This is a good paper, and hopefully the discussion helped make it even better.",0.8383169770240784,0.41572353641192117
a specific EBM is exhibited whose partition function is uncomputable. this paper rigorously explores the computational barriers of calculating the partition function and model selection. a glaring exception (for me) is the 2nd paragraph of Section 2.4 where the reduction from neural sequence to formal languages is presented.,"This is a deep theoretical paper with results that I consider very interesting. I have *not* had time to check them myself, but I have background in these theoretical matters and the results seem reasonable to me - the hardness of even checking the quality of a solution is well known for partition functions (as well as hardness of any reasonable approximation), but the undecidability seems new - I assume it comes naturally and it is a very interesting result - I have seen similar decidability issues for #P: general probabilistic polynomial time Turing machines (it is unclear if a connection was sought here). Reviewers are all positive about the content, and authors have acknowledge some points for improvement.",0.8254870176315308,0.3253829628229141
Perceiver IO is a generic architecture capable of handling general-purpose inputs and outputs across different tasks and modalities. the authors presented a thorough study of Perceiver IO across different application scenarios. Pros: This paper introduced a generic architecture for coping with different tasks with various input and output lengths.,"This paper proposes Perceiver IO, a general neural architecture that handles general purpose inputs and outputs. It operates directly in the raw input domains, and thus does away with modality specific architecture components. The paper contains extensive experiments showing the capabilities of this architecture in different domains. The paper received very positive reviews from all reviewers. Some concerns included a need for additional details such as a missing task from GLUE, FLOPs comparisons to past works, nomenclature for the versions of Perceiver IO, etc. These concerns were well addressed by the authors. Others concerns by reviewers were the lack of experiments in a multi task setting. However, it was acknowledged by the authors and reviewers that this is an open area of research and is a good fit for future work. Given this high quality submission, strong reviews and a very positive discussion amongst authors and reviewers, I recommend accepting this paper.",0.8708920478820801,0.5350615647104051
Fig 3: 5% and 25% of the data used by Singh et al................................................,"The paper proposes an interesting hypothesis about deep nets' generalization behavior inside RL methods: it suggests that the nets' implicit regularization favors a particular form of degeneracy, in which there is excessive aliasing of state-action pairs that tend to co-occur. It proposes a new regularizer to mitigate this problem. It evaluates the hypothesis and the regularizer empirically, and it provides suggestive derivations to motivate both. The reviewers praised the comprehensive empirical analysis, the insights into learning, and the combination of empirical and theoretical evidence. The authors participated responsively and helpfully in the discussion period, and addressed any concerns raised by the reviewers. This is a strong paper: it derives and motivates a novel hypothesis about an important problem, and analyzes this hypothesis both mathematically and experimentally.",0.7945069670677185,0.1541093047708273
"a new, musically relevant evaluation metric taking instruments into consideration. the proposed model was not evaluated with datasets with vocal which is the most essential sound source in popular music. the authors clearly demonstrate the case of AMT having relatively low resources. a new, musically relevant evaluation metric taking instruments into consideration.","This work concerns Automatic Music Transcription (AMT) -- transcribing notes given the audio of the music. The paper demonstrates that a single general-purpose transformer model can perform AMT for many instruments across several different transcription datasets. The method represents the first unified AMT model that can transcribe music audio with an arbitrary number of instruments. All reviewers rated this paper highly and are excited about seeing it at the conference. One reviewer noted that ""This paper seems to be a great milestone in the AMT research. It is probably the first unified AMT model that can take music audio with an arbitrary number of instruments."" The reviewers had some suggestions and comments, which appear to be addressed by the authors.",0.8329475522041321,0.41438851257165277
"section 4 is clear and concise. section 4 is clear and concise. in the main part it says ""This suggests that it may be easier to adapt EBBS models"".....................................","The paper addresses a problem encountered in many real-world applications, i. e. the treatment of tabular data, composed of heterogeneous feature types, where samples are not i. i. d. In this case, learning is more effective if the typically successful approach for i. i. d. data (boosted decision trees + committee techniques) is combined with GNN to take into account the dependencies between samples. The main contribution of the paper with respect to previous work in the field is the introduction of a principled approach to pursue such integration. One important component of the proposed approach is played by the definition of a specific bi-level loss (efficient bilevel boosted smoothing) that allows for convergence guarantees under mild assumptions. Both theoretical and experimental contributions are sound and convincing, justifying the claimed merits of the proposed approach. Another strong point is the fact that the proposed approach is general and amenable to support a broad family of propagation rules. One weakness with the original submission was presentation, mainly because some key information was confined into the supplementary material. The revised version addressed this problem and added some more empirical results that confirmed the superiority of the proposed approach. Finally, the fact that learning over tabular graph data is very important in Industry, the proposed approach may be of interest for a wide audience.",0.81809401512146,0.24566143626968065
"Despite the paper's strengths, it is a little dated and lacks formalism. author points out that more involved architecture might perform better.. Author points out that more involved architecture might perform better.......","This work combines steerable MLPs with equivariant message passing layers to form Steerable E(3) Equivariant GNNs (SEGNNs). It extends previous work such as Schnet and EGNNs, by allowing equivariant tensor messages (in contrast to scalar or vector messages). The paper also provides a unifying view of related work which is a nice overview for the ML community. It is overall well written, but would benefit from further revision to improve readability in some parts (in particular section 3, cf. reviews). It shows strong empirical results on the IS2RE task of the OC20 dataset and mixed results on the QM9 dataset.",0.8206934928894043,0.3126581894854705
"... if g(cos(theta)) is... range of [-0.2, 1], we can end up with cos(theta)... In the end, how is the verification threshold determined?... In p.8 l. 3, 93.73% (93.87% in Table 2); in p.8 l. 3. The paper is well written with nice presentation and clear description. The paper is well written with nice presentation and clear description.",All reviewers agree that the proposed SphereFace2 approach - training face recognition models by using multiple binary classification losses - is interesting and innovative. The reviewers agree that the paper is well written and are satisfied with the presented experimental study. The rebuttal addressed all additionally raised questions. I believe that the paper will be of interest to the audience attending ICLR and would recommend a presentation of the work as a spotlight.,0.8208929896354675,0.3093929284562667
"CertifyADP is a good tool for determining certifications, but the comparison needs to be fair. CertifyADP is not fair, but it is important to understand table 3.. The paper can be significantly improved (especially section 5). The authors proposed Adaptive Sampling and K-consensus algorithms to reduce the computational cost. The authors proposed Adaptive Sampling and K-consensus algorithms to reduce the computational cost.","This paper integrates model ensembles with randomized smoothing to improve the certified accuracy. The methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. Moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. Extensive experiments were conducted on CIFAR10 and ImageNet datasets. The strengths of the paper are as follows: + In terms of significance of the topic, the problem tackled in the paper is significant and highly relevant. + The motivation of using model ensemble is clearly illustrated via a figure and well justified with theoretical analysis. + Algorithmically, the paper proposes Adaptive Sampling and K-consensus algorithms to reduce the computational cost, making the method more practical. + Experimentally, the paper exhibits competitive results against several frameworks for training smooth classifiers and on several datasets.",0.8451727628707886,0.4786321086809039
........................................................,"Four reviewers have evaluated this submission with one score 6 and three scores 8. Overall, reviewers like the work and note that *a rigorous and principled approach is taken by this work*. AC agrees and advocates an accept.",0.761806845664978,0.03509143119057019
MetaLink is able to leverage the correlations among tasks successfully. the authors should highlight somehow that the edges show the conceptual connections between nodes (and not the real link in the algorithm flow). the writing could have been more precise in some parts of the paper. the paper was a bit clunky and the paper was a bit rushed.,"The paper describes a novel learning scenario where there are many related tasks, some seen at test time, and some seen only at training time, where additionally the task labels can be hidden or present. This approach generalizes both a ""relational setting"" (where auxiliary task labels could be used as features) and a ""meta setting"" (where new tasks need to be solved in a zero-shot setting using data from related tasks only). The idea behind the method is to do MTL with a common representation and a set of task-specific heads, and build a graph where (1) tasks are nodes associated with the parameters of their task-specific ""heads"" and (2) edges link examples to tasks with known labels. A GNN method is then used to find regularities in the graph. Pros - The setting is innovative and the approach is novel - The experimental results are strong Cons - Some of the terminology seems awkward and/or strained (eg ""knowledge graph"" for the task-example graph)",0.8324252367019653,0.30781499817967417
"aaron ramirez: paper relies heavily on readers previous knowledge. ramirez: paper is a novel combination of existing methods. he says the paper is clear and clear, but the results are not a guarantee. ramirez: paper is a good example of a novel combination of existing methods.","This paper introduces a new transformer architecture for representation learning in RL. The key ingredients of the proposed architectures are a novel combination of existing methods: (1) the use of LSTMs to reduce the need for large transformers and (2) a contrastive learning procedure that doesn't require human data augmentation. The resulting approach requires less prior knowledge and provides higher sample efficiency. The paper is convincing, with comprehensive experiments on multiple challenging and well-known benchmarks and an ablation study. The reviewers did expressed concerns that parts of the paper are a very difficult read and could use improvement, especially those relying on substantial external background. The intuition behind several components could be improved, and there are some clarity issues, as detailed in the individual reviews.",0.8412001132965088,0.4029906280338764
"the authors' attempt of reformulating bivariate causal discovery problem is a novel and inspiring endeavor. the authors' presentation is concise and clear, and the paper is easy to follow. one potential avenue for improvement is the motivation of the present reframing. the authors could also consider moving the related work section to the end of the introduction to the field of optimal transport.","This paper is a solid contribution to researchers in this field, as it provides a new idea for the basic problem of determining the direction of causality between two variables, using the functional causal model as a dynamical system and optimal transport.",0.8555322289466858,0.4279397465288639
False characterization of the results of the study. a re-structured paper would have helped to include more training details. a re-analysis of the results would have helped to improve the paper. the paper is well-written and easy to follow.,"This manuscript proposes and analyses can approach to address the centralized and personalized tasks in federated learning jointly. Existing work has tackled this issue by developing separate tasks. Instead, this manuscript proposes a shared architecture that aims to optimize centralized and personalized models. One observation motivating this work is that local models trained during federated learning effectively optimize local task performance. The resulting approach results well when label shifts primarily drive the client variability. Here, the centralized components are trained to optimize a balanced risk, while the local components are trained to optimize the standard empirical risk. Reviewers agree that the manuscript is well-written and appropriately addresses the timely issue posed. The main concerns are the clarity of the technical contributions and technical statements during the review. The authors respond to these concerns and have satisfied the reviewers. After discussion, most reviewers are generally strongly positive about the strength of the manuscript contributions.",0.8340192437171936,0.28813379295170305
a more detailed discussion of this limitation is preferred in the main paper as opposed to the appendix. the paper is generally well written and easy to follow. the proposed VaGraM loss is designed to specifically remedy the two presented issues in the original VAML loss. the proposed loss is an MLE weighted by sensitivity of value function to states.,"This paper studies model-based RL in the setting where the model can be misspecified. In this case, MLE of model parameters is a not necessarily a good idea because the error in the model estimate compounds when the model is used for planning. The authors solve this problem by optimizing a novel objective, which takes the quality of the next state prediction into account. This paper studies an important problem and this was recognized by all reviewers. Its initial reviews were positive and improved to 8, 8, 6, and 6 after the rebuttal. The rebuttal was comprehensive and exemplary. For instance, one concern of this paper was limited empirical evaluation. The authors added 5 new benchmarks and also included stabilizing improvements in their original baselines. I strongly support acceptance of this paper.",0.8375794887542725,0.38014083479841554
a. s. d. et al. : a series of experiments to study the role of input representations in language-modeling. b. s. d. et al. : a. d. et al. : a. d. et al. : a. d. et al. : a. d. et al. : a. d. et,"The authors address a very important question pertaining to the relevance of morphological complexity in the ability of transformer based conditional language models. Through extensive (controlled) experiments using 6 languages they answer as well as raise very interesting questions about the role of morphology/segmentation/vocab size which mat spawn more work in this area. All the reviewers were positive about the paper and agreed that the paper made significant contributions which would be useful to the community. More importantly, the authors and reviewers engaged in meaningful and insightful discussions through the discussion phase. The authors did a thorough job of addressing all reviewer concerns and changing the draft of the paper accordingly. I have no hesitation in recommending that this paper should be accepted.",0.8187089562416077,0.19991470407694578
"a sensitivity analysis would be included to test the proposed algorithm. the paper is a useful and apparently novel algorithm for an important problem in the application of RL methods in real-world settings. the paper is very clear and well written, and the explanation of the algorithm is easy to follow.","The authors introduce a method for improving reinforcement learning in sparse reward settings. In particular, they propose to take advantage of a suboptimal behavior policy as a guidance policy that is incorporated in a TRPO-like update. The reviewers agree that this is a novel and interesting idea and given the authors' rebuttal with additional experiments, clarifications and discussions, they agreed to accept the paper. However, they also point out several flaws (e. g. evaluation on a more challenging sparse-reward task such as Adroid) that I encourage the authors to address in the final version of the paper.",0.8578164577484131,0.35164460834736627
"aaron carroll: my concerns with this paper are in regards to the experimental design. carroll: the strength of the conclusions, and how the authors are choosing to interpret them. he says the authors are focusing on the EC and natural languages. carroll: the authors are focusing on the EC and natural languages.","This paper explores ways in which *emergent communication* (EC) methods from representation learning can be evaluated extrinsically, by hooking them into downstream NLP tasks. Reviewers agree that the paper is thorough, and finds encouraging results. This paper is borderline, and difficult to evaluate, even after very substantial discussion (some of it private). From my reading of the reviews and pieces of the paper, I'm very sympathetic to wvqW's concern that none of the present-day applications under study seem likely to benefit from this kind of emergent communication pretraining: *Natural* language pretraining, even transferring across natural languages, is for too strong a baseline, and it's not even conceptually clear how one could substantially outperform that baseline. I'm very concerned that the results in this paper will be—misleadingly—cited as proof that EC research is already contributing to downstream progress in NLP. However, the narrow claims in the paper itself seem to be sound, and two confident reviewers whom I trust argue strongly that the ideas results here are surprising and novel, and that the paper could be the starting point for productive discussion and future work in this area. I'm recommending spotlight presentation in the hope that the paper will provoke a nuanced discussion in that setting.",0.8258642554283142,0.3752801625856331
The a novel research direction with improved performance over state-of-the-art approaches.. (1) is not clear and the multiplication of X by E_phi is confusing. (2). The paper contributes non-trivial advancements over state-of-the-art approaches. Weak points: Not many here.,"The authors introduce the Time-Aware Multiperistence Spatio-Supra Graph CN that uses multiparameter persistence to capture the latent time dependencies in spatio-temporal data. This is a novel and experimentally well-supported work. The novelty is achieved by combining research in topological analysis (multipersistence) and neural networks. Technically sound. Clear presentation and extensive experimental section. Reviewers were uniformly positive, agreeing that the approach was interesting and well-motivated, and the experiments convincing. Some concerns that were raised were successfully addressed by the authors and revised in the manuscript. Happy to recommend acceptance. A veru nice paper!",0.8238521218299866,0.29789109180370965
"The paper provides various pre-trained BERT models with many checkpoints. The paper provides multi-BERTs, a set of 25 model checkpoints, and the Multi-Bootstrap, a non-parametric method to estimate the model uncertainty. The paper is an empirical analysis paper, which is useful for the community but the computational cost seems very high.",This paper is a resource and numerical investigation into the variability of BERT checkpoints. It also provides a bootstrap method for making investigations on the checkpoints. All reviewers appreciate this contribution that can be expected to be used by the NLP community.,0.8776308298110962,0.5021623969078064
(4) detailed details in empirical study and design of experiment.. I think the paper has a clarity issues and still in its early stages from that perspective. The METHOD section is very vague and not clearly explained. Notation are introduced without definition and no explanation for the background needed to understand the notation.,"This paper proposes a message passing neural network to solve PDEs. The paper has sound motivation, clear methodology, and extensive empirical study. However, on the other hand, some reviewers also raised their concerns, especially regarding the lack of clear notations and sufficient discussions on the difference between the proposed method and previous works. Furthermore, there is no ablation study and the generalization to multiple spatial resolution is not clearly explained. The authors did a very good job during the rebuttal period: many concerns/doubts/questions from the reviewers were successfully addressed and additional experiments have been performed to support the authors' answers. As a result, several reviewers decided to raise their scores, and the overall assessment on the paper turned to be quite positive.",0.8501148223876953,0.3643751454850038
XTX can bring significant improvements over Go-Explore in Zork1 but not other games. The switching policy is the element that looks most hardcoded. The main contribution is an exploration strategy with an in-episode switch from an exploitation policy to one aimed at exploration.,"I thank the authors for their submission and active participation in the discussions. All reviewers are unanimously leaning towards acceptance of this paper. Reviewers in particular liked that the paper is well-written and easy to follow [186e,TAdH,Exgo], well motivated [TAdH], interesting [PsKh], novel [186e] and provides gains over baselines [186e,TAdH,PsKh] with interesting ablations [186e,Exgo]. I thus recommend accepting the paper and I encourage the authors to further improve their paper based on the reviewer feedback.",0.8045262694358826,0.1570690929268797
"the paper is well-written and easy to follow. Weaknesses: This paper is just focusing on the task of image recognition. the effect of large-scale pre-training for more structured tasks like semantic segmentation, detection, etc. would be a lot more relevant.","This paper provides a very large-scale study on the pretraining of image recognition models. Specifically, three scaling factors (model sizes, dataset sizes, and training time) are extensively investigated. One important phenomenon observed by this paper is that stronger upstream accuracy may not necessarily contribute to stronger performance on downstream tasks---actually sometimes these two types of performance could even be at odds with each other Overall, all the reviewers enjoy reading this paper and highly appreciate the empirical results presented in this paper. There were only a few concerns raised by the reviewers but most were well addressed during the discussion period. All reviewers reach a consensus on accepting this paper and believe this study is worthy to be heard by the community.",0.8520716428756714,0.40152047500014304
the paper is technically well presented. it would be good to provide more examples to demonstrate the general theory. - The paper is hard to follow at many places since only informal theorems are provided throughout the paper. - The paper is hard to follow at many places since only informal theorems are provided throughout the paper.,"The paper studies an interesting question of whether neural networks can approximate the target function while keep the output in the constraint set. The constraint set is quite natural for e. g. multi-class classification, where the output has to stay on on the probability manifold. The challenge here is that traditional universal approximation theory only guarantees that f^(x)≈f(x) , but can not guarantee that f^(x) lies exactly in the same constraint set as f(x) . The paper made a significant contribution in the theory of deep learning -- It is shown that the neural network can indeed approximate any regular functions while keep the output stay in the regular constraint set. This gives a solid backup in terms of the representation power of neural networks in practice, to represent target functions whose outputs are in certain constraint set (e. g. probabilities).",0.8068716526031494,0.26095305590165985
"Pros: The proposed function of model capacity for encoder-decoder NMT model performance is useful for parameter allocation of NMT models training. Cons: The entire evaluation is not reproducible. Almost the entire evaluation is not reproducible. the paper uses undisclosed datasets, use of non-standard BLEU implementation, pre-processing of datasets not detailed.","This is a strong empirical paper that studies scaling laws for NMT in terms of several new aspects, such as the model quality as a function of the encoder and decoder sizes, and how the composition of data affects scaling, etc. The extensive empricial results offer new insights to the questions and provide valuable guidance for future research on deep NMT. The datasets used in the study are non-public, which may make it hard to reproduce the evaluation.",0.8632222414016724,0.46779151509205497
the paper is proposing an interesting method for an important problem. the paper is very well presented and gives enough context to the reader. the paper is very well written and the presentation is very well. the paper is very well presented and the authors explain the rationale and the theory behind it.,"The authors present a method called ""AdaRL"" that learns a structured latent representation that characterizes relationships between different variables in an RL system. The method is evaluated on modified Pong and Cart-Pole domains and it is shown to outperform other transfer learning baselines. The reviewers agree that the method makes sense and addresses an important problem of transfer in RL. The authors did a good job in the rebuttal to empirically validate their claims and provided extra experiments. The reviewers also point out that the evaluated domains are rather simple and the paper would benefit from evaluations in a more complex environment as well as better writing. Please focus on improving these aspects in the final version of the paper.",0.8480358123779297,0.4170889096955458
a graph matching network. a graph matching network. their proposed method achieves a speedup of 80-500x. Pros: As much as we appreciated the approach presented here the performance just seems unacceptable compared to other methods. Pros: As much as we appreciated the approach presented here the performance just seems unacceptable compared to other methods.,"This paper introduces a novel SE(3) equivariant graph matching network, along with a keypoint discovery and alignment approach, for the problem of protein-protein docking, with a novel loss based on optimal transport. The overall consensus is that this is an impactful solution to an important problem, whereby competitive results are achieved without the need for templates, refinement, and are achieved with substantially faster run times.",0.8382754921913147,0.3766228273510933
Weaknesses: No major weaknesses identified. 2021 [1] Mosbach et al. Weaknesses: No major weaknesses identified. 2021 [2] Mosbach et al. Weaknesses: No major weaknesses identified. 2021 [3] Mosbach et al.,"The paper reviews and draws connections between several parameter-efficient fine-tuning methods. All reviewers found the paper addresses an important research problem, and the theoretical justification and empirical analyses are convincing.",0.8169086575508118,0.21803571811566752
the paper lacks a discussion part about the actual overhead for retrieval and the time overhead for running the model seems to be significant (8-20X slower) The paper further extends KNN-LM to utilize not next tokens but all neighborhood information to get global context. the paper lacks a discussion part about the actual overhead for retrieval and the time overhead for running the model seems to be significant (8-20X slower),"This paper introduces a new type of language model, the GNN-LM, which uses a graph neural network to allow a language model to reference similar contexts in the training corpus in addition to the input context. The empirical results are good, and the model sets a new SOTA on the benchmark Wikitext-103 corpus, as well as improving over strong baselines on two other language modeling datasets (enwiki8 and Billion Word Benchmark). The main drawback, as noted by one reviewer, is the computational expense of the method with significant slowdowns compared to the baseline. Two reviewers voted strong accept, with a third raising several concerns. The largest concern was the lack of comparison to prior work, especially prior retrieval based methods on two datasets. The authors responded with an ablation study comparing their method to KNN-LM and showed their proposed GNN-LM performs better. Other concerns raised by the reviewer were the paper's lack of clarity (the authors should address the reviewers questions during the next revision) and incremental technical contribution. Another reviewer highlighted the paper's novelty, and this AC agrees it is sufficient for publication. Overall, the method is an interesting, if expensive, extension of retrieval based language models, and the empirical results support its effectiveness.",0.8217278122901917,0.43837825540039277
proposed method is based on the over-parameterization of deep models. the advantage of the proposed method compared with other expansion-based methods would narrow down... the proposed method is based on the over-parameterization of deep models. the computational required to compute the distance between filter subspaces requires calculating an SVD which is computationally heavy.,"The authors propose a memory-based continual learning method that decomposes the models' parameters and that shares a large number of the decomposed parameters across tasks. In other words, only a small number of parameters are task-specific and the memory usage of storing models from previous tasks is hence a fraction of the memory usage of previous approaches. The authors take advantage of their method to propose specific ensembling approaches and demonstrate the strong performance of their methods using several datasets. In the rebuttal, the authors were very reactive and provided many useful additional results during including a comparison of the computational cost of their method vs. others, results using two new datasets (CUBS & Flowers), and additional results on mini-ImageNet. They also answered, through additional experiments, several reviewer questions including the robustness to different first tasks in the sequence. Overall, the reviewers after the rebuttal/discussion period agree that this is a strong contribution: novel and fairly simple method with some theoretical justification, thorough empirical evaluation, well-written and easy to follow manuscript. It also opens a few interesting avenues some of which the authors have already explored in their paper (e. g., ensembling).",0.8289747834205627,0.36181096277303165
I am not sure the proofs are required. I think it is a good idea and a good application of it. I think...............................,"This paper proposes an innovative method for continual learning that modifies the direction of gradients on a new task to minimise forgetting on previous tasks without data replay. The method is mathematically rigorous with a strong theoretical analysis and excellent empirical results across multiple continual learning benchmarks. It is a clear accept. There was good discussion between the reviewers and authors that addressed a number of minor issues, including clarifying that the method has the same computational complexity as backpropagation. The authors are encouraged to make sure that these points are addressed in the final version of the paper.",0.8321943283081055,0.21216645886500676
